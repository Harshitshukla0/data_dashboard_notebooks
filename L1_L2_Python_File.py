# -*- coding: utf-8 -*-
"""L1/L2 G-Sheet .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UtRqhgx28yUHXwuFV8HEmohwdyPQO4jA
"""

from google.colab import userdata
import os
os.environ['AWS_ACCESS_KEY_ID'] = userdata.get('AWS_ACCESS_KEY_ID')
os.environ['AWS_SECRET_ACCESS_KEY'] = userdata.get('AWS_SECRET_ACCESS_KEY')
os.environ['FLIPSIDE_PRO_SF_PASSWORD'] = userdata.get('FLIPSIDE_PRO_SF_PASSWORD')
os.environ['COINGECKO_PRO_PASSWORD'] = userdata.get('COINGECKO_PRO_PASSWORD')

!pip install awswrangler
!pip install pandasql
import pandas as pd
import requests
import awswrangler as wr
import pandas as pd
import requests
import math
from lxml import etree
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings("ignore")
import boto3
from datetime import datetime, timedelta, date
!pip install openpyxl
from pandasql import sqldf
pysqldf = lambda q: sqldf(q, globals())
def read_full_table(schema, table_name):
    return wr.athena.read_sql_table(table=table_name, database=schema)
import time
boto3.setup_default_session(region_name="us-east-1")
wr.s3.does_object_exist("s3://noaa-ghcn-pds/fake")
def get_database(provider):
    return f"raw_{provider}"
def delete_table(provider, dataset):
    # Delete existing table (when needed)
    wr.catalog.delete_table_if_exists(database=get_database(provider), table=dataset)
def create_or_overwrite_table(df, provider, dataset):
    database = get_database(provider)
    # Get the list of existing databases
    existing_databases = wr.catalog.get_databases()
    # Check if the database exists, create it if it doesn't
    if database not in existing_databases:
        wr.catalog.create_database(name=database, exist_ok=True)
    BUCKET = "holocron-raw-us-east-1-368603383413"
    DATASETS_PATH = "notebook_datasets"
    # Create/Overwrite the table
    return wr.s3.to_parquet(
        df=df,
        path=f's3://{BUCKET}/{DATASETS_PATH}/{provider}/{dataset}',
        dataset=True,
        mode="overwrite",
        compression="gzip",
        partition_cols=[],
        database=database,  # Athena/Glue database
        table=dataset  # Athena/Glue table
    )


import datetime

current_datetime = datetime.datetime.now()
days_to_subtract = 1
new_date = current_datetime - datetime.timedelta(days=days_to_subtract)
new_date_pd = pd.to_datetime(new_date)
date_only = new_date_pd.date()
date_string = date_only.strftime('%Y-%m-%d')

api_key = '6ZWqonuAjop417n0IDMoppgFBhUCM4wD'


proxies = {
    'http': 'http://scraperapi:8a457df49571f250d536be0990207d40@proxy-server.scraperapi.com:8001',
    'https': 'http://scraperapi:8a457df49571f250d536be0990207d40@proxy-server.scraperapi.com:8001',
}

url = 'https://api.llama.fi/protocol/portal'
res = requests.get(url)
data = res.json()
data

"""## L1 Dashboard !"""

#Layer 1 Tvl data

layer1_tvl = ['Ethereum','Solana', 'Tron', 'BSC','Sui', 'Near','Polygon','Ton', 'Aptos','Cosmos', 'Avalanche']
tvl_data = pd.DataFrame()
for i in range(len(layer1_tvl)):
    url = f'https://api.llama.fi/v2/historicalChainTvl/{layer1_tvl[i]}'
    response = requests.get(url)
    data = response.json()
    temp = pd.DataFrame(data)
    temp.columns = ['date', 'tvl']
    temp['date'] = pd.to_datetime(temp['date'], unit='s')
    temp['chain'] = layer1_tvl[i]
    tvl_data = pd.concat([tvl_data, temp], ignore_index=True)

print(tvl_data['date'].max())
print(tvl_data['chain'].nunique())
print(tvl_data['chain'].unique())

from datetime import datetime


current_date = datetime.today().date()
formatted_date = current_date.strftime('%Y/%m/%d')
formatted_date

headers = {
    'accept': 'application/json',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'cache-control': 'no-cache',
    'content-type': 'application/json',
    'origin': 'https://explorer.provenance.io',
    'pragma': 'no-cache',
    'priority': 'u=1, i',
    'referer': 'https://explorer.provenance.io/',
    'sec-ch-ua': '"Not/A)Brand";v="8", "Chromium";v="126", "Google Chrome";v="126"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-site',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
}

response = requests.get('https://service-explorer.provenance.io/api/v2/spotlight', headers=headers)
data = response.json()['totalAum']['amount']
cosmos = pd.DataFrame({'date':[date_string], 'chain':['Cosmos'],'cos_tvl':[data]})
cosmos['date'] = pd.to_datetime(cosmos['date'])
# Convert 'cos_tvl' to numeric type before adding to 'cos_max['tvl']'
cosmos['cos_tvl'] = cosmos['cos_tvl'].astype(float)

cos = tvl_data[tvl_data['chain']=='Cosmos']
cos_max = cos[cos['date']==cos['date'].max()]

cosmos['tvl'] = cosmos['cos_tvl'] + cos_max['tvl'].astype(float).values[0]
cosmos.drop(columns=['cos_tvl'], inplace=True)
cosmos = pd.concat([cosmos,cos],ignore_index=True)
cosmos = cosmos.drop_duplicates('date',keep='first')
cosmos

tvl_df = tvl_data[tvl_data['chain']!='Cosmos']
final_tvl = pd.concat([tvl_df,cosmos],ignore_index=True)
final_tvl


print(final_tvl['date'].max())
print(final_tvl['chain'].nunique())
print(final_tvl['chain'].unique())

# Ethereum L2 TVL

headers = {
    'accept': '*/*',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'cache-control': 'no-cache',
    'content-type': 'application/json',
    'pragma': 'no-cache',
    'priority': 'u=1, i',
    'referer': 'https://l2beat.com/scaling/tvl',
    'sec-ch-ua': '"Not)A;Brand";v="99", "Google Chrome";v="127", "Chromium";v="127"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-origin',
    'trpc-accept': 'application/jsonl',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36',
    'x-trpc-source': 'nextjs-react',
}

params = {
    'batch': '1',
    'input': '{"0":{"json":{"range":"max","excludeAssociatedTokens":false,"type":"layer2"}}}',
}

response = requests.get('https://l2beat.com/api/trpc/scaling.summary.chart', params=params, headers=headers)
data = response.text
import json

# Assuming 'data' contains multiple JSON objects separated by a delimiter (e.g., newline)
data_objects = data.split('\n')  # Split into individual JSON objects

parsed_data = []
for obj in data_objects:
    if obj:  # Check if the object is not empty
        try:
            parsed_data.append(json.loads(obj))
        except json.JSONDecodeError as e:
            print(f"Error parsing object: {obj}, Error: {e}")

parse_data = parsed_data[3]['json'][2][0]
eth_l2 = pd.DataFrame(parse_data[0])
eth_l2.columns = ['date', 'tvl1', 'tvl2','tvl3','tvl4']
eth_l2['date'] = pd.to_datetime(eth_l2['date'],unit='s')
eth_l2.drop(columns=['tvl4'],inplace=True)
eth_l2['tvl'] = eth_l2.drop(columns=['date']).sum(axis=1)
eth_l2 = eth_l2[['date','tvl']]
eth_l2['chain'] = 'Ethereum'
eth_l2['tvl'] = eth_l2['tvl']/100
eth_l2

import requests

lst1 = ['blitz','beam-swap','step-exchange']

aval_subnet_tvl = pd.DataFrame()
for i in range(len(lst1)):
  headers = {
      'accept': '*/*',
      'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
      'cache-control': 'no-cache',
      'origin': 'https://defillama.com',
      'pragma': 'no-cache',
      'priority': 'u=1, i',
      'referer': 'https://defillama.com/',
      'sec-ch-ua': '"Not)A;Brand";v="99", "Google Chrome";v="127", "Chromium";v="127"',
      'sec-ch-ua-mobile': '?0',
      'sec-ch-ua-platform': '"macOS"',
      'sec-fetch-dest': 'empty',
      'sec-fetch-mode': 'cors',
      'sec-fetch-site': 'cross-site',
      'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36',
  }

  response = requests.get(f'https://api.llama.fi/updatedProtocol/{lst1[i]}', headers=headers)
  data =response.json()['chainTvls']
  for i in data.keys():
    datta = data[i]['tvl']
    df = pd.DataFrame(datta)
    df.columns = ['date','tvl']
    df['date'] = pd.to_datetime(df['date'],unit='s')
    df['project'] = 'Beam'
    aval_subnet_tvl = pd.concat([aval_subnet_tvl,df],ignore_index=True)

aval_subnet_tvl = aval_subnet_tvl.groupby('date')['tvl'].sum().reset_index()
aval_subnet_tvl['project'] = 'Avalanche'
# aval_subnet_tvl

plk_chain = ['Hydration','Moonbeam','Astar','Bifrost','Acala','Interlay','Moonriver','Parallel','Equilibrium','Kintsugi','Heiko',
             'Karura','EnergyWeb','Manta Atlantic','Polkadex','Shiden']

parachain_tvl = pd.DataFrame()

for i in range(len(plk_chain)):
  url = f'https://api.llama.fi/v2/historicalChainTvl/{plk_chain[i]}'
  response = requests.get(url)
  data = response.json()
  temp = pd.DataFrame(data)
  temp.columns = ['date','tvl']
  temp['date'] = pd.to_datetime(temp['date'],unit='s')
  temp['project'] = plk_chain[i]
  parachain_tvl = pd.concat([parachain_tvl,temp],ignore_index=True)

parachain_tvl_ = parachain_tvl.groupby('date')['tvl'].sum().reset_index()
parachain_tvl_['chain'] = 'Parachain'
# parachain_tvl_

final_tvl = pd.concat([final_tvl,parachain_tvl_],ignore_index=True)
final_tvl

q = """select A.date,A.chain, sum(A.tvl) as tvl from final_tvl A
left join parachain_tvl_ B
on A.date = B.date and A.chain = B.chain
group by 1,2
"""
final_tvl = pysqldf(q)
final_tvl['date'] = pd.to_datetime(final_tvl['date']).dt.date
final_tvl.head()

# final_tvl_curr = final_tvl[final_tvl['date']==final_tvl['date'].max()]
# final_tvl_curr.drop(columns=['date'],inplace=True)

# final_tvl_curr

#Stablecoin MCAP
def extract_pegged_usd(row):
    return row.get('peggedUSD') or row.get('peggedUSD')


response = requests.get("https://stablecoins.llama.fi/stablecoins?includePrices=false")
results = response.json()
temp = pd.json_normalize(results['peggedAssets'])
temp['id_chain_map'] = temp.apply(lambda x: {x['id']: x['chains']}, axis=1)
temp = temp[['id', 'id_chain_map']]


layer1 = ['Ethereum','Solana', 'Tron', 'BSC','Sui', 'Near','Polygon','TON', 'Aptos','Cosmos','Avalanche','Parachain']

stablecoin_df = pd.DataFrame()

for j in layer1:
  try:
    for i in range(len(temp['id_chain_map'])):
        temp_dict = temp['id_chain_map'][i]
        ids = list(temp_dict.keys())[0]
        chains = list(temp_dict.values())[0]

        if j in chains:
            url = f"https://stablecoins.llama.fi/stablecoincharts/{j}?stablecoin={ids}"
            response = requests.get(url)
            if response.status_code == 200:
                data = response.json()
                df = pd.DataFrame(data)
                df = df[['date', 'totalCirculatingUSD']]
                df['Stablecoin MCAP'] = df['totalCirculatingUSD'].apply(extract_pegged_usd)
                df.drop(columns=['totalCirculatingUSD'], inplace=True)
                df['date'] = pd.to_datetime(df['date'], unit='s')
                df['chain'] = j
                stablecoin_df = pd.concat([stablecoin_df, df], ignore_index=True)
  except:
    pass
stablecoin_mcap = stablecoin_df.groupby(['date','chain'])['Stablecoin MCAP'].sum().reset_index()
print(stablecoin_mcap['chain'].unique())
print(stablecoin_mcap['chain'].unique())
# stablecoin_mcap_cur = stablecoin_mcap[stablecoin_mcap['date']==stablecoin_mcap['date'].max()]

# stablecoin_mcap_cur.drop(columns=['date'],inplace=True)
# stablecoin_mcap_cur
stablecoin_mcap.loc[stablecoin_mcap['chain']=='TON', 'chain']='Ton'
# stablecoin_mcap.loc[stablecoin_mcap['chain']=='Cosmos Hub', 'chain']='Cosmos'
stablecoin_mcap

#Dex Volume

layer1 = ['Ethereum','Solana', 'Tron', 'BSC','Sui', 'Near','Polygon','Ton', 'Aptos','Cosmos','Avalanche']
dex_volume = pd.DataFrame()
for i in range(len(layer1)):
    url = f'https://api.llama.fi/overview/dexs/{layer1[i]}?excludeTotalDataChart=false&excludeTotalDataChartBreakdown=true&dataType=dailyVolume'
    response = requests.get(url)
    data = response.json()['totalDataChart']
    temp = pd.DataFrame(data)
    temp.columns = ['date', 'dex_volume']
    temp['date'] = pd.to_datetime(temp['date'], unit='s')
    temp['chain'] = layer1[i]
    dex_volume = pd.concat([dex_volume, temp], ignore_index=True)
    # print(dex_volume)

print(dex_volume['chain'].unique())
print(dex_volume['chain'].nunique())
dex_volume.head()

# dex_volume_cur = dex_volume[dex_volume['date']==dex_volume['date'].max()]
# dex_volume_cur.drop(columns=['date'],inplace=True)

# dex_volume_cur

dex_volume

#NFT Sales Volume

layer1 = ['Ethereum','Solana', 'Tron', 'BSC','Sui', 'Near','Polygon','Ton', 'Aptos','Cosmos','Avalanche','Parachain']
nft_sales_vol = pd.DataFrame()

for j in range(len(layer1)):
    v = pd.DataFrame()  # Correctly instantiate an empty DataFrame

    headers = {
        'accept': '*/*',
        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
        'cache-control': 'no-cache',
        'content-type': 'application/json',
        'origin': 'https://www.cryptoslam.io',
        'pragma': 'no-cache',
        'priority': 'u=1, i',
        'referer': 'https://www.cryptoslam.io/',
        'sec-ch-ua': '"Not/A)Brand";v="8", "Chromium";v="126", "Google Chrome";v="126"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"macOS"',
        'sec-fetch-dest': 'empty',
        'sec-fetch-mode': 'cors',
        'sec-fetch-site': 'same-site',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
    }

    response = requests.get(f'https://web-api.cryptoslam.io/v1/nft-indexes/{layer1[j]}', headers=headers)
    data = response.json()
    key1 = list(data.keys())
    for i in range(len(key1)):
        # print(key1[i]['monthlySummary'])
        temp = pd.DataFrame([data[key1[i]]['monthlySummary']])
        temp['date'] = key1[i]
        v = pd.concat([v, temp], ignore_index=True)
        v = v[['date', 'totalPriceUSD']]
        v.columns = ['date', 'nft_sales_volume']
        v['chain'] = layer1[j]
    nft_sales_vol = pd.concat([nft_sales_vol,v], ignore_index=True)

print(nft_sales_vol['chain'].unique())
# nft_sales_vol_curr = nft_sales_vol[nft_sales_vol['date']==nft_sales_vol['date'].max()]
# nft_sales_vol_curr.drop(columns=['date'],inplace=True)

# nft_sales_vol_curr
nft_sales_vol

#Full time Developers

layer1 = ['ethereum','solana', 'tron', 'bnb-chain','sui', 'near','polygon','ton', 'aptos','cosmos','avalanche']
chain = ['Ethereum','Solana', 'Tron', 'BSC','Sui', 'Near','Polygon','Ton', 'Aptos','Cosmos','Avalanche']

full_time_dev = pd.DataFrame()
for i in range(len(layer1)):
    headers = {
        'accept': '*/*',
        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
        'cache-control': 'no-cache',
        # 'cookie': 'GCLB=CMngjr3Us-PBiAEQAw',
        'pragma': 'no-cache',
        'priority': 'u=1, i',
        'referer': 'https://www.developerreport.com/ecosystems/ethereum',
        'sec-ch-ua': '"Not/A)Brand";v="8", "Chromium";v="126", "Google Chrome";v="126"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"macOS"',
        'sec-fetch-dest': 'empty',
        'sec-fetch-mode': 'cors',
        'sec-fetch-site': 'same-origin',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
    }

    response = requests.get(
        f'https://www.developerreport.com/api/charts/dev_mau_by_dev_type/{layer1[i]}',
        headers=headers,
    )
    data = response.json()['series'][0]['data']
    temp = pd.DataFrame(data)
    temp.columns = ['date', 'full_time_developers']
    temp['date'] = temp['date']/1000
    temp['date'] = pd.to_datetime(temp['date'], unit='s')
    temp['chain'] = chain[i]
    full_time_dev = pd.concat([full_time_dev, temp], ignore_index=True)


print(full_time_dev['date'].max())
print(full_time_dev['chain'].nunique())
print(full_time_dev['chain'].unique())
# full_time_dev_curr = full_time_dev[full_time_dev['date']==full_time_dev['date'].max()]
# full_time_dev_curr.drop(columns=['date'],inplace=True)

# full_time_dev_curr

full_time_dev

layer1 = ['ethereum','solana', 'tron', 'binance-smart-chain', 'polygon','the-open-network', 'aptos','cosmos','avalanche']
chain = ['Ethereum','Solana', 'Tron', 'BSC', 'Polygon','Ton', 'Aptos','Cosmos','Avalanche']

avg_txns_fee = pd.DataFrame()
active_adress = pd.DataFrame()



for i in range(len(layer1)):
  url = f"https://api.tokenterminal.com/v2/projects/{layer1[i]}/metrics?metric_ids=user_mau%2Cfees%2Ctransaction_fee_average"

  headers = {
      "accept": "application/json",
      "authorization": "Bearer 1ac6eaf7-9a60-4a1f-adc1-32ca4f85e49c"
  }

  response = requests.get(url, headers=headers)
  print(layer1[i])

  data = response.json()['data']
  df = pd.DataFrame(data)
  # df.columns = ['date', 'user_mau', 'fees', 'transaction_fee_average']
  txn_fee = df[['timestamp','project_name', 'transaction_fee_average']]
  txn_fee.columns = ['date','chain', 'avg_txn_fee']
  avg_txns_fee = pd.concat([avg_txns_fee,txn_fee], ignore_index=True)
  active_adress = pd.concat([active_adress,df], ignore_index=True)

chain = ['sui','near']
chains = ['Sui', 'Near']

fee_df = pd.DataFrame()
avg_txn_fees_df = pd.DataFrame()
dau_df = pd.DataFrame()


for i in range(len(chain)):
    headers = {
        'accept': '*/*',
        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
        'cache-control': 'no-cache',
        'origin': 'https://app.artemis.xyz',
        'pragma': 'no-cache',
        'priority': 'u=1, i',
        'referer': 'https://app.artemis.xyz/',
        'sec-ch-ua': '"Not)A;Brand";v="99", "Google Chrome";v="127", "Chromium";v="127"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"macOS"',
        'sec-fetch-dest': 'empty',
        'sec-fetch-mode': 'cors',
        'sec-fetch-site': 'cross-site',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36',
        'x-art-webtoken': 'eyJhbGciOiJIUzI1NiJ9.eyJpYXQiOjE3MjQ2NjI1MDIsImV4cCI6MTcyNDc0ODkwMn0.H2XyYajgaMbjDA-YY6cKyLylOhuBPd_e5BK6oIJCf88',
    }

    params = {
        'artemisIds': f'{chain[i]}',
        'startDate': '2024-05-26',
        'endDate': '2024-08-25',
    }

    response = requests.get('https://api.artemisxyz.com/data/DAU,AVG_TXN_FEE,FEES/', params=params, headers=headers)
    print(f'{chain[i]}')
    data = response.json()['data']['artemis_ids'][f'{chain[i]}']
    fee = pd.DataFrame(data['FEES'])
    fee.columns = ['date','fee']
    fee['chain'] = chains[i]
    fee_df = pd.concat([fee_df,fee], ignore_index=True)
    avg_txn_fees = pd.DataFrame(data['AVG_TXN_FEE'])
    avg_txn_fees.columns = ['date','avg_txn_fee']
    avg_txn_fees['chain'] = chains[i]
    avg_txn_fees_df = pd.concat([avg_txn_fees_df,avg_txn_fees], ignore_index=True)
    dau = pd.DataFrame(data['DAU'])
    dau['chain'] = chains[i]
    dau_df = pd.concat([dau_df,dau], ignore_index=True)

### FEES 30 day

fee_data = active_adress[['timestamp','project_name','fees']]
fee_data.columns = ['date','chain','fee']
fee_data['date'] = pd.to_datetime(fee_data['date']).dt.date
fee_data

fee_df = fee_df[fee_df['date'].astype(str) < date_string]
fees_final  = pd.concat([fee_data,fee_df], ignore_index=True)
fees_final

q = """select
    date,
    chain,
    sum(fee) over(partition by chain order by date rows between 29 preceding and current row) as fee_30d
    from fees_final
    """
fee_30d = pysqldf(q)

print(fee_30d['date'].max())
print(fee_30d['chain'].nunique())
print(fee_30d['chain'].unique())
fee_30d.head()
fee_30d.loc[fee_30d['chain']=='BNB Chain', 'chain']='BSC'
fee_30d.loc[fee_30d['chain'] == 'Cosmos Hub', 'chain'] = 'Cosmos'
fee_30d.loc[fee_30d['chain'] == 'TON', 'chain'] = 'Ton'


# fee_30d_cur = fee_30d[fee_30d['date']==fee_30d['date'].max()]
# fee_30d_cur.drop(columns=['date'],inplace=True)

# fee_30d_cur
fee_30d

## AVERAGE TRANSACTION FEE
avg_txns_fee['date'] = pd.to_datetime(avg_txns_fee['date'],format='ISO8601')
avg_txns_fee['date'] = pd.to_datetime(avg_txns_fee['date']).dt.date
avg_txns_fee
avg_txns_fee_final = pd.concat([avg_txns_fee,avg_txn_fees_df], ignore_index=True)

# print(avg_txns_fee_final['date'].max())

# avg_txns_fee_final['date'] = pd.to_datetime(avg_txns_fee_final['date']).dt.date

avg_txns_fee_final.loc[avg_txns_fee_final['chain']=='BNB Chain', 'chain']='BSC'
avg_txns_fee_final.loc[avg_txns_fee_final['chain'] == 'Cosmos Hub', 'chain'] = 'Cosmos'
avg_txns_fee_final.loc[avg_txns_fee_final['chain'] == 'TON', 'chain'] = 'Ton'

print(avg_txns_fee_final['chain'].nunique())
print(avg_txns_fee_final['chain'].unique())
avg_txns_fee_final.head()

# avg_txns_fee_final_cur = avg_txns_fee_final[avg_txns_fee_final['date'].astype(str) == '2024-07-14']
# avg_txns_fee_final_cur.drop(columns=['date'], inplace=True)
# avg_txns_fee_final_cur.loc[avg_txns_fee_final_cur['chain']=='BNB Chain', 'chain']='BSC'
# avg_txns_fee_final_cur.loc[avg_txns_fee_final_cur['chain']=='Cosmos Hub', 'chain']='Cosmos'
# avg_txns_fee_final_cur

#Monthly Active Adresses

address_data = active_adress[['timestamp','project_name','user_mau']]
address_data.columns = ['date','chain','monthly_active_address']
address_data['date'] = pd.to_datetime(fee_data['date']).dt.date

monthly_active_address = address_data.copy()

monthly_active_address.loc[monthly_active_address['chain']=='BNB Chain', 'chain']='BSC'
monthly_active_address.loc[monthly_active_address['chain']=='Cosmos Hub', 'chain']='Cosmos'
monthly_active_address.loc[monthly_active_address['chain']=='TON', 'chain']='Ton'

monthly_active_address

print(monthly_active_address['chain'].nunique())
print(monthly_active_address['chain'].unique())
monthly_active_address.head()


# monthly_active_address_cur = address_data[address_data['date']==address_data['date'].max()]
# monthly_active_address_cur.drop(columns=['date'],inplace=True)

nft_sales_vol['date'] = pd.to_datetime(nft_sales_vol['date']).dt.date
full_time_dev['date'] = pd.to_datetime(full_time_dev['date']).dt.date
stablecoin_mcap['date'] = pd.to_datetime(stablecoin_mcap['date']).dt.date
dex_volume['date'] = pd.to_datetime(dex_volume['date']).dt.date
final_tvl['date'] = pd.to_datetime(final_tvl['date']).dt.date
fee_30d['date'] = pd.to_datetime(fee_30d['date']).dt.date
avg_txns_fee_final['date'] = pd.to_datetime(avg_txns_fee_final['date']).dt.date
monthly_active_address
nft_sales_vol.head()

all_data = pd.merge(final_tvl, stablecoin_mcap,on=['date','chain'], how='outer')
all_data['date'] = pd.to_datetime(all_data['date']).dt.date
all_data = pd.merge(all_data,dex_volume, on=['date','chain'], how='outer')

all_data = pd.merge(all_data,full_time_dev, on=['date','chain'], how='outer')
all_data = pd.merge(all_data,nft_sales_vol, on=['date','chain'], how='outer')
all_data = pd.merge(all_data,fee_30d, on=['date','chain'], how='outer')
all_data = pd.merge(all_data,avg_txns_fee_final, on=['date','chain'], how='outer')
all_data = pd.merge(all_data,monthly_active_address, on=['date','chain'], how='outer')
all_data

all_data.to_csv('L1.csv')

# #calculating percentage change
def percent_change(df):
    for col in df:
        if col == 'tvl':
            df['tvl_7d_change'] = df.groupby('chain')['tvl'].pct_change(periods=7) * 100
            df['tvl_90d_trend'] = df.groupby('chain')['tvl'].pct_change(periods=90) * 100
        elif col == 'Stablecoin MCAP':
            df['mcap_7d_change'] = df.groupby('chain')['Stablecoin MCAP'].pct_change(periods=7) * 100
            df['mcap_90d_trend'] = df.groupby('chain')['Stablecoin MCAP'].pct_change(periods=90) * 100
        elif col == 'dex_volume':
            df['dex_vol_7d_change'] = df.groupby('chain')['dex_volume'].pct_change(periods=7) * 100
            df['dex_vol_90d_trend'] = df.groupby('chain')['dex_volume'].pct_change(periods=90) * 100
        elif col == 'full_time_developers':
            df['FTD_7d_change'] = df.groupby('chain')['full_time_developers'].pct_change(periods=7) * 100
            df['FTD_90d_trend'] = df.groupby('chain')['full_time_developers'].pct_change(periods=90) * 100
        elif col == 'nft_sales_volume':
            df['NFT_sale_vol_7d_change'] = df.groupby('chain')['nft_sales_volume'].pct_change(periods=7) * 100
            df['NFT_sale_vol_90d_trend'] = df.groupby('chain')['nft_sales_volume'].pct_change(periods=90) * 100
        elif col == 'fee_30d':
            df['fee_30d_7d_change'] = df.groupby('chain')['fee_30d'].pct_change(periods=7) * 100
            df['fee_30d_90d_trend'] = df.groupby('chain')['fee_30d'].pct_change(periods=90) * 100
        elif col == 'avg_txn_fee':
            df['avg_txn_fee_7d_change'] = df.groupby('chain')['avg_txn_fee'].pct_change(periods=7) * 100
            df['avg_txn_fee_90d_trend'] = df.groupby('chain')['avg_txn_fee'].pct_change(periods=90) * 100
        elif col == 'monthly_active_address':
            df['mau_7d_change'] = df.groupby('chain')['monthly_active_address'].pct_change(periods=7) * 100
            df['mau_90d_trend'] = df.groupby('chain')['monthly_active_address'].pct_change(periods=90) * 100

percent_change(all_data)

q = """SELECT *,
avg(tvl) OVER(PARTITION BY chain ORDER BY date ASC ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS avg7D_tvl,

avg("Stablecoin MCAP") OVER(PARTITION BY chain ORDER BY date ASC ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS avg7D_mcap,
avg(dex_volume) OVER (PARTITION BY chain ORDER BY date ASC ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS avg7D_dex_vol,
avg(full_time_developers) OVER(PARTITION BY chain ORDER BY date ASC ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS avg7D_FTD,
avg(nft_sales_volume) OVER(PARTITION BY chain ORDER BY date ASC ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS avg7D_NFT_sale_vol,
avg(fee_30d) OVER (PARTITION BY chain ORDER BY date ASC ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS avg7D_fee_30d,
avg(avg_txn_fee) OVER (PARTITION BY chain ORDER BY date ASC ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS avg7D_avg_txn_fee,
avg(monthly_active_address) OVER (PARTITION BY chain ORDER BY date ASC ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS avg7D_mau

FROM all_data"""

final_data_df = pysqldf(q)

final_data_df = final_data_df[final_data_df['date']<date_string]

# dev__nft_data = final_data_df[['date', 'chain','full_time_developers','avg7D_FTD', 'FTD_7d_change','nft_sales_volume','avg7D_NFT_sale_vol','NFT_sale_vol_7d_change',]]
# dev__nft_data

# L1 current data

l1_data_curr = final_data_df[final_data_df['date']==final_data_df['date'].max()]
l1_data_curr=l1_data_curr[['chain','tvl','avg7D_tvl','tvl_7d_change','Stablecoin MCAP','avg7D_mcap','mcap_7d_change','dex_volume','avg7D_dex_vol',
                           'dex_vol_7d_change','full_time_developers','avg7D_FTD', 'FTD_7d_change','nft_sales_volume','avg7D_NFT_sale_vol',
                           'NFT_sale_vol_7d_change','fee_30d','avg7D_fee_30d','fee_30d_7d_change','avg_txn_fee','avg7D_avg_txn_fee',
                           'avg_txn_fee_7d_change','monthly_active_address','avg7D_mau', 'mau_7d_change']]

provider = 'm31'
dataset = 'l1_current'
create_or_overwrite_table(l1_data_curr, provider, dataset)

l1_data_curr.to_csv('l1.csv')

import gspread
from google.auth import exceptions
from google.oauth2.service_account import Credentials


def authenticate_google_sheets():
    credentials = None

    try:
        credentials = Credentials.from_service_account_info(
        {
          "type": "service_account",
          "project_id": "ambient-stack-406610",
          "private_key_id": private_key_id,
          "private_key": "-----BEGIN PRIVATE KEY-----\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQCka1M1xiMk4uiX\n2clV7ijvPIoUfk8t4c4PTtIBSbDhEJH83gO3wK1CqdSDjjB6lNMQXiKNB5k8AXU1\n48qTAFwGWyEVwtX4LRLq6gXFWuprJe6W7gn7lKH8JWsjjnm0L7xf0Y3Bbp1ozoqD\noN/siJNqw9gK/86QqB2XTVkRrgPOWcVHSguajw7v013vR4uyNFB4PVMciGsMx0Rj\n8WdMqBhfG/gg/OwXUITFx9DBg5nogsHLdBNkJ2r6bD5bPvISXyBPKAu7aPu4b3xH\n+Bik5aHJnbxNLyhOk157vG8Hak0fG72aAXZ/50hdHfapSYkqx8BJ0VB8ZMOhJNPX\nlV0v72I7AgMBAAECggEAP3Pb7QjGT1nygYffF3aN/aXTdA066W4XY/j7OtwBkxod\n/QSBUszdELkR9qNNOkFtIwfxNZQVIv89CMscRpcA7MMGhatngBuFDXu7NmrbxPYi\nmcTLvXei+/hE3LgDZ/J0NFZe6qORw/zRn/LZ+CLNIYrrOXN3eIQox1dmZFhPx7Zm\nvj3DrC24eruVgWkYS6wAoWnNHRyZqPvFcE6kHpKOvnQHtwyJqyLObWcNaSryEdG5\nufr3gtS+ginbEEkpJ1JnQNIGXKpM/O06e5O/H6RE1iU1CTRUwWN3Yioz6W2fBBtR\nrh6hhhnkf27rR2J3v7DWQVM5c3Q6hqjZoCy7HckrAQKBgQDY8MQg0ZxAl86oqtVY\ncghY67m0ZCp2O5aHD32EeqLEuwgVjt6DCoLo4iEl0UBAXcP0TJUpFrZX9hJ1sYEC\nq2vUrjVYS4ijwfmli/00GIM/SR6RkW1tLTz+3EtebnILBucYoOhvmOYNH7s9FbzT\nPS3DUFQaVhgCNAFvhKwNVVDaWwKBgQDCBb8GawEXYMNRqLTTIGLnqeIXYf2cA0yH\nmkrJi02flaLdEufd8InRVIOeLs/jZJFr1/nS9rc5kmA1eIHg0mhbq2F7YCkfaPnV\nMAi2Rx3061yuIVM4yOlLHq01KUAT6/hXTd/nO+mFJ+SUgeEs8DotDSafOh8bR0oD\ngkHnp7ldoQKBgCwV0WVx8zTVJLP182ED21pmnNhhupdISLCtny461bTw5RWscN9k\nVXIJ8f6DZXEvHNEadv1gljGN2fZ82eC3ATS5KjIFN4E/vAG+Tvg1OwazTzj5uqkD\nFnAcSFyqSRagknnYySNUiPuFxUEGl9a9if0058JqWHqqItiMt4IGImYdAoGBAIDO\n+cwT/Ax+NA2heDL2PFNaiHxHlOwfkI4yE9aMAgOhfxdP9tl0WLq9Zgf9QgzP9m9n\nWjcBjhDNqcu17lvItHmvZK9Y3tQ4iCxNkGsa+bthCg2cmDiJwcAaZJl3gk/3h87G\nJ5DHSLgbDPi+5TRFZAoGwg5RstcUVAHSV1ipFDohAoGBAIZ/6oPxooI74VZUQfDD\nKxbJdN8xcsqT6CCdcHzvfLJ3W8fXNk/sI1FycYE0MM3mrIzW+TcGuyHToYfMVpfo\nDJ8lUq8jb97KY2SHa1gE36PFzpohkQS7GcFquE91Etp+ldvUX3ClXhB6VbKU4r9F\nac2RmwF0T+v6FD5DZ4m/iplb\n-----END PRIVATE KEY-----\n",
          "client_email": "hs-testing@ambient-stack-406610.iam.gserviceaccount.com",
          "client_id": "117454584926828343266",
          "auth_uri": "https://accounts.google.com/o/oauth2/auth",
          "token_uri": "https://oauth2.googleapis.com/token",
          "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
          "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/hs-testing%40ambient-stack-406610.iam.gserviceaccount.com",
          "universe_domain": "googleapis.com"
        },
            scopes=['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']
        )
    except exceptions.GoogleAuthError as e:
        print(f"Authentication failed: {e}")

    return credentials


schema_name = "raw_m31"
table_name = "l1_current"
data = read_full_table(schema_name, table_name)



credentials = authenticate_google_sheets()
client = gspread.authorize(credentials)
spreadsheet = client.open("m31_Google_sheet_data_DePIN + Web3")


df = data.copy()
df = df.round(0)
df = df.astype(str)


sheet = spreadsheet.worksheet("test")

# Clear existing content in the sheet
# sheet.clear()

# Convert DataFrame to list of lists
data_list = [df.columns.values.tolist()] + df.values.tolist()

# Update the sheet with the DataFrame data
sheet.update(data_list)

l1_data_hist = final_data_df[['date','chain', 'tvl', 'tvl_90d_trend', 'Stablecoin MCAP','mcap_90d_trend', 'dex_volume','dex_vol_90d_trend',
       'full_time_developers','FTD_90d_trend', 'nft_sales_volume','NFT_sale_vol_90d_trend', 'fee_30d','fee_30d_90d_trend',
               'avg_txn_fee','avg_txn_fee_90d_trend','monthly_active_address','mau_90d_trend']]


from datetime import datetime, timedelta

current_date = datetime.now()
date_90_days_ago = current_date - timedelta(days=92)

l1_data_hist = l1_data_hist[l1_data_hist['date'].astype('datetime64[ns]') >= date_90_days_ago]

provider = 'm31'
dataset = 'l1_trend'
create_or_overwrite_table(l1_data_hist, provider, dataset)

import gspread
from google.auth import exceptions
from google.oauth2.service_account import Credentials


def authenticate_google_sheets():
    credentials = None

    try:
        credentials = Credentials.from_service_account_info(
        {
          "type": "service_account",
          "project_id": "ambient-stack-406610",
          "private_key_id": private_key_id,
          "private_key": "-----BEGIN PRIVATE KEY-----\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQCka1M1xiMk4uiX\n2clV7ijvPIoUfk8t4c4PTtIBSbDhEJH83gO3wK1CqdSDjjB6lNMQXiKNB5k8AXU1\n48qTAFwGWyEVwtX4LRLq6gXFWuprJe6W7gn7lKH8JWsjjnm0L7xf0Y3Bbp1ozoqD\noN/siJNqw9gK/86QqB2XTVkRrgPOWcVHSguajw7v013vR4uyNFB4PVMciGsMx0Rj\n8WdMqBhfG/gg/OwXUITFx9DBg5nogsHLdBNkJ2r6bD5bPvISXyBPKAu7aPu4b3xH\n+Bik5aHJnbxNLyhOk157vG8Hak0fG72aAXZ/50hdHfapSYkqx8BJ0VB8ZMOhJNPX\nlV0v72I7AgMBAAECggEAP3Pb7QjGT1nygYffF3aN/aXTdA066W4XY/j7OtwBkxod\n/QSBUszdELkR9qNNOkFtIwfxNZQVIv89CMscRpcA7MMGhatngBuFDXu7NmrbxPYi\nmcTLvXei+/hE3LgDZ/J0NFZe6qORw/zRn/LZ+CLNIYrrOXN3eIQox1dmZFhPx7Zm\nvj3DrC24eruVgWkYS6wAoWnNHRyZqPvFcE6kHpKOvnQHtwyJqyLObWcNaSryEdG5\nufr3gtS+ginbEEkpJ1JnQNIGXKpM/O06e5O/H6RE1iU1CTRUwWN3Yioz6W2fBBtR\nrh6hhhnkf27rR2J3v7DWQVM5c3Q6hqjZoCy7HckrAQKBgQDY8MQg0ZxAl86oqtVY\ncghY67m0ZCp2O5aHD32EeqLEuwgVjt6DCoLo4iEl0UBAXcP0TJUpFrZX9hJ1sYEC\nq2vUrjVYS4ijwfmli/00GIM/SR6RkW1tLTz+3EtebnILBucYoOhvmOYNH7s9FbzT\nPS3DUFQaVhgCNAFvhKwNVVDaWwKBgQDCBb8GawEXYMNRqLTTIGLnqeIXYf2cA0yH\nmkrJi02flaLdEufd8InRVIOeLs/jZJFr1/nS9rc5kmA1eIHg0mhbq2F7YCkfaPnV\nMAi2Rx3061yuIVM4yOlLHq01KUAT6/hXTd/nO+mFJ+SUgeEs8DotDSafOh8bR0oD\ngkHnp7ldoQKBgCwV0WVx8zTVJLP182ED21pmnNhhupdISLCtny461bTw5RWscN9k\nVXIJ8f6DZXEvHNEadv1gljGN2fZ82eC3ATS5KjIFN4E/vAG+Tvg1OwazTzj5uqkD\nFnAcSFyqSRagknnYySNUiPuFxUEGl9a9if0058JqWHqqItiMt4IGImYdAoGBAIDO\n+cwT/Ax+NA2heDL2PFNaiHxHlOwfkI4yE9aMAgOhfxdP9tl0WLq9Zgf9QgzP9m9n\nWjcBjhDNqcu17lvItHmvZK9Y3tQ4iCxNkGsa+bthCg2cmDiJwcAaZJl3gk/3h87G\nJ5DHSLgbDPi+5TRFZAoGwg5RstcUVAHSV1ipFDohAoGBAIZ/6oPxooI74VZUQfDD\nKxbJdN8xcsqT6CCdcHzvfLJ3W8fXNk/sI1FycYE0MM3mrIzW+TcGuyHToYfMVpfo\nDJ8lUq8jb97KY2SHa1gE36PFzpohkQS7GcFquE91Etp+ldvUX3ClXhB6VbKU4r9F\nac2RmwF0T+v6FD5DZ4m/iplb\n-----END PRIVATE KEY-----\n",
          "client_email": "hs-testing@ambient-stack-406610.iam.gserviceaccount.com",
          "client_id": "117454584926828343266",
          "auth_uri": "https://accounts.google.com/o/oauth2/auth",
          "token_uri": "https://oauth2.googleapis.com/token",
          "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
          "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/hs-testing%40ambient-stack-406610.iam.gserviceaccount.com",
          "universe_domain": "googleapis.com"
        },
            scopes=['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']
        )
    except exceptions.GoogleAuthError as e:
        print(f"Authentication failed: {e}")

    return credentials


schema_name = "raw_m31"
table_name = "l1_trend"
data = read_full_table(schema_name, table_name)



credentials = authenticate_google_sheets()
client = gspread.authorize(credentials)
spreadsheet = client.open("m31_Google_sheet_data_DePIN + Web3")


df = data.copy()
df = df.round(0)
df = df.astype(str)


sheet = spreadsheet.worksheet("test")

# Clear existing content in the sheet
# sheet.clear()

# Convert DataFrame to list of lists
data_list = [df.columns.values.tolist()] + df.values.tolist()

# Update the sheet with the DataFrame data
sheet.update(data_list)

"""## Layer 2

"""

#TVL
import requests

headers = {
    'accept': '*/*',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'cache-control': 'no-cache',
    'pragma': 'no-cache',
    'priority': 'u=1, i',
    'referer': 'https://l2beat.com/scaling/tvl',
    'sec-ch-ua': '"Not/A)Brand";v="8", "Chromium";v="126", "Google Chrome";v="126"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-origin',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
}

response = requests.get('https://l2beat.com/api/tvl/scaling.json', headers=headers)
data = response.json()['daily']['data']
df = pd.DataFrame(data)
df.columns = list(response.json()['daily']['types'])
df = df[['timestamp', 'totalUsd']]
df.columns = ['date', 'total_tvl']
df['date'] = pd.to_datetime(df['date'], unit='s').dt.date
total_tvl = df.copy()





layer2 = ['arbitrum','optimism','base','blast','mantle','starknet','linea','mantapacific','mode','scroll',
'immutablex','polygonzkevm','zksync-era','loopring','rhinofi','bobanetwork','kinto']

chain = ['Arbitrum One','OP','Base','Blast','Mantle','Starknet','Linea','Manta','Mode','Scroll',
'Immutable X','Polygon zkEVM','Zksync Era','loopring','rhino.fi','Boba','Kinto']

layer2_tvl = pd.DataFrame()
for i in range(len(layer2)):
    headers = {
        'accept': '*/*',
        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
        'cache-control': 'no-cache',
        'origin': 'https://l2beat.com',
        'pragma': 'no-cache',
        'priority': 'u=1, i',
        'referer': 'https://l2beat.com/scaling/projects/arbitrum',
        'sec-ch-ua': '"Not/A)Brand";v="8", "Chromium";v="126", "Google Chrome";v="126"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"macOS"',
        'sec-fetch-dest': 'empty',
        'sec-fetch-mode': 'cors',
        'sec-fetch-site': 'same-origin',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
    }

    response = requests.get(f'https://l2beat.com/api/tvl/{layer2[i]}.json', headers=headers)
    data = response.json()['daily']['data']
    df = pd.DataFrame(data)
    df.columns = list(response.json()['daily']['types'])
    df = df[['timestamp','totalUsd']]
    df.columns = ['date','tvl']
    df['date'] = pd.to_datetime(df['date'],unit='s').dt.date
    df['chain'] = chain[i]
    layer2_tvl = pd.concat([layer2_tvl,df], ignore_index=True)



tvl_mrkt_share = pd.merge(layer2_tvl,total_tvl,on='date',how='inner')
tvl_mrkt_share['tvl_market_share'] = (tvl_mrkt_share['tvl']/tvl_mrkt_share['total_tvl'])*100
tvl_mrkt_share.drop(columns=['total_tvl'],inplace=True)


print(tvl_mrkt_share['chain'].nunique())
print(tvl_mrkt_share['chain'].unique())
print(tvl_mrkt_share['date'].max())
tvl_mrkt_share.head()

# tvl_mrkt_share_curr = tvl_mrkt_share[tvl_mrkt_share['date']==tvl_mrkt_share['date'].max()]
# tvl_mrkt_share_curr.drop(columns=['date'],inplace=True)
# tvl_mrkt_share_curr

tvl_mrkt_share['date'] = pd.to_datetime(tvl_mrkt_share['date']).dt.date
tvl_mrkt_share

#Stable coin Market cap

project = ['polygon_zkevm', 'optimism' ,'arbitrum', 'imx', 'zksync_era', 'base',
'linea', 'scroll', 'mantle', 'loopring' ,'starknet' ,'rhino',  'manta','blast', 'mode' ]

chain = ['Polygon zkEVM','OP','Arbitrum One','Immutable X','Zksync Era','Base','Linea',
'Scroll','Mantle','loopring','Starknet','rhino.fi','Manta','Blast','Mode']


print(len(project))
print(len(chain))
chain_map = pd.DataFrame({'chain':chain,'project':project})
chain_map


mcap_l2 = pd.DataFrame()

headers = {
    'accept': '*/*',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'cache-control': 'no-cache',
    'origin': 'https://www.growthepie.xyz',
    'pragma': 'no-cache',
    'priority': 'u=1, i',
    'referer': 'https://www.growthepie.xyz/',
    'sec-ch-ua': '"Not/A)Brand";v="8", "Chromium";v="126", "Google Chrome";v="126"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-site',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
}

response = requests.get('https://api.growthepie.xyz/v1/metrics/stables_mcap.json', headers=headers)
data = response.json()['data']['chains'] #['polygon_zkevm']['daily']
for i in data.keys():
    df = pd.DataFrame(data[i]['daily']['data'])
    df.columns = ['date', 'mcap_usd', 'mcap_eth']
    df['date'] = df['date']/1000
    df['date'] = pd.to_datetime(df['date'],unit='s')
    df['project'] = i
    mcap_l2 = pd.concat([mcap_l2, df], ignore_index=True)



mcap_l2 = pd.merge(mcap_l2,chain_map,on='project',how='inner')
mcap_l2.drop(columns=['project','mcap_eth'],inplace=True)
print(mcap_l2['chain'].nunique())
print(mcap_l2['chain'].unique())
print(mcap_l2['date'].max())
mcap_l2['date'] = pd.to_datetime(mcap_l2['date']).dt.date
mcap_l2.head()

# mcap_l2_curr = mcap_l2[mcap_l2['date']==mcap_l2['date'].max()]
# mcap_l2_curr.drop(columns=['date'],inplace=True)

#transaction cost
project = ['polygon_zkevm', 'optimism', 'arbitrum', 'zksync_era', 'base',
 'linea', 'scroll', 'mantle', 'starknet', 'manta','blast','mode']

chain = ['Polygon zkEVM','OP','Arbitrum One','Zksync Era','Base','Linea',
'Scroll','Mantle','Starknet','Manta','Blast','Mode']

print(len(project))
print(len(chain))
chain_map = pd.DataFrame({'chain':chain,'project':project})
chain_map

transaction_cost = pd.DataFrame()
headers = {
    'accept': '*/*',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'cache-control': 'no-cache',
    'origin': 'https://www.growthepie.xyz',
    'pragma': 'no-cache',
    'priority': 'u=1, i',
    'referer': 'https://www.growthepie.xyz/',
    'sec-ch-ua': '"Not/A)Brand";v="8", "Chromium";v="126", "Google Chrome";v="126"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-site',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
}

response = requests.get('https://api.growthepie.xyz/v1/metrics/txcosts.json', headers=headers)
data = response.json()['data']['chains']
for i in data.keys():
    df = pd.DataFrame(data[i]['daily']['data'])
    df.columns = ['date', 'txn_cost_usd', 'txn_cost_eth']
    df['date'] = df['date']/1000
    df['date'] = pd.to_datetime(df['date'],unit='s')
    df['project'] = i
    transaction_cost = pd.concat([transaction_cost, df], ignore_index=True)


transaction_cost = pd.merge(transaction_cost,chain_map,on='project',how='inner')
transaction_cost.drop(columns=['project','txn_cost_eth'],inplace=True)
print(transaction_cost['chain'].nunique())
print(transaction_cost['chain'].unique())
print(transaction_cost['date'].max())
transaction_cost['date'] = pd.to_datetime(transaction_cost['date']).dt.date
transaction_cost.head()
# transaction_cost_curr = transaction_cost[transaction_cost['date']==transaction_cost['date'].max()]
# transaction_cost_curr.drop(columns=['date'],inplace=True)

#Monthly Active Address
project = ['polygon_zkevm', 'optimism', 'arbitrum', 'imx' ,'zksync_era',
'base','linea', 'scroll', 'mantle', 'loopring' ,'starknet', 'rhino',
 'manta', 'blast', 'mode']

chain = ['Polygon zkEVM','OP','Arbitrum One','Immutable X','Zksync Era','Base','Linea',
'Scroll','Mantle','loopring','Starknet','rhino.fi','Manta','Blast','Mode']

print(len(project))
print(len(chain))
chain_map = pd.DataFrame({'chain':chain,'project':project})
chain_map

monthly_active_adress = pd.DataFrame()

headers = {
    'accept': '*/*',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'cache-control': 'no-cache',
    'origin': 'https://www.growthepie.xyz',
    'pragma': 'no-cache',
    'priority': 'u=1, i',
    'referer': 'https://www.growthepie.xyz/',
    'sec-ch-ua': '"Not/A)Brand";v="8", "Chromium";v="126", "Google Chrome";v="126"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-site',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
}

response = requests.get('https://api.growthepie.xyz/v1/metrics/daa.json', headers=headers)
data = response.json()['data']['chains']
for i in data.keys():
    df = pd.DataFrame(data[i]['monthly']['data'])
    df.columns = ['date', 'monthly_active_adress']
    df['date'] = df['date']/1000
    df['date'] = pd.to_datetime(df['date'],unit='s')
    df['project'] = i
    monthly_active_adress = pd.concat([monthly_active_adress, df], ignore_index=True)


monthly_active_adress = pd.merge(monthly_active_adress,chain_map,on='project',how='inner')
monthly_active_adress.drop(columns=['project'],inplace=True)
print(monthly_active_adress['chain'].nunique())
print(monthly_active_adress['chain'].unique())
print(monthly_active_adress['date'].max())
monthly_active_adress['date'] = pd.to_datetime(monthly_active_adress['date']).dt.date
monthly_active_adress.head()
# monthly_active_adress_curr = monthly_active_adress[monthly_active_adress['date']==monthly_active_adress['date'].max()]
# monthly_active_adress_curr.drop(columns=['date'],inplace=True)

#Monthaly Transaction count

layer2 = ['arbitrum','optimism','base','blast','mantle','starknet','linea','mantapacific','mode','scroll',
'immutablex','polygonzkevm','zksync-era','loopring','rhinofi','bobanetwork','kinto']

chain = ['Arbitrum One','OP','Base','Blast','Mantle','Starknet','Linea','Manta','Mode','Scroll',
'Immutable X','Polygon zkEVM','Zksync Era','loopring','rhino.fi','Boba','Kinto']

monthly_txn_count = pd.DataFrame()

for i in range(len(layer2)):
    headers = {
        'accept': '*/*',
        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
        'cache-control': 'no-cache',
        'origin': 'https://l2beat.com',
        'pragma': 'no-cache',
        'priority': 'u=1, i',
        'referer': 'https://l2beat.com/scaling/projects/base',
        'sec-ch-ua': '"Not/A)Brand";v="8", "Chromium";v="126", "Google Chrome";v="126"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"macOS"',
        'sec-fetch-dest': 'empty',
        'sec-fetch-mode': 'cors',
        'sec-fetch-site': 'same-origin',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
    }

    response = requests.get(f'https://l2beat.com/api/activity/{layer2[i]}.json', headers=headers)
    data = response.json()['daily']['data']
    temp = pd.DataFrame(data)
    temp.columns = ['date', 'transactions', 'tvl']
    temp['date'] = pd.to_datetime(temp['date'],unit='s')
    temp['chain'] = chain[i]
    monthly_txn_count = pd.concat([monthly_txn_count,temp],ignore_index=True)

q = """select
    date,
    chain,
    sum(transactions) over(partition by chain order by date rows between 29 preceding and current row) as transactions_30d
    from monthly_txn_count
    """
monthly_txn_count_30d = pysqldf(q)


print(monthly_txn_count_30d['chain'].nunique())
print(monthly_txn_count_30d['chain'].unique())
print(monthly_txn_count_30d['date'].max())
monthly_txn_count_30d['date'] = pd.to_datetime(monthly_txn_count_30d['date']).dt.date
monthly_txn_count_30d.head()
#
# monthly_txn_count_30d_curr = monthly_txn_count_30d[monthly_txn_count_30d['date']==monthly_txn_count_30d['date'].max()]
# monthly_txn_count_30d_curr.drop(columns=['date'],inplace=True)

#Onchain Profits

project = ['polygon_zkevm', 'optimism' ,'arbitrum' ,'base', 'scroll', 'starknet','blast', 'mode']
chain = ['Polygon zkEVM','OP','Arbitrum One','Base','Scroll','Starknet','Blast','Mode']
chain_map = pd.DataFrame({'chain':chain,'project':project})
chain_map

onchain_profit = pd.DataFrame()

headers = {
    'accept': '*/*',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'cache-control': 'no-cache',
    'origin': 'https://www.growthepie.xyz',
    'pragma': 'no-cache',
    'priority': 'u=1, i',
    'referer': 'https://www.growthepie.xyz/',
    'sec-ch-ua': '"Not/A)Brand";v="8", "Chromium";v="126", "Google Chrome";v="126"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-site',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
}

response = requests.get('https://api.growthepie.xyz/v1/metrics/profit.json', headers=headers)
data = response.json()['data']['chains']
for i in data.keys():
    df = pd.DataFrame(data[i]['daily']['data'])
    df.columns = ['date', 'onchain_profit', 'onchain_profit_eth']
    df['date'] = df['date']/1000
    df['date'] = pd.to_datetime(df['date'],unit='s')
    df['project'] = i
    onchain_profit = pd.concat([onchain_profit, df], ignore_index=True)

onchain_profit = pd.merge(onchain_profit,chain_map,on='project',how='inner')
onchain_profit.drop(columns=['project','onchain_profit_eth'],inplace=True)
print(onchain_profit['chain'].nunique())
print(onchain_profit['chain'].unique())
print(onchain_profit['date'].max())
onchain_profit['date'] = pd.to_datetime(onchain_profit['date']).dt.date
onchain_profit.head()

# onchain_profit_curr = onchain_profit[onchain_profit['date']==onchain_profit['date'].max()]
# onchain_profit_curr.drop(columns=['date'],inplace=True)

#NFT Sales Volume


project = ['Arbitrum','Blast', 'Immutable']

chain = ['Arbitrum One','Blast','Immutable X']

chain_map = pd.DataFrame({'chain':chain,'project':project})
chain_map

layer2 = ['Arbitrum','OP','Base','Blast','Mantle','Starknet','Linea','Manta','Mode','Scroll',
'Immutable','Polygon zkEVM','Zksync Era','loopring','rhino.fi','Boba','Kinto']



nft_sales_vol = pd.DataFrame()

for j in range(len(layer2)):
    v = pd.DataFrame()  # Correctly instantiate an empty DataFrame

    headers = {
        'accept': '*/*',
        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
        'cache-control': 'no-cache',
        'content-type': 'application/json',
        'origin': 'https://www.cryptoslam.io',
        'pragma': 'no-cache',
        'priority': 'u=1, i',
        'referer': 'https://www.cryptoslam.io/',
        'sec-ch-ua': '"Not/A)Brand";v="8", "Chromium";v="126", "Google Chrome";v="126"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"macOS"',
        'sec-fetch-dest': 'empty',
        'sec-fetch-mode': 'cors',
        'sec-fetch-site': 'same-site',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
    }

    response = requests.get(f'https://web-api.cryptoslam.io/v1/nft-indexes/{layer2[j]}', headers=headers)
    data = response.json()
    key1 = list(data.keys())
    for i in range(len(key1)):
        # print(key1[i]['monthlySummary'])
        temp = pd.DataFrame([data[key1[i]]['monthlySummary']])
        temp['date'] = key1[i]
        v = pd.concat([v, temp], ignore_index=True)
        v = v[['date', 'totalPriceUSD']]
        v['project'] = layer2[j]
    nft_sales_vol = pd.concat([nft_sales_vol,v], ignore_index=True)


nft_sales_vol = pd.merge(nft_sales_vol,chain_map,on='project',how='inner')
nft_sales_vol.drop(columns=['project'],inplace=True)
nft_sales_vol.rename(columns={'totalPriceUSD':'nft_sales_volume'},inplace=True)
print(nft_sales_vol['chain'].nunique())
print(nft_sales_vol['chain'].unique())
print(nft_sales_vol['date'].max())
nft_sales_vol['date'] = pd.to_datetime(nft_sales_vol['date']).dt.date
nft_sales_vol.head()
# nft_sales_vol_curr = nft_sales_vol[nft_sales_vol['date']==nft_sales_vol['date'].max()]
# nft_sales_vol_curr.drop(columns=['date'],inplace=True)

#Dex Volume
layer2 = ['Arbitrum','Optimism','Base','Blast','Mantle','Starknet','Linea','Manta','Mode','Scroll',
'Immutable','Polygon zkEVM','Zksync Era','loopring','rhino.fi','Boba','Kinto']

chain = ['Arbitrum One','OP','Base','Blast','Mantle','Starknet','Linea','Manta','Mode','Scroll',
'Immutable X','Polygon zkEVM','Zksync Era','loopring','rhino.fi','Boba','Kinto']

dex_volume = pd.DataFrame()
for i in range(len(layer2)):
    url = f'https://api.llama.fi/overview/dexs/{layer2[i]}?excludeTotalDataChart=false&excludeTotalDataChartBreakdown=true&dataType=dailyVolume'
    response = requests.get(url)
    data = response.json()['totalDataChart']
    temp = pd.DataFrame(data)
    temp.columns = ['date', 'dex_volume']
    temp['date'] = pd.to_datetime(temp['date'], unit='s')
    temp['chain'] = chain[i]
    dex_volume = pd.concat([dex_volume, temp], ignore_index=True)

print(dex_volume['chain'].nunique())
print(dex_volume['chain'].unique())
print(dex_volume['date'].max())
dex_volume['date'] = pd.to_datetime(dex_volume['date']).dt.date
dex_volume.head()

# dex_volume_curr = dex_volume[dex_volume['date']==dex_volume['date'].max()]
# dex_volume_curr.drop(columns=['date'],inplace=True)

#Merging All the Data
# We need to chain the merge commands correctly to avoid providing multiple values for the 'on' parameter
all_data = tvl_mrkt_share.merge(mcap_l2, on=['date','chain'], how='outer')
all_data = all_data.merge(transaction_cost, on=['date','chain'], how='outer')
all_data = all_data.merge(monthly_active_adress, on=['date','chain'], how='outer')
all_data = all_data.merge(monthly_txn_count_30d, on=['date','chain'], how='outer')
all_data = all_data.merge(onchain_profit, on=['date','chain'], how='outer')
all_data = all_data.merge(nft_sales_vol, on=['date','chain'], how='outer')
all_data = all_data.merge(dex_volume, on=['date','chain'], how='outer')

# all_data['date'] = datetime.datetime.now().date()
all_data = all_data[['date','chain','tvl','mcap_usd','txn_cost_usd','monthly_active_adress','transactions_30d','onchain_profit','nft_sales_volume','dex_volume','tvl_market_share']]
all_data['date'] = pd.to_datetime(all_data['date'])

all_data.columns

# #calculating percentage change
def percent_change(df):
    for col in df:
        if col == 'tvl':
            df['tvl_7d_change'] = df.groupby('chain')['tvl'].pct_change(periods=7) * 100
            df['tvl_90d_trend'] = df.groupby('chain')['tvl'].pct_change(periods=90) * 100
        elif col == 'mcap_usd':
            df['mcap_7d_change'] = df.groupby('chain')['mcap_usd'].pct_change(periods=7) * 100
            df['mcap_90d_trend'] = df.groupby('chain')['mcap_usd'].pct_change(periods=90) * 100
        elif col == 'dex_volume':
            df['dex_vol_7d_change'] = df.groupby('chain')['dex_volume'].pct_change(periods=7) * 100
            df['dex_vol_90d_trend'] = df.groupby('chain')['dex_volume'].pct_change(periods=90) * 100
        elif col == 'txn_cost_usd':
            df['txn_cost_7d_change'] = df.groupby('chain')['txn_cost_usd'].pct_change(periods=7) * 100
            df['txn_cost_90d_trend'] = df.groupby('chain')['txn_cost_usd'].pct_change(periods=90) * 100
        elif col == 'monthly_active_adress':
            df['mau_7d_change'] = df.groupby('chain')['monthly_active_adress'].pct_change(periods=7) * 100
            df['mau_90d_trend'] = df.groupby('chain')['monthly_active_adress'].pct_change(periods=90) * 100
        elif col == 'transactions_30d':
            df['txn30_7d_change'] = df.groupby('chain')['transactions_30d'].pct_change(periods=7) * 100
            df['txn30_90d_trend'] = df.groupby('chain')['transactions_30d'].pct_change(periods=90) * 100
        elif col == 'nft_sales_volume':
            df['NFT_sale_vol_7d_change'] = df.groupby('chain')['nft_sales_volume'].pct_change(periods=7) * 100
            df['NFT_sale_vol_90d_trend'] = df.groupby('chain')['nft_sales_volume'].pct_change(periods=90) * 100
        elif col == 'onchain_profit':
            df['onchain_profit_7d_change'] = df.groupby('chain')['onchain_profit'].pct_change(periods=7) * 100
            df['onchain_profit_90d_trend'] = df.groupby('chain')['onchain_profit'].pct_change(periods=90) * 100
        elif col == 'tvl_market_share':
            df['tvl_mrkt_share_7d_change'] = df.groupby('chain')['tvl_market_share'].diff(periods=7)
            df['tvl_mrkt_share_90d_trend'] = df.groupby('chain')['tvl_market_share'].diff(periods=90)



percent_change(all_data)

all_data.columns

q = """SELECT *,
avg(tvl) OVER(PARTITION BY chain ORDER BY date ASC ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS avg7D_tvl,
avg("mcap_usd") OVER(PARTITION BY chain ORDER BY date ASC ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS avg7D_mcap,
avg(dex_volume) OVER (PARTITION BY chain ORDER BY date ASC ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS avg7D_dex_vol,
avg(txn_cost_usd) OVER(PARTITION BY chain ORDER BY date ASC ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS avg7D_txn_cost,
avg(nft_sales_volume) OVER(PARTITION BY chain ORDER BY date ASC ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS avg7D_NFT_sale_vol,
avg(transactions_30d) OVER (PARTITION BY chain ORDER BY date ASC ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS avg7D_txn_30d,
avg(tvl_market_share) OVER (PARTITION BY chain ORDER BY date ASC ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS avg7D_mrkt_share,
avg(monthly_active_adress) OVER (PARTITION BY chain ORDER BY date ASC ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS avg7D_mau,
avg(onchain_profit) OVER (PARTITION BY chain ORDER BY date ASC ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS avg7D_onchain_profit

FROM all_data"""

final_data_df = pysqldf(q)
final_data_df['date'] = pd.to_datetime(final_data_df['date'])
layer_2_data = final_data_df[final_data_df['date']<all_data['date'].max()]

layer_2_data.columns

l2_data_curr = layer_2_data[layer_2_data['date']==layer_2_data['date'].max()]

l2_data_curr = layer_2_data[['date', 'chain', 'tvl','avg7D_tvl','tvl_7d_change','tvl_market_share','avg7D_mrkt_share','tvl_mrkt_share_7d_change',
'mcap_usd', 'avg7D_mcap','mcap_7d_change','txn_cost_usd','avg7D_txn_cost','txn_cost_7d_change','monthly_active_adress','avg7D_mau','mau_7d_change',
'dex_volume','avg7D_dex_vol','dex_vol_7d_change','transactions_30d','avg7D_txn_30d','txn30_7d_change', 'onchain_profit','avg7D_onchain_profit' ,
                             'onchain_profit_7d_change','nft_sales_volume','avg7D_NFT_sale_vol','NFT_sale_vol_7d_change']]



provider = 'm31'
dataset = 'layer2_current'
create_or_overwrite_table(l2_data_curr, provider, dataset)

import gspread
from google.auth import exceptions
from google.oauth2.service_account import Credentials


def authenticate_google_sheets():
    credentials = None

    try:
        credentials = Credentials.from_service_account_info(
        {
          "type": "service_account",
          "project_id": "ambient-stack-406610",
          "private_key_id": private_key_id,
          "private_key": "-----BEGIN PRIVATE KEY-----\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQCka1M1xiMk4uiX\n2clV7ijvPIoUfk8t4c4PTtIBSbDhEJH83gO3wK1CqdSDjjB6lNMQXiKNB5k8AXU1\n48qTAFwGWyEVwtX4LRLq6gXFWuprJe6W7gn7lKH8JWsjjnm0L7xf0Y3Bbp1ozoqD\noN/siJNqw9gK/86QqB2XTVkRrgPOWcVHSguajw7v013vR4uyNFB4PVMciGsMx0Rj\n8WdMqBhfG/gg/OwXUITFx9DBg5nogsHLdBNkJ2r6bD5bPvISXyBPKAu7aPu4b3xH\n+Bik5aHJnbxNLyhOk157vG8Hak0fG72aAXZ/50hdHfapSYkqx8BJ0VB8ZMOhJNPX\nlV0v72I7AgMBAAECggEAP3Pb7QjGT1nygYffF3aN/aXTdA066W4XY/j7OtwBkxod\n/QSBUszdELkR9qNNOkFtIwfxNZQVIv89CMscRpcA7MMGhatngBuFDXu7NmrbxPYi\nmcTLvXei+/hE3LgDZ/J0NFZe6qORw/zRn/LZ+CLNIYrrOXN3eIQox1dmZFhPx7Zm\nvj3DrC24eruVgWkYS6wAoWnNHRyZqPvFcE6kHpKOvnQHtwyJqyLObWcNaSryEdG5\nufr3gtS+ginbEEkpJ1JnQNIGXKpM/O06e5O/H6RE1iU1CTRUwWN3Yioz6W2fBBtR\nrh6hhhnkf27rR2J3v7DWQVM5c3Q6hqjZoCy7HckrAQKBgQDY8MQg0ZxAl86oqtVY\ncghY67m0ZCp2O5aHD32EeqLEuwgVjt6DCoLo4iEl0UBAXcP0TJUpFrZX9hJ1sYEC\nq2vUrjVYS4ijwfmli/00GIM/SR6RkW1tLTz+3EtebnILBucYoOhvmOYNH7s9FbzT\nPS3DUFQaVhgCNAFvhKwNVVDaWwKBgQDCBb8GawEXYMNRqLTTIGLnqeIXYf2cA0yH\nmkrJi02flaLdEufd8InRVIOeLs/jZJFr1/nS9rc5kmA1eIHg0mhbq2F7YCkfaPnV\nMAi2Rx3061yuIVM4yOlLHq01KUAT6/hXTd/nO+mFJ+SUgeEs8DotDSafOh8bR0oD\ngkHnp7ldoQKBgCwV0WVx8zTVJLP182ED21pmnNhhupdISLCtny461bTw5RWscN9k\nVXIJ8f6DZXEvHNEadv1gljGN2fZ82eC3ATS5KjIFN4E/vAG+Tvg1OwazTzj5uqkD\nFnAcSFyqSRagknnYySNUiPuFxUEGl9a9if0058JqWHqqItiMt4IGImYdAoGBAIDO\n+cwT/Ax+NA2heDL2PFNaiHxHlOwfkI4yE9aMAgOhfxdP9tl0WLq9Zgf9QgzP9m9n\nWjcBjhDNqcu17lvItHmvZK9Y3tQ4iCxNkGsa+bthCg2cmDiJwcAaZJl3gk/3h87G\nJ5DHSLgbDPi+5TRFZAoGwg5RstcUVAHSV1ipFDohAoGBAIZ/6oPxooI74VZUQfDD\nKxbJdN8xcsqT6CCdcHzvfLJ3W8fXNk/sI1FycYE0MM3mrIzW+TcGuyHToYfMVpfo\nDJ8lUq8jb97KY2SHa1gE36PFzpohkQS7GcFquE91Etp+ldvUX3ClXhB6VbKU4r9F\nac2RmwF0T+v6FD5DZ4m/iplb\n-----END PRIVATE KEY-----\n",
          "client_email": "hs-testing@ambient-stack-406610.iam.gserviceaccount.com",
          "client_id": "117454584926828343266",
          "auth_uri": "https://accounts.google.com/o/oauth2/auth",
          "token_uri": "https://oauth2.googleapis.com/token",
          "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
          "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/hs-testing%40ambient-stack-406610.iam.gserviceaccount.com",
          "universe_domain": "googleapis.com"
        },
            scopes=['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']
        )
    except exceptions.GoogleAuthError as e:
        print(f"Authentication failed: {e}")

    return credentials


schema_name = "raw_m31"
table_name = "layer2_current"
data = read_full_table(schema_name, table_name)



credentials = authenticate_google_sheets()
client = gspread.authorize(credentials)
spreadsheet = client.open("m31_Google_sheet_data_DePIN + Web3")


df = data.copy()
df = df.round(0)
df = df.astype(str)


sheet = spreadsheet.worksheet("test")

# Clear existing content in the sheet
# sheet.clear()

# Convert DataFrame to list of lists
data_list = [df.columns.values.tolist()] + df.values.tolist()

# Update the sheet with the DataFrame data
sheet.update(data_list)



l2_data_hist = all_data[['date', 'chain', 'tvl','tvl_90d_trend', 'mcap_usd','mcap_90d_trend', 'txn_cost_usd','txn_cost_90d_trend',
'monthly_active_adress','mau_90d_trend', 'transactions_30d','txn30_90d_trend', 'onchain_profit','onchain_profit_90d_trend',
'nft_sales_volume','NFT_sale_vol_90d_trend', 'dex_volume','dex_vol_90d_trend', 'tvl_market_share','tvl_mrkt_share_90d_trend']]



from datetime import datetime, timedelta

current_date = datetime.now()
date_90_days_ago = current_date - timedelta(days=92)

layer_2_hist = layer_2_data[layer_2_data['date'].astype('datetime64[ns]') >= date_90_days_ago]



provider = 'm31'
dataset = 'layer2_trend'
create_or_overwrite_table(layer_2_hist, provider, dataset)

import gspread
from google.auth import exceptions
from google.oauth2.service_account import Credentials


def authenticate_google_sheets():
    credentials = None

    try:
        credentials = Credentials.from_service_account_info(
        {
          "type": "service_account",
          "project_id": "ambient-stack-406610",
          "private_key_id": private_key_id,
          "private_key": "-----BEGIN PRIVATE KEY-----\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQCka1M1xiMk4uiX\n2clV7ijvPIoUfk8t4c4PTtIBSbDhEJH83gO3wK1CqdSDjjB6lNMQXiKNB5k8AXU1\n48qTAFwGWyEVwtX4LRLq6gXFWuprJe6W7gn7lKH8JWsjjnm0L7xf0Y3Bbp1ozoqD\noN/siJNqw9gK/86QqB2XTVkRrgPOWcVHSguajw7v013vR4uyNFB4PVMciGsMx0Rj\n8WdMqBhfG/gg/OwXUITFx9DBg5nogsHLdBNkJ2r6bD5bPvISXyBPKAu7aPu4b3xH\n+Bik5aHJnbxNLyhOk157vG8Hak0fG72aAXZ/50hdHfapSYkqx8BJ0VB8ZMOhJNPX\nlV0v72I7AgMBAAECggEAP3Pb7QjGT1nygYffF3aN/aXTdA066W4XY/j7OtwBkxod\n/QSBUszdELkR9qNNOkFtIwfxNZQVIv89CMscRpcA7MMGhatngBuFDXu7NmrbxPYi\nmcTLvXei+/hE3LgDZ/J0NFZe6qORw/zRn/LZ+CLNIYrrOXN3eIQox1dmZFhPx7Zm\nvj3DrC24eruVgWkYS6wAoWnNHRyZqPvFcE6kHpKOvnQHtwyJqyLObWcNaSryEdG5\nufr3gtS+ginbEEkpJ1JnQNIGXKpM/O06e5O/H6RE1iU1CTRUwWN3Yioz6W2fBBtR\nrh6hhhnkf27rR2J3v7DWQVM5c3Q6hqjZoCy7HckrAQKBgQDY8MQg0ZxAl86oqtVY\ncghY67m0ZCp2O5aHD32EeqLEuwgVjt6DCoLo4iEl0UBAXcP0TJUpFrZX9hJ1sYEC\nq2vUrjVYS4ijwfmli/00GIM/SR6RkW1tLTz+3EtebnILBucYoOhvmOYNH7s9FbzT\nPS3DUFQaVhgCNAFvhKwNVVDaWwKBgQDCBb8GawEXYMNRqLTTIGLnqeIXYf2cA0yH\nmkrJi02flaLdEufd8InRVIOeLs/jZJFr1/nS9rc5kmA1eIHg0mhbq2F7YCkfaPnV\nMAi2Rx3061yuIVM4yOlLHq01KUAT6/hXTd/nO+mFJ+SUgeEs8DotDSafOh8bR0oD\ngkHnp7ldoQKBgCwV0WVx8zTVJLP182ED21pmnNhhupdISLCtny461bTw5RWscN9k\nVXIJ8f6DZXEvHNEadv1gljGN2fZ82eC3ATS5KjIFN4E/vAG+Tvg1OwazTzj5uqkD\nFnAcSFyqSRagknnYySNUiPuFxUEGl9a9if0058JqWHqqItiMt4IGImYdAoGBAIDO\n+cwT/Ax+NA2heDL2PFNaiHxHlOwfkI4yE9aMAgOhfxdP9tl0WLq9Zgf9QgzP9m9n\nWjcBjhDNqcu17lvItHmvZK9Y3tQ4iCxNkGsa+bthCg2cmDiJwcAaZJl3gk/3h87G\nJ5DHSLgbDPi+5TRFZAoGwg5RstcUVAHSV1ipFDohAoGBAIZ/6oPxooI74VZUQfDD\nKxbJdN8xcsqT6CCdcHzvfLJ3W8fXNk/sI1FycYE0MM3mrIzW+TcGuyHToYfMVpfo\nDJ8lUq8jb97KY2SHa1gE36PFzpohkQS7GcFquE91Etp+ldvUX3ClXhB6VbKU4r9F\nac2RmwF0T+v6FD5DZ4m/iplb\n-----END PRIVATE KEY-----\n",
          "client_email": "hs-testing@ambient-stack-406610.iam.gserviceaccount.com",
          "client_id": "117454584926828343266",
          "auth_uri": "https://accounts.google.com/o/oauth2/auth",
          "token_uri": "https://oauth2.googleapis.com/token",
          "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
          "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/hs-testing%40ambient-stack-406610.iam.gserviceaccount.com",
          "universe_domain": "googleapis.com"
        },
            scopes=['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']
        )
    except exceptions.GoogleAuthError as e:
        print(f"Authentication failed: {e}")

    return credentials


schema_name = "raw_m31"
table_name = "layer2_trend"
data = read_full_table(schema_name, table_name)



credentials = authenticate_google_sheets()
client = gspread.authorize(credentials)
spreadsheet = client.open("m31_Google_sheet_data_DePIN + Web3")


df = data.copy()
df = df.round(0)
df = df.astype(str)


sheet = spreadsheet.worksheet("test")

# Clear existing content in the sheet
# sheet.clear()

# Convert DataFrame to list of lists
data_list = [df.columns.values.tolist()] + df.values.tolist()

# Update the sheet with the DataFrame data
sheet.update(data_list)

"""## Dashboard 3 Defi Rolls UP"""

layer3 = ['dydx', 'degate3','loopring', 'paradex','zkspace','immutablex','apex','aevo','reya','lyra']
chain = ['dydx-v3', 'DeGate V1','Loopring', 'Paradex','ZKSpace','Immutable X','Apex','Aevo','Reya','Lyra']

tvl_data = pd.DataFrame()
for i in range(len(chain)):
    try:
        headers = {
            'accept': '*/*',
            'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
            'cache-control': 'no-cache',
            'origin': 'https://l2beat.com',
            'pragma': 'no-cache',
            'priority': 'u=1, i',
            'referer': 'https://l2beat.com/scaling/projects/dydx',
            'sec-ch-ua': '"Not/A)Brand";v="8", "Chromium";v="126", "Google Chrome";v="126"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"macOS"',
            'sec-fetch-dest': 'empty',
            'sec-fetch-mode': 'cors',
            'sec-fetch-site': 'same-origin',
            'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
        }

        response = requests.get(f'https://l2beat.com/api/tvl/{layer3[i]}.json', headers=headers)
        data = response.json()['daily']['data']
        df = pd.DataFrame(data)
        df.columns = response.json()['daily']['types']
        df = df[['timestamp', 'totalUsd']]
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')
        df.columns = ['date', 'tvl']
        df['chain'] = chain[i]
        tvl_data = pd.concat([tvl_data, df], ignore_index=True)
    except:
        pass


print(tvl_data['date'].max())
print(tvl_data['chain'].nunique())
print(tvl_data['chain'].unique())
# tvl_data_cur = tvl_data[tvl_data['date'] == tvl_data['date'].max()]
# tvl_data_cur.drop(columns=['date'],inplace=True)
tvl_data['date'] = pd.to_datetime(tvl_data['date']).dt.date
tvl_data

#transaction count


transaction_count = pd.DataFrame()

for i in range(len(layer3)):
    try:
        headers = {
            'accept': '*/*',
            'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
            'cache-control': 'no-cache',
            'origin': 'https://l2beat.com',
            'pragma': 'no-cache',
            'priority': 'u=1, i',
            'referer': 'https://l2beat.com/scaling/projects/immutablex',
            'sec-ch-ua': '"Not/A)Brand";v="8", "Chromium";v="126", "Google Chrome";v="126"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"macOS"',
            'sec-fetch-dest': 'empty',
            'sec-fetch-mode': 'cors',
            'sec-fetch-site': 'same-origin',
            'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
        }

        response = requests.get(f'https://l2beat.com/api/activity/{layer3[i]}.json', headers=headers)
        data = response.json()['daily']['data']
        df = pd.DataFrame(data)
        df.columns = response.json()['daily']['types']
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')
        df = df[['timestamp', 'transactions']]
        df.columns = ['date', 'transactions']
        print(chain[i])
        df['chain'] = chain[i]
        transaction_count = pd.concat([transaction_count, df], ignore_index=True)
    except:
        pass

print(transaction_count['date'].max())
print(transaction_count['chain'].nunique())
print(transaction_count['chain'].unique())
transaction_count

q = """select
    date,
    chain,
    sum(transactions) over(partition by chain order by date rows between 29 preceding and current row) as transactions_30d
    from transaction_count
    """
transactions_30d = pysqldf(q)


print(transactions_30d['date'].max())
print(transactions_30d['chain'].nunique())
print(transactions_30d['chain'].unique())

# transactions_30d_cur = transactions_30d[transactions_30d['date'] == transactions_30d['date'].max()]
# transactions_30d_cur.drop(columns=['date'],inplace=True)
transactions_30d['date'] = pd.to_datetime(transactions_30d['date']).dt.date
transactions_30d

chain = ['dYdX-v3','apex','aevo','lyra','paradex']
chains = ['dydx-v3','Apex','Aevo','Lyra','Paradex']

dex_Volume = pd.DataFrame()
for i in range(len(chain)):
  headers = {
      'sec-ch-ua': '"Not/A)Brand";v="8", "Chromium";v="126", "Google Chrome";v="126"',
      'Referer': 'https://defillama.com/',
      'sec-ch-ua-mobile': '?0',
      'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
      'sec-ch-ua-platform': '"macOS"',
  }
  params = ''
  response = requests.get(f'https://api.llama.fi/summary/derivatives/{chain[i]}', params=params, headers=headers)
  data = response.json() ['totalDataChart']
  df = pd.DataFrame(data)
  df.rename(columns= ({0:'date', 1:'dex_Volumne'}), inplace = True)
  df['date'] = pd.to_datetime(df['date'], unit='s')
  df['chain'] = chains[i]
  dex_Volume = pd.concat([dex_Volume,df], ignore_index=True)



print(dex_Volume['date'].max())
print(dex_Volume['chain'].nunique())
print(dex_Volume['chain'].unique())
# dex_Volume_cur = dex_Volume[dex_Volume['date'] == dex_Volume['date'].max()]
# dex_Volume_cur.drop(columns=['date'],inplace=True)

dex_Volume['date'] = pd.to_datetime(dex_Volume['date']).dt.date
dex_Volume

#Stablecoin MCAP
import requests

stable_mcap = pd.DataFrame()
headers = {
    'accept': '*/*',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'cache-control': 'no-cache',
    'origin': 'https://www.growthepie.xyz',
    'pragma': 'no-cache',
    'priority': 'u=1, i',
    'referer': 'https://www.growthepie.xyz/',
    'sec-ch-ua': '"Not/A)Brand";v="8", "Chromium";v="126", "Google Chrome";v="126"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-site',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
}

response = requests.get('https://api.growthepie.xyz/v1/metrics/stables_mcap.json', headers=headers)
data = response.json()['data']['chains']
for i in data.keys():
    dataa = data[i]['daily']['data']
    df = pd.DataFrame(dataa)
    df.columns = ['date', 'mcap', 'mcapeth']
    df['date'] = df['date']/1000
    df['date'] = pd.to_datetime(df['date'], unit='s')
    df['chain'] = i
    stable_mcap = pd.concat([stable_mcap, df], ignore_index=True)

stable_mcap = stable_mcap[stable_mcap['chain'].isin(['loopring','imx'])]
stable_mcap.loc[stable_mcap['chain'] == 'imx', 'chain'] = 'Immutable X'
stable_mcap.loc[stable_mcap['chain'] == 'loopring', 'chain'] = 'Loopring'

print(stable_mcap['date'].max())
print(stable_mcap['chain'].nunique())
print(stable_mcap['chain'].unique())
stable_mcap['date'] = pd.to_datetime(stable_mcap['date']).dt.date
stable_mcap

# stable_mcap_cur = stable_mcap[stable_mcap['date'] == stable_mcap['date'].max()]
# stable_mcap_cur.drop(columns=['date','mcapeth'],inplace=True)

import datetime

all_data = pd.merge(tvl_data, transactions_30d, on=['date','chain'], how='outer')
all_data = pd.merge(all_data, dex_Volume, on=['date','chain'], how='outer')
all_data = pd.merge(all_data, stable_mcap, on=['date','chain'], how='outer')

all_data



# provider = 'm31'
# dataset = 'Defi_Rollups'

all_data.columns

# #calculating percentage change
def percent_change(df):
    for col in df:
        if col == 'tvl':
            df['tvl_7d_change'] = df.groupby('chain')['tvl'].pct_change(periods=7) * 100
            df['tvl_90d_trend'] = df.groupby('chain')['tvl'].pct_change(periods=90) * 100
        elif col == 'mcap':
            df['mcap_7d_change'] = df.groupby('chain')['mcap'].pct_change(periods=7) * 100
            df['mcap_90d_trend'] = df.groupby('chain')['mcap'].pct_change(periods=90) * 100
        elif col == 'dex_Volumne':
            df['dex_vol_7d_change'] = df.groupby('chain')['dex_Volumne'].pct_change(periods=7) * 100
            df['dex_vol_90d_trend'] = df.groupby('chain')['dex_Volumne'].pct_change(periods=90) * 100
        elif col == 'transactions_30d':
            df['txn_cost_7d_change'] = df.groupby('chain')['transactions_30d'].pct_change(periods=7) * 100
            df['txn_cost_90d_trend'] = df.groupby('chain')['transactions_30d'].pct_change(periods=90) * 100


percent_change(all_data)

q = """SELECT *,
avg(tvl) OVER(PARTITION BY chain ORDER BY date ASC ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS avg7D_tvl,
avg("mcap") OVER(PARTITION BY chain ORDER BY date ASC ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS avg7D_mcap,
avg(dex_Volumne) OVER (PARTITION BY chain ORDER BY date ASC ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS avg7D_dex_vol,
avg(transactions_30d) OVER(PARTITION BY chain ORDER BY date ASC ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS avg7D_txn_count

FROM all_data"""

final_data_df = pysqldf(q)
defi_rollups = final_data_df[final_data_df['date']<final_data_df['date'].max()]

defi_rollups_curr=defi_rollups[['date', 'chain','tvl','avg7D_tvl','tvl_7d_change','transactions_30d','avg7D_txn_count','txn_cost_7d_change','mcap','avg7D_mcap',
                 'mcap_7d_change','dex_Volumne','avg7D_dex_vol','dex_vol_7d_change']]

defi_rollups_curr = defi_rollups_curr[defi_rollups_curr['date'] == defi_rollups_curr['date'].max()]

provider = 'm31'
dataset = 'defi_rollups_curr'
create_or_overwrite_table(defi_rollups_curr, provider, dataset)

defi_rollups_curr

import gspread
from google.auth import exceptions
from google.oauth2.service_account import Credentials


def authenticate_google_sheets():
    credentials = None

    try:
        credentials = Credentials.from_service_account_info(
        {
          "type": "service_account",
          "project_id": "ambient-stack-406610",
          "private_key_id": private_key_id,
          "private_key": "-----BEGIN PRIVATE KEY-----\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQCka1M1xiMk4uiX\n2clV7ijvPIoUfk8t4c4PTtIBSbDhEJH83gO3wK1CqdSDjjB6lNMQXiKNB5k8AXU1\n48qTAFwGWyEVwtX4LRLq6gXFWuprJe6W7gn7lKH8JWsjjnm0L7xf0Y3Bbp1ozoqD\noN/siJNqw9gK/86QqB2XTVkRrgPOWcVHSguajw7v013vR4uyNFB4PVMciGsMx0Rj\n8WdMqBhfG/gg/OwXUITFx9DBg5nogsHLdBNkJ2r6bD5bPvISXyBPKAu7aPu4b3xH\n+Bik5aHJnbxNLyhOk157vG8Hak0fG72aAXZ/50hdHfapSYkqx8BJ0VB8ZMOhJNPX\nlV0v72I7AgMBAAECggEAP3Pb7QjGT1nygYffF3aN/aXTdA066W4XY/j7OtwBkxod\n/QSBUszdELkR9qNNOkFtIwfxNZQVIv89CMscRpcA7MMGhatngBuFDXu7NmrbxPYi\nmcTLvXei+/hE3LgDZ/J0NFZe6qORw/zRn/LZ+CLNIYrrOXN3eIQox1dmZFhPx7Zm\nvj3DrC24eruVgWkYS6wAoWnNHRyZqPvFcE6kHpKOvnQHtwyJqyLObWcNaSryEdG5\nufr3gtS+ginbEEkpJ1JnQNIGXKpM/O06e5O/H6RE1iU1CTRUwWN3Yioz6W2fBBtR\nrh6hhhnkf27rR2J3v7DWQVM5c3Q6hqjZoCy7HckrAQKBgQDY8MQg0ZxAl86oqtVY\ncghY67m0ZCp2O5aHD32EeqLEuwgVjt6DCoLo4iEl0UBAXcP0TJUpFrZX9hJ1sYEC\nq2vUrjVYS4ijwfmli/00GIM/SR6RkW1tLTz+3EtebnILBucYoOhvmOYNH7s9FbzT\nPS3DUFQaVhgCNAFvhKwNVVDaWwKBgQDCBb8GawEXYMNRqLTTIGLnqeIXYf2cA0yH\nmkrJi02flaLdEufd8InRVIOeLs/jZJFr1/nS9rc5kmA1eIHg0mhbq2F7YCkfaPnV\nMAi2Rx3061yuIVM4yOlLHq01KUAT6/hXTd/nO+mFJ+SUgeEs8DotDSafOh8bR0oD\ngkHnp7ldoQKBgCwV0WVx8zTVJLP182ED21pmnNhhupdISLCtny461bTw5RWscN9k\nVXIJ8f6DZXEvHNEadv1gljGN2fZ82eC3ATS5KjIFN4E/vAG+Tvg1OwazTzj5uqkD\nFnAcSFyqSRagknnYySNUiPuFxUEGl9a9if0058JqWHqqItiMt4IGImYdAoGBAIDO\n+cwT/Ax+NA2heDL2PFNaiHxHlOwfkI4yE9aMAgOhfxdP9tl0WLq9Zgf9QgzP9m9n\nWjcBjhDNqcu17lvItHmvZK9Y3tQ4iCxNkGsa+bthCg2cmDiJwcAaZJl3gk/3h87G\nJ5DHSLgbDPi+5TRFZAoGwg5RstcUVAHSV1ipFDohAoGBAIZ/6oPxooI74VZUQfDD\nKxbJdN8xcsqT6CCdcHzvfLJ3W8fXNk/sI1FycYE0MM3mrIzW+TcGuyHToYfMVpfo\nDJ8lUq8jb97KY2SHa1gE36PFzpohkQS7GcFquE91Etp+ldvUX3ClXhB6VbKU4r9F\nac2RmwF0T+v6FD5DZ4m/iplb\n-----END PRIVATE KEY-----\n",
          "client_email": "hs-testing@ambient-stack-406610.iam.gserviceaccount.com",
          "client_id": "117454584926828343266",
          "auth_uri": "https://accounts.google.com/o/oauth2/auth",
          "token_uri": "https://oauth2.googleapis.com/token",
          "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
          "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/hs-testing%40ambient-stack-406610.iam.gserviceaccount.com",
          "universe_domain": "googleapis.com"
        },
            scopes=['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']
        )
    except exceptions.GoogleAuthError as e:
        print(f"Authentication failed: {e}")

    return credentials


schema_name = "raw_m31"
table_name = "defi_rollups_curr"
data = read_full_table(schema_name, table_name)



credentials = authenticate_google_sheets()
client = gspread.authorize(credentials)
spreadsheet = client.open("m31_Google_sheet_data_DePIN + Web3")


df = data.copy()
df = df.round(0)
df = df.astype(str)


sheet = spreadsheet.worksheet("test")

# Clear existing content in the sheet
# sheet.clear()

# Convert DataFrame to list of lists
data_list = [df.columns.values.tolist()] + df.values.tolist()

# Update the sheet with the DataFrame data
sheet.update(data_list)

defi_rollups_hist = defi_rollups[['date', 'chain', 'tvl','tvl_90d_trend', 'mcap','mcap_90d_trend','transactions_30d','txn_cost_90d_trend',
              'dex_Volumne','dex_vol_90d_trend']]

defi_rollups_hist.sort_values(by='date',ascending=False,inplace=True)
from datetime import datetime, timedelta

current_date = datetime.now()
date_90_days_ago = current_date - timedelta(days=92)

defi_rollups_trend = defi_rollups_hist[defi_rollups_hist['date'].astype('datetime64[ns]') >= date_90_days_ago]


provider = 'm31'
dataset = 'defi_rollups_trend'
create_or_overwrite_table(defi_rollups_trend, provider, dataset)

import gspread
from google.auth import exceptions
from google.oauth2.service_account import Credentials


def authenticate_google_sheets():
    credentials = None

    try:
        credentials = Credentials.from_service_account_info(
        {
          "type": "service_account",
          "project_id": "ambient-stack-406610",
          "private_key_id": private_key_id,
          "private_key": "-----BEGIN PRIVATE KEY-----\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQCka1M1xiMk4uiX\n2clV7ijvPIoUfk8t4c4PTtIBSbDhEJH83gO3wK1CqdSDjjB6lNMQXiKNB5k8AXU1\n48qTAFwGWyEVwtX4LRLq6gXFWuprJe6W7gn7lKH8JWsjjnm0L7xf0Y3Bbp1ozoqD\noN/siJNqw9gK/86QqB2XTVkRrgPOWcVHSguajw7v013vR4uyNFB4PVMciGsMx0Rj\n8WdMqBhfG/gg/OwXUITFx9DBg5nogsHLdBNkJ2r6bD5bPvISXyBPKAu7aPu4b3xH\n+Bik5aHJnbxNLyhOk157vG8Hak0fG72aAXZ/50hdHfapSYkqx8BJ0VB8ZMOhJNPX\nlV0v72I7AgMBAAECggEAP3Pb7QjGT1nygYffF3aN/aXTdA066W4XY/j7OtwBkxod\n/QSBUszdELkR9qNNOkFtIwfxNZQVIv89CMscRpcA7MMGhatngBuFDXu7NmrbxPYi\nmcTLvXei+/hE3LgDZ/J0NFZe6qORw/zRn/LZ+CLNIYrrOXN3eIQox1dmZFhPx7Zm\nvj3DrC24eruVgWkYS6wAoWnNHRyZqPvFcE6kHpKOvnQHtwyJqyLObWcNaSryEdG5\nufr3gtS+ginbEEkpJ1JnQNIGXKpM/O06e5O/H6RE1iU1CTRUwWN3Yioz6W2fBBtR\nrh6hhhnkf27rR2J3v7DWQVM5c3Q6hqjZoCy7HckrAQKBgQDY8MQg0ZxAl86oqtVY\ncghY67m0ZCp2O5aHD32EeqLEuwgVjt6DCoLo4iEl0UBAXcP0TJUpFrZX9hJ1sYEC\nq2vUrjVYS4ijwfmli/00GIM/SR6RkW1tLTz+3EtebnILBucYoOhvmOYNH7s9FbzT\nPS3DUFQaVhgCNAFvhKwNVVDaWwKBgQDCBb8GawEXYMNRqLTTIGLnqeIXYf2cA0yH\nmkrJi02flaLdEufd8InRVIOeLs/jZJFr1/nS9rc5kmA1eIHg0mhbq2F7YCkfaPnV\nMAi2Rx3061yuIVM4yOlLHq01KUAT6/hXTd/nO+mFJ+SUgeEs8DotDSafOh8bR0oD\ngkHnp7ldoQKBgCwV0WVx8zTVJLP182ED21pmnNhhupdISLCtny461bTw5RWscN9k\nVXIJ8f6DZXEvHNEadv1gljGN2fZ82eC3ATS5KjIFN4E/vAG+Tvg1OwazTzj5uqkD\nFnAcSFyqSRagknnYySNUiPuFxUEGl9a9if0058JqWHqqItiMt4IGImYdAoGBAIDO\n+cwT/Ax+NA2heDL2PFNaiHxHlOwfkI4yE9aMAgOhfxdP9tl0WLq9Zgf9QgzP9m9n\nWjcBjhDNqcu17lvItHmvZK9Y3tQ4iCxNkGsa+bthCg2cmDiJwcAaZJl3gk/3h87G\nJ5DHSLgbDPi+5TRFZAoGwg5RstcUVAHSV1ipFDohAoGBAIZ/6oPxooI74VZUQfDD\nKxbJdN8xcsqT6CCdcHzvfLJ3W8fXNk/sI1FycYE0MM3mrIzW+TcGuyHToYfMVpfo\nDJ8lUq8jb97KY2SHa1gE36PFzpohkQS7GcFquE91Etp+ldvUX3ClXhB6VbKU4r9F\nac2RmwF0T+v6FD5DZ4m/iplb\n-----END PRIVATE KEY-----\n",
          "client_email": "hs-testing@ambient-stack-406610.iam.gserviceaccount.com",
          "client_id": "117454584926828343266",
          "auth_uri": "https://accounts.google.com/o/oauth2/auth",
          "token_uri": "https://oauth2.googleapis.com/token",
          "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
          "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/hs-testing%40ambient-stack-406610.iam.gserviceaccount.com",
          "universe_domain": "googleapis.com"
        },
            scopes=['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']
        )
    except exceptions.GoogleAuthError as e:
        print(f"Authentication failed: {e}")

    return credentials


schema_name = "raw_m31"
table_name = "defi_rollups_trend"
data = read_full_table(schema_name, table_name)



credentials = authenticate_google_sheets()
client = gspread.authorize(credentials)
spreadsheet = client.open("m31_Google_sheet_data_DePIN + Web3")


df = data.copy()
df = df.round(0)
df = df.astype(str)


sheet = spreadsheet.worksheet("test")

# Clear existing content in the sheet
# sheet.clear()

# Convert DataFrame to list of lists
data_list = [df.columns.values.tolist()] + df.values.tolist()

# Update the sheet with the DataFrame data
sheet.update(data_list)

y=final_data_df[['date', 'chain','avg7D_tvl','tvl_7d_change','avg7D_txn_count','txn_cost_7d_change','avg7D_mcap','mcap_7d_change', 'avg7D_dex_vol','dex_vol_7d_change']]

y.to_csv('roll_up2.csv')

import requests

headers = {
    'accept': '*/*',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'cache-control': 'no-cache',
    'content-type': 'application/json',
    'pragma': 'no-cache',
    'priority': 'u=1, i',
    'referer': 'https://l2beat.com/scaling/tvl',
    'sec-ch-ua': '"Not)A;Brand";v="99", "Google Chrome";v="127", "Chromium";v="127"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-origin',
    'trpc-accept': 'application/jsonl',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36',
    'x-trpc-source': 'nextjs-react',
}

params = {
    'batch': '1',
    'input': '{"0":{"json":{"range":"max","excludeAssociatedTokens":false,"type":"layer2"}}}',
}

response = requests.get('https://l2beat.com/api/trpc/scaling.summary.chart', params=params, headers=headers)
data = response.text
import json

# Assuming 'data' contains multiple JSON objects separated by a delimiter (e.g., newline)
data_objects = data.split('\n')  # Split into individual JSON objects

parsed_data = []
for obj in data_objects:
    if obj:  # Check if the object is not empty
        try:
            parsed_data.append(json.loads(obj))
        except json.JSONDecodeError as e:
            print(f"Error parsing object: {obj}, Error: {e}")

parse_data = parsed_data[3]['json'][2][0]
df = pd.DataFrame(parse_data[0])
df.columns = ['date', 'tvl1', 'tvl2','tvl3','tvl4']
df['date'] = pd.to_datetime(df['date'],unit='s')
df.drop(columns=['tvl4'],inplace=True)
df['tvl'] = df.drop(columns=['date']).sum(axis=1)
df = df[['date','tvl']]
df['chain'] = 'Ethereum'
df['tvl'] = df['tvl']/100
df



"""<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=acd04261-284b-42ed-9c40-7dfba8e13a4d' target="_blank">
<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>
Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>
"""