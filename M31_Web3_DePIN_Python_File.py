# -*- coding: utf-8 -*-
"""M31_relative_metrics (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zeeuEy0qiazHrJMOFEIdatlvzJLop31d
"""

from google.colab import userdata
import os
os.environ['AWS_ACCESS_KEY_ID'] = userdata.get('AWS_ACCESS_KEY_ID')
os.environ['AWS_SECRET_ACCESS_KEY'] = userdata.get('AWS_SECRET_ACCESS_KEY')
os.environ['FLIPSIDE_PRO_SF_PASSWORD'] = userdata.get('FLIPSIDE_PRO_SF_PASSWORD')
os.environ['COINGECKO_PRO_PASSWORD'] = userdata.get('COINGECKO_PRO_PASSWORD')

!pip install awswrangler
!pip install pandasql
import awswrangler as wr
import pandas as pd

import requests

import math
from lxml import etree
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings("ignore")
import boto3
from datetime import datetime, timedelta, date
!pip install openpyxl
from pandasql import sqldf
pysqldf = lambda q: sqldf(q, globals())


def read_full_table(schema, table_name):
    return wr.athena.read_sql_table(table=table_name, database=schema)


import time

boto3.setup_default_session(region_name="us-east-1")
wr.s3.does_object_exist("s3://noaa-ghcn-pds/fake")
def get_database(provider):
    return f"raw_{provider}"

def delete_table(provider, dataset):
    # Delete existing table (when needed)
    wr.catalog.delete_table_if_exists(database=get_database(provider), table=dataset)

def create_or_overwrite_table(df, provider, dataset):
    database = get_database(provider)

    # Get the list of existing databases
    existing_databases = wr.catalog.get_databases()

    # Check if the database exists, create it if it doesn't
    if database not in existing_databases:
        wr.catalog.create_database(name=database, exist_ok=True)

    BUCKET = "holocron-raw-us-east-1-368603383413"
    DATASETS_PATH = "notebook_datasets"

    # Create/Overwrite the table
    return wr.s3.to_parquet(
        df=df,
        path=f's3://{BUCKET}/{DATASETS_PATH}/{provider}/{dataset}',
        dataset=True,
        mode="overwrite",
        compression="gzip",
        partition_cols=[],
        database=database,  # Athena/Glue database
        table=dataset  # Athena/Glue table
    )


# Modify the function to handle different data types appropriately and to ignore NaN values
def floor_except_first_two_cols(df):
    for col in df.columns:
        if col not in ['date','project','price_change_30d','fee_change_7d','volume_change_7d','tvs_change_7d','capacity_change_7d','used_storage_change_7d']:
            df[col] = df[col].apply(lambda x: math.floor(x) if isinstance(x, (int, float)) and pd.notnull(x) else x)
    return df


API_KEY = os.getenv('COINGECKO_PRO_PASSWORD', '')
headers = {
    'content-type': "application/json"
}

API_URL = "https://pro-api.coingecko.com/api/v3"
headers["x-cg-pro-api-key"] = API_KEY

print(os.environ.get("ENVIRONMENT", "local") == "local")
print(API_URL)
print("*********",headers)

class CoingeckoClient:

    REQUESTS_PER_MINUTE = 490

    def __init__(self):
        self.request_count = 0
        self.start_time = time.time()

    def make_request(self, url):
        c = 0
        response_json = None
        while c < 5 and response_json is None:
            try:
                response = requests.get(url, headers=headers)
                response.raise_for_status()
                self.update_rate_limit()
                response_json = response.json()
                return response_json
            except Exception as e:
                logging.exception(f"Error while Fetching data: {url}")
                c += 1
                sleep = (2 ** random.uniform(0, 1))
                time.sleep(sleep)
                if c == 5:
                    raise e
        return response_json

    def wait_for_rate_limit(self):
        elapsed_time = time.time() - self.start_time
        if elapsed_time < 60 and self.request_count >= CoingeckoClient.REQUESTS_PER_MINUTE:
            time_to_wait = 60 - elapsed_time
            time.sleep(time_to_wait)
            self.request_count = 0
        self.start_time = time.time()

    def update_rate_limit(self):
        self.request_count += 1

    def get_coins(self):
        url = API_URL + "/coins/list?include_platform=true"
        self.wait_for_rate_limit()
        response_json = self.make_request(url)
        return response_json

    def get_coingecko_price_data(self, id, days):
        template_url = API_URL + "/coins/{}/market_chart?vs_currency=usd&days={}&interval=daily"
        url = template_url.format(id, days)
        self.wait_for_rate_limit()
        response_json = self.make_request(url)
        return response_json


import numpy as np

def floor_except_first_two_columns(df):
    # Iterate over columns starting from the third column
    for col in df.columns:
        if col not in ['date','project','totalCPU_change_7d','activeCPU_change_7d', 'totalGPU_change_7d', 'activeGPU_change_7d']:
            # Ignore infinite values and then perform the floor operation
            df[col] = np.where(np.isfinite(df[col]), np.floor(df[col]), df[col])
    return df

# Assuming dashboard3_compute_data is your dataframe
# dashboard3_compute_data = floor_except_first_two_cols(dashboard3_compute_data)
import datetime

current_datetime = datetime.datetime.now()
days_to_subtract = 0
new_date = current_datetime - datetime.timedelta(days=days_to_subtract)
new_date_pd = pd.to_datetime(new_date)
date_only = new_date_pd.date()
date_string = date_only.strftime('%Y-%m-%d')

proxies = {
    'http': 'http://scraperapi:8a457df49571f250d536be0990207d40@proxy-server.scraperapi.com:8001',
    'https': 'http://scraperapi:8a457df49571f250d536be0990207d40@proxy-server.scraperapi.com:8001',
}

sheet_id = '16lXG8a-RY8vDK3Gcy4Wo-xaWLW1u09c0o5q_yXRQPnY'
xls = pd.ExcelFile(f"https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=xlsx", engine='openpyxl')
token_id = pd.read_excel(xls, 'coingecko_id', header = 0)
token_id

token_list = ['uma',
'api3',
'switch-token',
'dia-data',
'pyth-network',
'band-protocol',
'connext',
'stargate-finance',
'hop-protocol',
'synapse-2',
'chainlink',
'across-protocol',
'allbridge', 'clore-ai',
'render-token',
'golem',
'threefold-token',
'akash-network',
'zelcash',
'aleph',
'cudos',
'internet-computer', 'siacoin',
'filecoin',
'lambda',
'arweave',
'bittorrent',
'storj']

len(token_list)

client = CoingeckoClient()
x=client.get_coins()
coins_df=pd.DataFrame(x)
coins_list=coins_df['id'].tolist()

coins_list=token_list
market_data_df=pd.DataFrame()
print(coins_list)

for i in range(len(coins_list)):
    if i%50==0:
        print(i)
    # print("*********")
    try:
        coin_data=client.get_coingecko_price_data(coins_list[i], 'max')
        coin_data_prices=coin_data['prices']
        coin_data_market_caps=coin_data['market_caps']
        coin_data_total_volumes=coin_data['total_volumes']

        coin_data_prices_df=pd.DataFrame(coin_data_prices)
        coin_data_market_caps_df=pd.DataFrame(coin_data_market_caps)
        coin_data_total_volumes_df=pd.DataFrame(coin_data_total_volumes)
        coin_data_prices_df.columns=['date','prices']
        coin_data_market_caps_df.columns=['date','market_cap']
        coin_data_total_volumes_df.columns=['date','volume']

        merged = pd.merge(coin_data_prices_df, coin_data_market_caps_df, on='date', how='inner')
        coin_data_df = pd.merge(merged,coin_data_total_volumes_df, on='date', how='inner')
        coin_data_df['token']=coins_list[i]

        coin_data_df=coin_data_df[['date','token','prices','market_cap','volume']]

        market_data_df=pd.concat([market_data_df,coin_data_df],ignore_index=True)
    except:
        pass

print(1)
market_data_df['date']=market_data_df['date']/1000
print(2)

market_data_df["date"] = pd.to_datetime(market_data_df["date"], unit="s")
market_data_df['date'] = market_data_df['date'].dt.to_period('D').dt.to_timestamp()


market_data_df.drop_duplicates(subset=['date','token'], keep='first', inplace=True)
market_data_df


coins_df = coins_df[['id', 'name']]
map_token_list = coins_df[coins_df['id'].isin(token_list)]
map_token_list

#calculating 30 day price change
market_data_df = market_data_df.sort_values(by=['token', 'date'])
market_data_df['price_change_30d'] = market_data_df.groupby('token')['prices'].pct_change(periods=30) * 100
market_data_df




#mapping coin name

q = """select A.*, B.name from market_data_df A
inner join map_token_list B
on A.token = B.id """
final_market_data = pysqldf(q)
final_market_data


#Selecting latest date data
final_market_data = final_market_data[final_market_data['date']==final_market_data['date'].max()]
final_market_data.reset_index(drop=True, inplace=True)
final_market_data

#Calculating total supply

total_supply_data1 = pd.DataFrame()
for i in range(len(token_list)):
    headers = {
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
        'cache-control': 'max-age=0',
        'priority': 'u=0, i',
        'referer': 'https://www.coingecko.com/en/coins/api3',
        'sec-ch-ua': '\"Chromium\";v=\"124\", \"Google Chrome\";v=\"124\", \"Not-A.Brand\";v=\"99\"',
        'sec-ch-ua-arch': '\"arm\"',
        'sec-ch-ua-bitness': '\"64\"',
        'sec-ch-ua-full-version': '\"124.0.6367.61\"',
        'sec-ch-ua-full-version-list': '\"Chromium\";v=\"124.0.6367.61\", \"Google Chrome\";v=\"124.0.6367.61\", \"Not-A.Brand\";v=\"99.0.0.0\"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-model': '\"\"',
        'sec-ch-ua-platform': '\"macOS\"',
        'sec-ch-ua-platform-version': '\"13.3.0\"',
        'sec-fetch-dest': 'document',
        'sec-fetch-mode': 'navigate',
        'sec-fetch-site': 'same-origin',
        'sec-fetch-user': '?1',
        'upgrade-insecure-requests': '1',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
    }

    response = requests.get(f'https://www.coingecko.com/en/coins/{token_list[i]}',  headers=headers, proxies=proxies, verify=False)
    # print(f'https://www.coingecko.com/en/coins/{token_list[i]}')
    tree = etree.HTML(response.text)
    data = tree.xpath('//*[@id="gecko-coin-page-container"]/div[6]/div[2]/table/tbody/tr[3]/td/span/text()')

    total_supply = data[0] if data else '0'
    temp = pd.DataFrame({'token': [token_list[i]], 'fdmcap': [total_supply]})
    total_supply_data1 = pd.concat([total_supply_data1, temp], ignore_index=True)

total_supply_data1 =total_supply_data1[total_supply_data1['fdmcap'] != '0']

total_supply_data1['fdmcap'] = total_supply_data1['fdmcap'].str.replace('$', '').str.replace(',', '').astype(float)


# #Calculating total supply
token_list1 = ['api3','stargate-finance', 'hop-protocol', 'synapse-2', 'across-protocol', 'allbridge']
total_supply_data = pd.DataFrame()
for i in range(len(token_list1)):
    headers = {
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
        'cache-control': 'max-age=0',
        'priority': 'u=0, i',
        'referer': 'https://www.coingecko.com/en/coins/api3',
        'sec-ch-ua': '\"Chromium\";v=\"124\", \"Google Chrome\";v=\"124\", \"Not-A.Brand\";v=\"99\"',
        'sec-ch-ua-arch': '\"arm\"',
        'sec-ch-ua-bitness': '\"64\"',
        'sec-ch-ua-full-version': '\"124.0.6367.61\"',
        'sec-ch-ua-full-version-list': '\"Chromium\";v=\"124.0.6367.61\", \"Google Chrome\";v=\"124.0.6367.61\", \"Not-A.Brand\";v=\"99.0.0.0\"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-model': '\"\"',
        'sec-ch-ua-platform': '\"macOS\"',
        'sec-ch-ua-platform-version': '\"13.3.0\"',
        'sec-fetch-dest': 'document',
        'sec-fetch-mode': 'navigate',
        'sec-fetch-site': 'same-origin',
        'sec-fetch-user': '?1',
        'upgrade-insecure-requests': '1',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
    }

    response = requests.get(f'https://www.coingecko.com/en/coins/{token_list1[i]}',  headers=headers, proxies=proxies, verify=False)
    print(f'https://www.coingecko.com/en/coins/{token_list1[i]}')
    tree = etree.HTML(response.text)
    data = tree.xpath('//*[@id="gecko-coin-page-container"]/div[6]/div[2]/table/tbody/tr[4]/td/span/text()')
    print(response)
    total_supply = data[0] if data else '0'
    temp = pd.DataFrame({'token': [token_list1[i]], 'fdmcap': [total_supply]})
    total_supply_data = pd.concat([total_supply_data, temp], ignore_index=True)

total_supply_data =total_supply_data[total_supply_data['fdmcap'] != '0']

total_supply_data['fdmcap'] = total_supply_data['fdmcap'].str.replace('$', '').str.replace(',', '').astype(float)



fully_diluted_mcap = pd.concat([total_supply_data, total_supply_data1], ignore_index=True)
fully_diluted_mcap = fully_diluted_mcap.drop(fully_diluted_mcap[fully_diluted_mcap['token'] == 'threefold-token'].index)
fully_diluted_mcap

import requests


headers = {
    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'cache-control': 'max-age=0',
    # 'cookie': '_cc_id=775f68c8131637f213faf7dbc28f380c; _au_1d=AU1D-0100-001700044659-GEQ6FTCC-4MAD; cookie_notice_accept=1; _au_last_seen_pixels=eyJhcG4iOjE3MDQyNjA0NzAsInR0ZCI6MTcwNDI2MDQ3MCwicHViIjoxNzA0MjYwNDcwLCJydWIiOjE3MDQyNjA0NzAsInRhcGFkIjoxNzA0MjYwNDcwLCJhZHgiOjE3MDQyNjA0NzAsImdvbyI6MTcwNDI2MDQ3MCwic21hcnQiOjE3MDQyNjA1MDUsImltcHIiOjE3MDQyNjA1MDcsInNvbiI6MTcwNDI2MDQ3MCwiYmVlcyI6MTcwNDI2MDUwNSwiY29sb3NzdXMiOjE3MDQyNjA1MDUsImFtbyI6MTcwNDI2MDUwNSwiYWRvIjoxNzA0MjYwNTA1LCJvcGVueCI6MTcwNDI2MDUwNSwidW5ydWx5IjoxNzA0MjYwNTA1LCJ0YWJvb2xhIjoxNzA0MjYwNTA1LCJpbmRleCI6MTcwNDI2MDQ3MCwicHBudCI6MTcwNDI2MDUwNX0%3D; _ga=GA1.1.1762277819.1700044653; _uc_referrer=https://www.google.com/; optimize_uuid=2cf9a0aa221593149b1004353dfa2f657c3ed62f2d17864275; _gcl_au=1.1.36717941.1707894211; __cuid=8f6542c1dcba4a45a5ce9bf8d12db9ef; amp_fef1e8=17dd2c29-3767-433e-9310-f09fc37f80a6R...1hn3dhunj.1hn3eid8f.3.0.3; _ga_4PE9HX30YZ=GS1.1.1708438123.1.1.1708439795.0.0.0; _cq_duid=1.1712128835.57Q9ClfFwSZDfVQA; selected_iso=usd; remember_user_token=eyJfcmFpbHMiOnsibWVzc2FnZSI6Ilcxc3hNemd5TXpNME0xMHNJaVF5WVNReE1DUlllVWhzYVc1MmJXOVpUblJDWmxJeGQyRkdkMkpQSWl3aU1UY3hNemM0TkRRNE5pNDBOVGd6TXpVMklsMD0iLCJleHAiOiIyMDI0LTA1LTIwVDExOjE0OjQ2LjQ1OFoiLCJwdXIiOiJjb29raWUucmVtZW1iZXJfdXNlcl90b2tlbiJ9fQ%3D%3D--8c76870ee463c26710551fb09f3b9046a94894e2; __stripe_mid=970cd071-beb4-4222-9bc5-bf823d4d9523a2a913; hubspotutk=767356041fa3f4d93a81809594ef3054; _cq_suid=1.1714373337.d3nyeLS2oG74tpah; cf_clearance=22Hp44swBk52JqlF1Fgif2H8muGpq2ppFWfxDD.J1ko-1714374363-1.0.1.1-V5IAU1l_.ckZ9U.03TihNVNRMPvDzXCojRoxRGmboz32xUUUZx2oI5R0Br1Ja.yAf.q4NdXPs308CrbaxzafqQ; __hstc=234611265.767356041fa3f4d93a81809594ef3054.1713784524510.1713784524510.1714388686002.2; __hssrc=1; _hjSessionUser_3096373=eyJpZCI6IjJiNDAzNmQ0LTM5ZDQtNWQ0NC04NmNlLThkNjY1N2QzNDA1NiIsImNyZWF0ZWQiOjE3MTQzODg2ODUzMDAsImV4aXN0aW5nIjp0cnVlfQ==; mp_4e762b32525bd7ecddda43ce51776343_mixpanel=%7B%22distinct_id%22%3A%20%22%24device%3A18cc8eef9757c1-0fd151e51af53f-1f525637-157188-18cc8eef9757c1%22%2C%22%24device_id%22%3A%20%2218cc8eef9757c1-0fd151e51af53f-1f525637-157188-18cc8eef9757c1%22%2C%22%24search_engine%22%3A%20%22google%22%2C%22%24initial_referrer%22%3A%20%22https%3A%2F%2Fwww.google.com%2F%22%2C%22%24initial_referring_domain%22%3A%20%22www.google.com%22%2C%22__mps%22%3A%20%7B%7D%2C%22__mpso%22%3A%20%7B%22%24initial_referrer%22%3A%20%22https%3A%2F%2Fwww.google.com%2F%22%2C%22%24initial_referring_domain%22%3A%20%22www.google.com%22%7D%2C%22__mpus%22%3A%20%7B%7D%2C%22__mpa%22%3A%20%7B%7D%2C%22__mpu%22%3A%20%7B%7D%2C%22__mpr%22%3A%20%5B%5D%2C%22__mpap%22%3A%20%5B%5D%7D; _session_id=b657903aa0299419ebfec9f92041b6fc; OptanonConsent=isGpcEnabled=0&datestamp=Mon+May+13+2024+11%3A27%3A29+GMT%2B0530+(India+Standard+Time)&version=202312.1.0&browserGpcFlag=0&isIABGlobal=false&hosts=&consentId=4ccc8ca6-aa1d-4bba-8c6e-c9245bf79771&interactionCount=1&landingPath=NotLandingPage&groups=C0001%3A1%2CC0002%3A1%2CC0004%3A1%2CC0003%3A1&AwaitingReconsent=false; _ga_1Y6C78JXR3=GS1.1.1715579420.181.1.1715579849.0.0.0; __gads=ID=9ab8fa3c042baf1e:T=1700044657:RT=1715580058:S=ALNI_MZ6qNOWtibSp2iM267SxkxqrS1YGA; __gpi=UID=00000c87cc192efd:T=1700044657:RT=1715580058:S=ALNI_MbNOh8E2v_n1PZaHUvetMCp_lBFoA; __eoi=ID=1a3aa11a5e6c73f8:T=1706855080:RT=1715580058:S=AA-Afjb1_FkUCVAU4vf9nalz3tjC; _ga_LJR3232ZPB=GS1.1.1715579420.265.1.1715580088.0.0.0',
    'priority': 'u=0, i',
    'referer': 'https://www.coingecko.com/en/coins/api3',
    'sec-ch-ua': '"Chromium";v="124", "Google Chrome";v="124", "Not-A.Brand";v="99"',
    'sec-ch-ua-arch': '"arm"',
    'sec-ch-ua-bitness': '"64"',
    'sec-ch-ua-full-version': '"124.0.6367.79"',
    'sec-ch-ua-full-version-list': '"Chromium";v="124.0.6367.79", "Google Chrome";v="124.0.6367.79", "Not-A.Brand";v="99.0.0.0"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-model': '""',
    'sec-ch-ua-platform': '"macOS"',
    'sec-ch-ua-platform-version': '"13.3.0"',
    'sec-fetch-dest': 'document',
    'sec-fetch-mode': 'navigate',
    'sec-fetch-site': 'same-origin',
    'sec-fetch-user': '?1',
    'upgrade-insecure-requests': '1',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
}

response = requests.get('https://www.coingecko.com/en/coins/threefold',headers=headers, proxies=proxies,verify=False)
tree = etree.HTML(response.text)
data = tree.xpath('//*[@id="gecko-coin-page-container"]/div[6]/div[2]/table/tbody/tr[2]/td/span/text()')
print(response)
temp = pd.DataFrame({'token': ['threefold-token'], 'fdmcap': data})
temp['fdmcap'] = temp['fdmcap'].str.replace('$', '').str.replace(',', '').astype(float)
fully_diluted_mcap = pd.concat([fully_diluted_mcap, temp], ignore_index=True)
fully_diluted_mcap

final_market_data = pd.merge(final_market_data, fully_diluted_mcap, on='token', how='outer')
final_market_data

token_list.extend(['RedStone', 'TWAP', 'ChainlinkCCIP', 'DLN', 'IONET','Store Cloud'])
ticker = ['UMA','API3','Switchboard','DIA','Pyth', 'Band', 'Connext','Stargate', 'Hop','Synapse',
'Chainlink', 'Across', 'Allbridge', 'CLORE.AI', 'RNDR', 'GLM','TFT','AKT','FLUX','ALEPH', 'CUDOS','ICP',
'SC', 'FIL','LAMB','AR','BTT','STORJ','RedStone', 'TWAP', 'Chainlink_CCIP', 'DLN', 'io.net','Store Cloud']


name = ['UMA','API3','Switch Token', 'DIA','Pyth Network','Band Protocol','Connext','Stargate Finance','Hop Protocol','Synapse',
'Chainlink','Across Protocol','Allbridge','Clore.ai','Render','Golem','ThreeFold','Akash Network','Flux','Aleph.im',
'Cudos','Internet Computer','Siacoin','Filecoin', 'Lambda','Arweave','BitTorrent','Storj','RedStone', 'TWAP', 'ChainlinkCCIP', 'DLN', 'IONET', 'Store Cloud']
# name

token_df = pd.DataFrame(({'token': token_list, 'ticker': ticker, 'name': name}))
token_df = token_df.merge(token_id, on='name', how='left')

print(len(token_list))
print(len(ticker))
print(len(name))

# final_market_data.drop(columns=['name'], inplace=True)
final_market_data_df = pd.merge(final_market_data,token_df, on='token', how='right')

# final_market_data_df = pd.merge(token_df, final_market_data, on='token', how='left')
final_market_data_df

market_data = final_market_data_df[['ticker', 'name_y', 'market_cap','fdmcap','price_change_30d', 'logo id']]
market_data

market_data['fdmcap'] = market_data['fdmcap']/10**6
market_data['market_cap'] = market_data['market_cap']/10**6
market_data.to_csv('market_data1.csv', index=False)
market_data

market_data = pd.read_csv('market_data1.csv')
market_data

## Oracle

tvs = { 'Switchboard' : '$2.185b', 'TWAP' : '$982.52m', 'UMA' : '$299.84m', 'RedStone' : '$2.656b', 'DIA' : '$26.46m', 'Band' : '$69.33m', 'API3' : '$1.058b', 'Chainlink' : '$31.776b', 'Pyth' : '$6.547b'}
tvs_df = pd.DataFrame([tvs])
tvs_df = tvs_df.transpose().reset_index()
tvs_df.columns = ['project', 'total_value_secured']
tvs_df

import requests
owner_list = ["smartcontractkit", "pyth-network", "RedStoneCoin", "api3dao", "switchboard-xyz", "diadata-org", "UMAprotocol", "bandprotocol", "orbs-network"]
repo_list = ["chainlink", "pyth-crosschain", "redstone-rs", "airnode", "switchboard", "diadata", "protocol", "chain", "twap"]
projectlst = ['Chainlink','Pyth', 'RedStone', 'API3', 'Switchboard', 'DIA', 'UMA', 'Band', 'TWAP']


github_star = pd.DataFrame()

for i in range(len(owner_list)):
    headers = {"Authorization": "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"}
    stars_response = requests.get(f"https://api.github.com/repos/{owner_list[i]}/{repo_list[i]}", headers=headers)
    stars = stars_response.json()["stargazers_count"]
    watchers = stars_response.json()["subscribers_count"]
    temp = pd.DataFrame([[projectlst[i], stars, watchers]], columns=['project', 'stars', 'watchers'])
    github_star = pd.concat([github_star, temp], ignore_index=True)
    # print(stars, watchers)

github_star

lst1 = ['Pyth Network', 'Redstone Finance', 'API3', 'DIA', 'Band Protocol', 'Switchboard']

project_lst = ['Pyth', 'RedStone', 'API3', 'DIA', 'Band', 'Switchboard']

commits_df = pd.DataFrame()
for i in range(len(lst1)):
    url = 'https://api.gokustats.xyz/weekly-commits/?daysBack={}&ecosystem={}'.format(90, lst1[i])
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        print(project_lst[i])
        temp = pd.DataFrame(data)
        temp.columns = ['date', 'weekly_commits']
        # temp = temp.rename(columns={'% Core': 'commits_core', 'Sub-Ecosystems': 'commits_core_sub'})
        temp['project'] = project_lst[i]
        commits_df = pd.concat([commits_df, temp], ignore_index=True)
        # print(data)
    else:
        print('Failed to retrieve data. Status code:', response.status_code)

lst2 = ['Chainlink', 'UMA']
commits_df_1 = pd.DataFrame()
for i in range(len(lst2)):
    url = 'https://api.gokustats.xyz/weekly-commits/?daysBack={}&ecosystem={}'.format(90, lst2[i])
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        temp = pd.DataFrame(data)
        temp.columns = ['date', 'weekly_commits', 'commits_core_sub']
        # temp = temp.rename(columns={'% Core': 'commits_core', 'Sub-Ecosystems': 'commits_core_sub'})
        temp['project'] = lst2[i]
        temp.drop(columns=['commits_core_sub'], inplace=True)
        commits_df_1 = pd.concat([commits_df_1, temp], ignore_index=True)

    else:
        print('Failed to retrieve data. Status code:', response.status_code)

weekly_commit_data = pd.concat([commits_df, commits_df_1], ignore_index=True)

lsts1 = ['Pyth Network', 'Redstone Finance', 'API3', 'DIA', 'Band Protocol', 'Switchboard']
project_lst = ['Pyth', 'RedStone', 'API3', 'DIA', 'Band', 'Switchboard']


active_devs = pd.DataFrame()

for i in range(len(lsts1)):

    import requests

    headers = {
    'sec-ch-ua': '"Google Chrome";v="123", "Not:A-Brand";v="8", "Chromium";v="123"',
    'Referer': 'https://app.artemis.xyz/',
    'sec-ch-ua-mobile': '?0',
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
    'sec-ch-ua-platform': '"macOS"',
    }

    params = {
    'ecosystem': f'{lsts1[i]}',
    'includeForks': 'false',
    'daysBack': '90',
    }

    response = requests.get('https://api.artemisxyz.com/weekly-active-devs/', params=params, headers=headers)

    if response.status_code == 200:
        data = response.json()
        print(project_lst[i])
        temp = pd.DataFrame(data)
        temp.columns = ['date', 'weekly_devs']
        temp['project'] = project_lst[i]
        active_devs = pd.concat([active_devs, temp], ignore_index=True)
    else:
        print('Failed to retrieve data. Status code:', response.status_code)


active_devs_1 = pd.DataFrame()

lsts2 = ['Chainlink', 'UMA']
for i in range(len(lsts2)):

    headers = {
    'sec-ch-ua': '"Google Chrome";v="123", "Not:A-Brand";v="8", "Chromium";v="123"',
    'Referer': 'https://app.artemis.xyz/',
    'sec-ch-ua-mobile': '?0',
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
    'sec-ch-ua-platform': '"macOS"',
    }

    params = {
    'ecosystem': f'{lsts2[i]}',
    'includeForks': 'false',
    'daysBack': '90',
    }

    response = requests.get('https://api.artemisxyz.com/weekly-active-devs/', params=params, headers=headers, proxies = proxies, verify=False)
    # url = 'https://api.gokustats.xyz/weekly-active-devs/?daysBack={}&ecosystem={}'.format(90, lsts2[i])
    # requests.get(url)
    if response.status_code == 200:
        data = response.json()
        temp = pd.DataFrame(data)
        temp.columns = ['date', 'weekly_devs', 'core_devs']
        temp = temp[['date', 'weekly_devs']]
        temp['project'] = lsts2[i]
        active_devs_1 = pd.concat([active_devs_1, temp], ignore_index=True)
    else:
        print('Failed to retrieve data. Status code:', response.status_code)

active_devs_data = pd.concat([active_devs, active_devs_1], ignore_index=True)

weekly_commit_dev_data = pd.merge(weekly_commit_data, active_devs_data, how='inner', on=['date', 'project'])
weekly_commit_dev_data

#transaction value enabled
url = 'https://chain.link/'
response = requests.get(url, proxies=proxies, verify=False)
if response.status_code == 200:
    tree = etree.HTML(response.text)
    rows = tree.xpath('//*[@id="stats"]/div/div/div/div/div/div/div/div[1]/div[2]/text()')
    tve_data = pd.DataFrame({'transactions_value_enabled': [int(rows[0].replace(',', ''))]})
    tve_data['project'] = 'Chainlink'
else:
    print('Failed to retrieve data. Status code:', response.status_code)

tve_data

from datetime import datetime

#combining total value secured data and stars and watchers data
total_value_secured_data = pd.merge(tvs_df, github_star, how='right', on=['project'])

#combining weekly commit data and active devs
total_value_secured_data = pd.merge(total_value_secured_data, weekly_commit_dev_data, how='left', on=['project'])

# Drop duplicates from 'project' column keeping the one with max date
total_value_secured_data['date'] = pd.to_datetime(total_value_secured_data['date'])
total_value_secured_data = total_value_secured_data.sort_values(by='date')
total_value_secured_data = total_value_secured_data.drop_duplicates(subset='project', keep='last')

#combining transaction value enabled data
total_value_secured_data = pd.merge(total_value_secured_data, tve_data, how='left', on=['project'])

total_value_secured_data['date'] = str(datetime.now().date())
total_value_secured_data['transactions_value_enabled'] = total_value_secured_data['transactions_value_enabled']/10**9
dashboad1_oracle = total_value_secured_data[['date', 'project', 'total_value_secured', 'transactions_value_enabled', 'stars', 'watchers', 'weekly_commits', 'weekly_devs']]
dashboad1_oracle

q = """select B.date, A.name_y as project, A.market_cap, A.fdmcap, A.price_change_30d,
B.total_value_secured, B.transactions_value_enabled, B.stars, B.watchers, B.weekly_commits, B.weekly_devs, A.'logo id' as logo_id
from dashboad1_oracle B
inner join market_data A
on A.ticker=B.project"""
dashboard1_oracle = pysqldf(q)
dashboard1_oracle

def convert_value(value):
    if pd.isnull(value):
        return None
    if 'b' in value:
        return float(value.replace('$', '').replace('b', '')) * 1000  # Convert billion to million
    elif 'm' in value:
        return float(value.replace('$', '').replace('m', ''))  # No conversion needed for million
    else:
        return None

dashboard1_oracle['total_value_secured'] = dashboard1_oracle['total_value_secured'].apply(convert_value)

previous_data = read_full_table('raw_m31', 'oracle_his_data')
previous_data = previous_data[previous_data['date']==previous_data['date'].max()]
# previous_data.rename(columns={'total_value_secured': 'total_value_secured_previous'}, inplace=True)

dashboard1_oracle_copy = dashboard1_oracle.copy()

q = """select A.*, B.total_value_secured as total_value_secured_previous  from dashboard1_oracle_copy A
join previous_data B
on A.project = B.project"""

dashboard1_oracle_data = pysqldf(q)
dashboard1_oracle_data

# Fix the issue by converting 'total_value_secured' and 'total_value_secured_previous' to floats before performing operations.
def convert_value(value):
    if pd.isnull(value):
        return None
    if isinstance(value, str):
        if 'b' in value:
            return float(value.replace('$', '').replace('b', '')) * 1000  # Convert billion to million
        elif 'm' in value:
            return float(value.replace('$', '').replace('m', ''))  # No conversion needed for million
        else:
            return float(value.replace('$', '').replace(',', ''))
    return float(value)

dashboard1_oracle_data['total_value_secured'] = dashboard1_oracle_data['total_value_secured'].apply(convert_value)
dashboard1_oracle_data['total_value_secured_previous'] = dashboard1_oracle_data['total_value_secured_previous'].apply(convert_value)

dashboard1_oracle_data['tvs_change_7d'] = ((dashboard1_oracle_data['total_value_secured'] - dashboard1_oracle_data['total_value_secured_previous'])/dashboard1_oracle_data['total_value_secured_previous'])*100
dashboard1_oracle_data.drop(columns=['total_value_secured_previous'], inplace=True)
dashboard1_oracle_data

# def convert_value(value):
#     if pd.isnull(value):
#         return None
#     if 'b' in value:
#         return float(value.replace('$', '').replace('b', '')) * 1000  # Convert billion to million
#     elif 'm' in value:
#         return float(value.replace('$', '').replace('m', ''))  # No conversion needed for million
#     else:
#         return None

# Apply function to the column
# dashboard1_oracle['total_value_secured'] = dashboard1_oracle['total_value_secured'].apply(convert_value)

# Apply the modified function to the DataFrame
dashboard1_oracle_data = floor_except_first_two_cols(dashboard1_oracle_data)
dashboard1_oracle_data.to_csv('oracle.csv', index=False)


# dashboard1_oracle = pd.concat([ dashboard1_oracle, new_df], ignore_index=True)

dashboard1_oracle_data['date'] = pd.to_datetime(dashboard1_oracle_data['date'])

provider = 'm31'
dataset = 'oracle'

create_or_overwrite_table(dashboard1_oracle_data, provider, dataset)
dashboard1_oracle_data

# Debugging the ArrowTypeError by checking the types of columns in historrical_data_oracle before concatenation
# and ensuring all are in the expected format (specifically, to string where relevant).

def ensure_string_columns(df, columns):
    for col in columns:
        if col in df.columns:
            df[col] = df[col].astype(str)
    return df

# Reading the existing table
historrical_data_oracle = read_full_table('raw_m31', 'oracle_his_data')

# Ensure specific columns are in string format in both DataFrames before concatenation
columns_to_stringify = ['date', 'project', 'total_value_secured', 'transactions_value_enabled', 'logo_id']
dashboard1_oracle_data = ensure_string_columns(dashboard1_oracle_data, columns_to_stringify)
historrical_data_oracle = ensure_string_columns(historrical_data_oracle, columns_to_stringify)

# Concatenating the DataFrames
historrical_data_oracle = pd.concat([dashboard1_oracle_data, historrical_data_oracle], ignore_index=True)

# Check and ensure all relevant columns are still in the correct format in the combined DataFrame
historrical_data_oracle = ensure_string_columns(historrical_data_oracle, columns_to_stringify)

# Defining provider and dataset
provider = 'm31'
dataset = 'oracle_his_data'

# Creating or overwriting the table
create_or_overwrite_table(historrical_data_oracle, provider, dataset)

historrical_data_oracle

"""## Dashboard 2

"""

import requests

#30 days fee


headers = {
    'x-tt-terminal-jwt': 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJmcm9udEVuZCI6InRlcm1pbmFsIGRhc2hib2FyZCIsImlhdCI6MTcxMjEwMzg2OSwiZXhwIjoxNzEzMzEzNDY5fQ.flKoggJZ-PijYwjBlDrXLmSXaWpyzhNUaWVKbzff0ac',
    'sec-ch-ua': '"Google Chrome";v="123", "Not:A-Brand";v="8", "Chromium";v="123"',
    'x-app-version': '1712144848071+9aa7e157',
    'sec-ch-ua-mobile': '?0',
    'authorization': 'your-token-terminal-key',
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
    'Referer': 'https://tokenterminal.com/',
    'sec-ch-ua-platform': '"macOS"',
}

params = {
    'project_slugs': 'stargate,across,hop-protocol,synapse,allbridge,dln,connext',
    'interval': '180d',
}

response = requests.get('https://api.tokenterminal.com/v3/internal/charts/metrics/fees', params=params, headers=headers)
print(response)

if response.status_code == 200:
    data = response.json()['data']
    fee_data = pd.DataFrame(data)
    fee_data
else:
    print('Failed to retrieve data. Status code:', response.status_code)

fee_data.rename(columns={'api_id': 'project', 'timestamp': 'date', 'value': 'fee_30d'}, inplace=True)
fee_data['date'] = pd.to_datetime(fee_data['date']).dt.date
# fee_data['30d_moving_sum'] = fee_data.groupby('project')['fee_30d'].transform(lambda x: x.rolling(window=30).sum())
fee_data_copy = fee_data.copy()

q = """SELECT
    project,
    date,
    SUM(fee_30d) OVER (PARTITION BY project ORDER BY date ROWS BETWEEN 29 PRECEDING AND CURRENT ROW) AS fee_30d
FROM
fee_data_copy
"""

fee_data_df = pysqldf(q)

fee_data_df = fee_data_df.copy()
fee_data_df['fee_change_7d'] = fee_data_df.groupby('project')['fee_30d'].pct_change(periods=7)
#
fee_data_df

# import requests

# headers = {
# }

# params = {
#     'limit': '1000',
# }

# response = requests.get('https://api.dune.com/api/v1/query/3625459/results', params=params, headers=headers)
# data = response.json()
# data
# ['result']['rows']
# chainlink_fee = pd.DataFrame(data)
# chainlink_fee.rename(columns={'reward_usd':'fee_30d'}, inplace=True)
# chainlink_fee['project'] = 'chainlink_CCIP'
# chainlink_fee['date'] = fee_data_df['date'].max()
# chainlink_fee = chainlink_fee[['date', 'project', 'fee_30d']]
# chainlink_fee

# chainlink_fee['fee_change_7d'] = chainlink_fee.loc[chainlink_fee['project'] == 'chainlink_CCIP', 'fee_change_7d'] = ((217596.4611 - 217596) / 217596) * 100
# chainlink_fee.to_csv('chainlink_fee.csv')
# # combining all fee data
# fee_data_df = pd.concat([fee_data_df, chainlink_fee], ignore_index=True)
# # fee_data_df['fee_change_7d'] = fee_data_df.groupby('project')['fee_30d'].pct_change(periods=7)

chainlink_fee = pd.DataFrame({'date':['2024-07-14'],'project':['chainlink_CCIP'],'fee_30d':[229596.4611],'fee_change_7d':[0.0]})
fee_data_df = pd.concat([fee_data_df, chainlink_fee], ignore_index=True)
fee_data_df

url = 'https://api.gokustats.xyz/weekly-commits/?daysBack=90&ecosystem=Chainlink'
response = requests.get(url)
data = response.json()
dff = pd.DataFrame(data)
dff = dff[['date', 'Chainlink Core']]
dff.columns = ['date', 'weekly_commits']
dff['project'] = 'chainlink_CCIP'
dff

#weekly commits

lsts = ['Synapse Protocol', 'Across Protocol', 'Allbridge', 'Connext Network', 'Hop Protocol', 'Tellor']
project_lst = ['synapse', 'across', 'allbridge', 'connext', 'hop-protocol', 'tellor']

weekly_commit_data = pd.DataFrame()
for i in range(len(lsts)):
    url = 'https://api.gokustats.xyz/weekly-commits/?daysBack={}&ecosystem={}'.format(90, lsts[i])
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        temp = pd.DataFrame(data)
        temp['project'] = project_lst[i]
        temp.columns = ['date', 'weekly_commits', 'project']
        weekly_commit_data = pd.concat([weekly_commit_data, temp], ignore_index=True)
    else:
        print('Failed to retrieve data. Status code:', response.status_code)



url = 'https://api.gokustats.xyz/weekly-commits/?daysBack=90&ecosystem=Chainlink'
response = requests.get(url)
data = response.json()
dff = pd.DataFrame(data)
dff = dff[['date', 'Chainlink Core']]
dff.columns = ['date', 'weekly_commits']
dff['project'] = 'chainlink_CCIP'


weekly_commit_data = pd.concat([weekly_commit_data, dff], ignore_index=True)







#weekly active devs

lsts1 = ['Synapse Protocol', 'Across Protocol', 'Allbridge', 'Connext Network', 'Hop Protocol', 'Tellor']
project_lst = ['synapse', 'across', 'allbridge', 'connext', 'hop-protocol', 'tellor']

weekly_dev_data = pd.DataFrame()
for i in range(len(lsts1)):
    url = 'https://api.gokustats.xyz/weekly-active-devs/?daysBack={}&ecosystem={}'.format(90, lsts1[i])
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        temp = pd.DataFrame(data)
        temp['project'] = project_lst[i]
        temp.columns = ['date', 'weekly_devs', 'project']
        weekly_dev_data = pd.concat([weekly_dev_data, temp], ignore_index=True)
    else:
        print('Failed to retrieve data. Status code:', response.status_code)



headers = {
'sec-ch-ua': '"Google Chrome";v="123", "Not:A-Brand";v="8", "Chromium";v="123"',
'Referer': 'https://app.artemis.xyz/',
'sec-ch-ua-mobile': '?0',
'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
'sec-ch-ua-platform': '"macOS"',
}

params = {
'ecosystem': 'Chainlink',
'includeForks': 'false',
'daysBack': '90',
}

response = requests.get('https://api.artemisxyz.com/weekly-active-devs/', params=params, headers=headers)
data = response.json()
df = pd.DataFrame(data)
df = df[['date', 'Chainlink Core']]
df.columns = ['date', 'weekly_devs']
df['project'] = 'chainlink_CCIP'
df

weekly_dev_data = pd.concat([weekly_dev_data, df], ignore_index=True)





weekly_commit_dev_data = pd.merge(weekly_commit_data, weekly_dev_data, how='inner', on=['date', 'project'])
weekly_commit_dev_data

headers = {
'sec-ch-ua': '"Google Chrome";v="123", "Not:A-Brand";v="8", "Chromium";v="123"',
'Referer': 'https://app.artemis.xyz/',
'sec-ch-ua-mobile': '?0',
'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
'sec-ch-ua-platform': '"macOS"',
}

params = {
'ecosystem': 'Chainlink',
'includeForks': 'false',
'daysBack': '90',
}

response = requests.get('https://api.artemisxyz.com/weekly-active-devs/', params=params, headers=headers)
data = response.json()
df = pd.DataFrame(data)
df = df[['date', 'Chainlink Core']]
df.columns = ['date', 'active_devs']
df['project'] = 'chainlink_CCIP'
df

import requests

#weekly watchers and stars

owner_list = ["matrix-org", "stargate", "allbridge-io", "debridge-finance", "connext", "hop-protocol", "across-protocol", "smartcontractkit"]
repo_list = ["synapse", "stargate",  "allbridge-core-js-sdk", "dln-taker","monorepo", "hop", "contracts-v2","chainlink"]
projectlst = ['synapse', 'stargate',  'allbridge', 'dln', 'connext', 'hop-protocol', 'across', 'chainlink_CCIP']


github_star = pd.DataFrame()

for i in range(len(owner_list)):
    headers = {"Authorization": "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"}
    stars_response = requests.get(f"https://api.github.com/repos/{owner_list[i]}/{repo_list[i]}", headers=headers)
    print(projectlst[i])
    stars = stars_response.json()["stargazers_count"]
    watchers = stars_response.json()["subscribers_count"]
    temp = pd.DataFrame([[projectlst[i], stars, watchers]], columns=['project', 'stars', 'watchers'])
    github_star = pd.concat([github_star, temp], ignore_index=True)
    # print(stars, watchers)

github_star

dashboard2 = pd.merge(fee_data_df, github_star, how='left', on=['project'])
dashboard2 = dashboard2[dashboard2['date']== dashboard2['date'].max()]
dashboard2.reset_index(drop=True, inplace=True)
weekly_commit_dev_data =weekly_commit_dev_data[weekly_commit_dev_data['date']== weekly_commit_dev_data['date'].max()]
dashboard2 = pd.merge(dashboard2, weekly_commit_dev_data, how='left', on=['project'])
dashboard2.drop(columns=['date_y'], inplace=True)
dashboard2.rename(columns={'date_x': 'date'}, inplace=True)

dashboard2_bridges = dashboard2[['date', 'project', 'fee_30d','fee_change_7d', 'stars', 'watchers', 'weekly_commits', 'weekly_devs']]
dashboard2_bridges

dashboard2_bridges['project'] = dashboard2_bridges['project'].str.capitalize()
dashboard2_bridges

# Update values in the 'project' column where the value is 'Dln' to 'DLN'
dashboard2_bridges.loc[dashboard2_bridges['project'] == 'Dln', 'project'] = 'DLN'
dashboard2_bridges.loc[dashboard2_bridges['project'] == 'Chainlink_ccip', 'project'] = 'Chainlink_CCIP'
dashboard2_bridges.loc[dashboard2_bridges['project'] == 'Hop-protocol', 'project'] = 'Hop'
dashboard2_bridges

import requests

headers = {
    'sec-ch-ua': '"Google Chrome";v="123", "Not:A-Brand";v="8", "Chromium";v="123"',
    'Referer': 'https://defillama.com/',
    'sec-ch-ua-mobile': '?0',
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
    'sec-ch-ua-platform': '"macOS"',
}

response = requests.get('https://bridges.llama.fi/bridges', headers=headers)
data = response.json()['bridges']
bridge_volume = pd.DataFrame(data)
bridge_volume = bridge_volume[['displayName','monthlyVolume' ]]
bridge_volume.columns = ['project', 'volume_30d']
bridge_volume['volume_30d'] = bridge_volume['volume_30d'].astype(float)/10**6
bridge_volume

dashboard2_bridges = dashboard2_bridges.merge(bridge_volume, on='project', how='left')
dashboard2_bridges

q = """select B.date, A.name_y as project, A.market_cap, A.fdmcap, A.price_change_30d,
B.fee_30d,B.fee_change_7d, B.stars, B.watchers, B.weekly_commits, B.weekly_devs, B.volume_30d, A.'logo id' as logo_id
from dashboard2_bridges B
inner join market_data A
on A.ticker=B.project"""
dashboard2_bridges = pysqldf(q)
dashboard2_bridges

previous_data_bridge = read_full_table('raw_m31', 'bridges_hist_data')
previous_data_bridge = previous_data_bridge[previous_data_bridge['date'] == previous_data_bridge['date'].max()]
previous_data_bridge

# # reading previous data
# previous_data_bridge = pd.read_csv('Bridges.csv')
# previous_data_bridge

dashboard2_bridges_copy = dashboard2_bridges.copy()

q = """select A.*, B.volume_30d as volume_30d_previous from dashboard2_bridges_copy A
join previous_data_bridge B
on A.project = B.project"""

dashboard2_bridges_data = pysqldf(q)
dashboard2_bridges_data

dashboard2_bridges_data['volume_change_7d'] = ((dashboard2_bridges_data['volume_30d'] - dashboard2_bridges_data['volume_30d_previous'])/dashboard2_bridges_data['volume_30d_previous'])*100
dashboard2_bridges_data

dashboard2_bridges_data = floor_except_first_two_cols(dashboard2_bridges_data)
dashboard2_bridges_data.to_csv('Bridges.csv', index=False)

dashboard2_bridges_data['date'] = pd.to_datetime(dashboard2_bridges_data['date'])

provider = 'm31'
dataset = 'bridges'

create_or_overwrite_table(dashboard2_bridges_data, provider, dataset)

dashboard2_bridges_data

bridge_hist_data = read_full_table('raw_m31', 'bridges_hist_data')
bridge_hist_data = pd.concat([bridge_hist_data, dashboard2_bridges_data], ignore_index=True)
bridge_hist_data

provider = 'm31'
dataset = 'bridges_hist_data'

create_or_overwrite_table(bridge_hist_data, provider, dataset)

bridge_hist_data

"""##   dashboard 3"""

import requests
df = pd.read_csv('/content/sample_data/flux_data (1).csv')
headers = {
    'accept': '*/*',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'content-type': 'application/x-www-form-urlencoded',
    'origin': 'https://bunnyanalyst.com',
    'referer': 'https://bunnyanalyst.com/',
    'sec-ch-ua': '"Google Chrome";v="123", "Not:A-Brand";v="8", "Chromium";v="123"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'cross-site',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
    'x-client-data': 'CI+2yQEIprbJAQipncoBCI7iygEIk6HLAQia/swBCIagzQEYj87NARjYhs4B',
    'x-client-version': 'Chrome/JsCore/9.9.2/FirebaseCore-web',
    'x-firebase-gmpid': '1:668131552956:web:a1c426d069a7a5e805aab9',
}

params = {
    'key': 'AIzaSyBdH2ApSbKiM6Gpo813gFIP0qNVxdw-3eQ',
}

data = {
    'grant_type': 'refresh_token',
    'refresh_token': f"{df['refresh_token'].iloc[0]}",
}

response = requests.post('https://securetoken.googleapis.com/v1/token', params=params, headers=headers, data=data)
response

data = response.json()
data_df = pd.DataFrame([data])
data_df.to_csv('flux_data.csv', index=False)
access_token = data['access_token']
user_id = data['user_id']

import requests

headers = {
    'accept': 'application/json, text/plain, */*',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'origin': 'https://bunnyanalyst.com',
    'referer': 'https://bunnyanalyst.com/',
    'sec-ch-ua': '"Google Chrome";v="123", "Not:A-Brand";v="8", "Chromium";v="123"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'cross-site',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
}

params = {
    'uid': f'{user_id}',
    'token': f'{access_token}',
}

response = requests.get(
    'https://bunny-analyst-api-server-prod-osjo.onrender.com/api/get_flux_top_revenue_metrics',
    params=params,
    headers=headers,
)


data = response.json()['result']
data
response

#cpu
cpu_data = data['cpu_utilization_graph']['data'][1:]  # Skip the header row
cpu_data_df = pd.DataFrame(cpu_data)
cpu_data_df.columns = ['date', 'leased_cpu', 'used_cpu']
cpu_data_df['date'] = cpu_data_df['date'].astype(int)/1000
cpu_data_df['date'] = pd.to_datetime(cpu_data_df['date'], unit='s')
cpu_data_df['totalCPU'] = cpu_data_df['leased_cpu'] + cpu_data_df['used_cpu']
cpu_data_df.rename(columns={'leased_cpu': 'activeCPU'}, inplace=True)
cpu_data_df = cpu_data_df[['date','activeCPU', 'totalCPU']]
cpu_data_df

#storage
hdd_data = data['hdd_utilization_graph']['data'][1:]  # Skip the header row
hdd_data_df = pd.DataFrame(hdd_data)
hdd_data_df.columns = ['date', 'leased_storage', 'used_storage']
hdd_data_df['date'] = hdd_data_df['date'].astype(int)/1000
hdd_data_df['date'] = pd.to_datetime(hdd_data_df['date'], unit='s')
hdd_data_df['totalStorage'] = hdd_data_df['leased_storage'] + hdd_data_df['used_storage']
# hdd_data_df.rename(columns={'leased_cpu': 'activeCPU'}, inplace=True)
hdd_data_df = hdd_data_df[['date','totalStorage']]
hdd_data_df

#memory data
memory_data = data['ram_utilization_graph']['data'][1:]  # Skip the header row
memory_data_df = pd.DataFrame(memory_data)
memory_data_df.columns = ['date', 'leased_memory', 'used_memory']
memory_data_df['date'] = memory_data_df['date'].astype(int)/1000
memory_data_df['date'] = pd.to_datetime(memory_data_df['date'], unit='s')
memory_data_df['totalMemory'] = memory_data_df['leased_memory'] + memory_data_df['used_memory']
memory_data_df = memory_data_df[['date','totalMemory']]
memory_data_df


flux_data = memory_data_df.merge(cpu_data_df, on='date', how='inner').merge(hdd_data_df, on='date', how='inner')
flux_data['project'] = 'flux'
flux_data = flux_data[flux_data['date']==flux_data['date'].max()]
flux_data

import requests

headers = {
    'accept': 'application/json, text/javascript, */*; q=0.01',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'origin': 'https://leaderboard.fluxcore.ai',
    'referer': 'https://leaderboard.fluxcore.ai/',
    'sec-ch-ua': '"Google Chrome";v="123", "Not:A-Brand";v="8", "Chromium";v="123"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-site',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
}

response = requests.get('https://service.fluxcore.ai/api/getLeaderBoardStats', headers=headers)
data = response.json()
flux_gpu = pd.DataFrame([data])
flux_gpu = flux_gpu[['TotalGPU']]
flux_gpu.columns = ['totalGPU']
flux_gpu['project'] = 'flux'
flux_gpu


flux_data = flux_data.merge(flux_gpu, on='project', how='inner')
flux_data

#Akash network
import requests

headers = {
    'accept': 'application/json, text/plain, */*',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'origin': 'https://stats.akash.network',
    'referer': 'https://stats.akash.network/',
    'sec-ch-ua': '"Google Chrome";v="123", "Not:A-Brand";v="8", "Chromium";v="123"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'cross-site',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
}

response = requests.get('https://api.cloudmos.io/v1/dashboard-data', headers=headers)

data = response.json()['networkCapacity']
# data = response.json()['networkCapacity']
akash_network = pd.DataFrame([data])
akash_network.drop(columns=['activeProviderCount','pendingCPU','activeMemory','activeStorage', 'pendingGPU','pendingMemory','pendingStorage','availableCPU','availableGPU','availableMemory','availableStorage'], inplace=True)
akash_network['totalMemory'] = akash_network['totalMemory']/(10**12)
akash_network['totalStorage'] = akash_network['totalStorage']/(10**12)
akash_network['activeCPU'] = akash_network['activeCPU']/(1000)
akash_network['totalCPU'] = akash_network['totalCPU']/(1000)
akash_network['project'] = 'akash-network'

akash_network

# # import requests
# # refres_token = pd.read_csv('refresh_token.csv')
# # headers = {
# #     'accept': '*/*',
# #     'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
# #     'apikey': 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Im14dGZka3BweHlmbG1tZ2x1bGxmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3MDgwNDI1ODEsImV4cCI6MjAyMzYxODU4MX0.mNkDiJaCBB5twRNypzThEKl-s8d5VjasNyJj1l9BK9o',
# #     'authorization': 'Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Im14dGZka3BweHlmbG1tZ2x1bGxmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3MDgwNDI1ODEsImV4cCI6MjAyMzYxODU4MX0.mNkDiJaCBB5twRNypzThEKl-s8d5VjasNyJj1l9BK9o',
# #     'content-type': 'application/json;charset=UTF-8',
# #     'origin': 'https://cloud.io.net',
# #     'referer': 'https://cloud.io.net/',
# #     'sec-ch-ua': '\"Google Chrome\";v=\"123\", \"Not:A-Brand\";v=\"8\", \"Chromium\";v=\"123\"',
# #     'sec-ch-ua-mobile': '?0',
# #     'sec-ch-ua-platform': '\"macOS\"',
# #     'sec-fetch-dest': 'empty',
# #     'sec-fetch-mode': 'cors',
# #     'sec-fetch-site': 'same-site',
# #     'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
# #     'x-client-info': 'supabase-js-web/2.39.2',
# # }

# # params = {
# #     'grant_type': 'refresh_token',
# # }

# # json_data = {
# #     'refresh_token': f"{refres_token['refresh_token'][0]}",
# # }

# # response = requests.post('https://id.io.net/auth/v1/token', params=params, headers=headers, json=json_data)

# import requests

# import requests

# headers = {
#     'accept': 'application/json, text/plain, */*',
#     'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
#     'origin': 'https://explorer.io.net',
#     'priority': 'u=1, i',
#     'referer': 'https://explorer.io.net/',
#     'sec-ch-ua': '"Chromium";v="124", "Google Chrome";v="124", "Not-A.Brand";v="99"',
#     'sec-ch-ua-mobile': '?0',
#     'sec-ch-ua-platform': '"macOS"',
#     'sec-fetch-dest': 'empty',
#     'sec-fetch-mode': 'cors',
#     'sec-fetch-site': 'cross-site',
#     'token': 'eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6Ink2SUlqV1JidlBKR1pIRzdCYV9rYSJ9.eyJodHRwczovL2lvLm5ldC91c2VyIjp7ImlvX2lkIjoiMGFiODUwN2YtZDM3My00Y2IwLWI5MDEtMjMxMmJhZWU2YmM4IiwicGhvbmVfdmVyaWZpZWQiOmZhbHNlLCJwcmltYXJ5X21haWwiOiJhcmp1bi5wYXRoYWtAcHlvci54eXoifSwiaXNzIjoiaHR0cHM6Ly90ZXN0LXVzZXJzLW1pZ3JhdGlvbi51cy5hdXRoMC5jb20vIiwic3ViIjoiYXV0aDB8NjYyNzlhMzRlZDdlMTVhYjU2NzYwMGU0IiwiYXVkIjpbImh0dHBzOi8vdGVzdC11c2Vycy1taWdyYXRpb24udXMuYXV0aDAuY29tL2FwaS92Mi8iLCJodHRwczovL3Rlc3QtdXNlcnMtbWlncmF0aW9uLnVzLmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE3MTU1ODI3MjEsImV4cCI6MTcxNzMxMDcyMSwic2NvcGUiOiJvcGVuaWQgcHJvZmlsZSBlbWFpbCB1cGRhdGU6Y3VycmVudF91c2VyX21ldGFkYXRhIHJlYWQ6Y3VycmVudF91c2VyIG9mZmxpbmVfYWNjZXNzIiwiYXpwIjoicnY5amkzb2o0YnRzSWljVXNya0hyN0JzdkNSaHV3SGIifQ.E_S-U39sMmdiiHggcOMDvNArF53AsllZoZOnzCA3WX33vH38c1KY4TDeV491uudQCB_I1QFaKhKWsuz2ENJ5ygHgvE0gZlZUwHNobBup7EsFOgFaoGTenjnS3QvHHLP2rqRNf0rhxSyY_-Bb4RKR_fBkuYkzZGJs44h5_dtp-Pr3FMRn4n2_vWQiJV0Hwmw0gW2w1zin1RFXM_YTw69guVK5INgvnXvjhAjkEkBkz46O3GRyAzsahVFWXEEs9lr_1SDuysEuljBRDK9Gta8vl1GThqF0_biIrS6w3oLwTVTxeYgrB-Bbi8c_ptDjq6UFQhaqdTMM4cOcmTWJ0Efvrg',
#     'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
# }

# response = requests.get('https://api.io.solutions/v1/io-explorer/network/info', headers=headers)
# data = response.json()['access_token']
# refresh_token = response.json()['refresh_token']
# df = pd.DataFrame([{'refresh_token': refresh_token}])
# df.to_csv('refresh_token.csv', index=False)

# #Render

# # headers = {
# #     'sec-ch-ua': '"Google Chrome";v="123", "Not:A-Brand";v="8", "Chromium";v="123"',
# #     'Accept': 'application/json, text/plain, */*',
# #     'Referer': 'https://cloud.io.net/',
# #     'sec-ch-ua-mobile': '?0',
# #     'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
# #     'Token': f'{data}',
# #     'sec-ch-ua-platform': '"macOS"',
# # }
# # response = requests.get('https://api.io.solutions/v1/io-explorer/network/info', headers=headers)

# import requests

# headers = {
#     'accept': 'application/json, text/plain, */*',
#     'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
#     'cache-control': 'no-cache',
#     'origin': 'https://cloud.io.net',
#     'pragma': 'no-cache',
#     'priority': 'u=1, i',
#     'referer': 'https://cloud.io.net/',
#     'sec-ch-ua': '"Google Chrome";v="125", "Chromium";v="125", "Not.A/Brand";v="24"',
#     'sec-ch-ua-mobile': '?0',
#     'sec-ch-ua-platform': '"macOS"',
#     'sec-fetch-dest': 'empty',
#     'sec-fetch-mode': 'cors',
#     'sec-fetch-site': 'cross-site',
#     'token': 'eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6Ink2SUlqV1JidlBKR1pIRzdCYV9rYSJ9.eyJodHRwczovL2lvLm5ldC91c2VyIjp7ImNyZWF0ZWQiOmZhbHNlLCJpb19pZCI6IjBhYjg1MDdmLWQzNzMtNGNiMC1iOTAxLTIzMTJiYWVlNmJjOCIsInByaW1hcnlfbWFpbCI6ImFyanVuLnBhdGhha0BweW9yLnh5eiJ9LCJpc3MiOiJodHRwczovL3Rlc3QtdXNlcnMtbWlncmF0aW9uLnVzLmF1dGgwLmNvbS8iLCJzdWIiOiJhdXRoMHw2NjI3OWEzNGVkN2UxNWFiNTY3NjAwZTQiLCJhdWQiOlsiaHR0cHM6Ly90ZXN0LXVzZXJzLW1pZ3JhdGlvbi51cy5hdXRoMC5jb20vYXBpL3YyLyIsImh0dHBzOi8vdGVzdC11c2Vycy1taWdyYXRpb24udXMuYXV0aDAuY29tL3VzZXJpbmZvIl0sImlhdCI6MTcxNzE1ODA2OSwiZXhwIjoxNzE4ODg2MDY5LCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIHVwZGF0ZTpjdXJyZW50X3VzZXJfbWV0YWRhdGEgcmVhZDpjdXJyZW50X3VzZXIgb2ZmbGluZV9hY2Nlc3MiLCJhenAiOiJydjlqaTNvajRidHNJaWNVc3JrSHI3QnN2Q1JodXdIYiJ9.YvZMys1Au4cAaO429lkJp37AjowL3xsTZ6vUNWG4myHONYjrHRreTvHdoVRquyAvf9DdKlovrFmzmc53QbvYKdFY__jIgUQ_KZbCioM5KB5KBSnE7fz8zUokNXxoS-tNk82rTYkdUFWDdPYMwrhz4eVRjbAmcPNuG9Rpr61OsQz8zig_b2iDZ_DzQf_bTu-I1DisGUnrOkeDvUbcsujvy29XLQMXJ5iOlTasgYNyEkN0S49d8bhmq1VSh2JFuvDCJIpVzrzQ9k9SemQV9phJM-DXkY-OO0Vh2MKYm7w2W3QHIBzmdGxJYnOMjFUusBLUAmWZSHFS0dg0B1x3cuvD5g',
#     'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
# }

# response = requests.get('https://api.io.solutions/v1/io-explorer/network/info', headers=headers)





# if response.status_code == 200:
#     data = response.json()['data'][1]
#     print(data)
#     render_network = pd.DataFrame([data])
#     render_network.drop(columns=['id', 'icon','busy_percent' ], inplace=True)
#     render_network= render_network[['name','active_gpu', 'active_cpu', 'total_gpu', 'total_cpu']]
#     render_network.columns = ['name','activeGPU', 'activeCPU', 'totalGPU', 'totalCPU']
#     render_network.rename(columns={'name': 'project'}, inplace=True)
#     render_network

# print('Failed to retrieve data. Status code:', response.status_code)

import requests
import requests

import requests

headers = {
    'accept': 'application/json, text/plain, */*',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'cache-control': 'no-cache',
    'origin': 'https://id.io.net',
    'pragma': 'no-cache',
    'priority': 'u=1, i',
    'referer': 'https://id.io.net/',
    'sec-ch-ua': '"Not)A;Brand";v="99", "Google Chrome";v="127", "Chromium";v="127"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'cross-site',
    'token': 'eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6Ink2SUlqV1JidlBKR1pIRzdCYV9rYSJ9.eyJodHRwczovL2lvLm5ldC91c2VyIjp7ImNyZWF0ZWQiOmZhbHNlLCJpb19pZCI6IjBhYjg1MDdmLWQzNzMtNGNiMC1iOTAxLTIzMTJiYWVlNmJjOCIsInByaW1hcnlfbWFpbCI6ImFyanVuLnBhdGhha0BweW9yLnh5eiJ9LCJpc3MiOiJodHRwczovL3Rlc3QtdXNlcnMtbWlncmF0aW9uLnVzLmF1dGgwLmNvbS8iLCJzdWIiOiJhdXRoMHw2NjI3OWEzNGVkN2UxNWFiNTY3NjAwZTQiLCJhdWQiOlsiaHR0cHM6Ly90ZXN0LXVzZXJzLW1pZ3JhdGlvbi51cy5hdXRoMC5jb20vYXBpL3YyLyIsImh0dHBzOi8vdGVzdC11c2Vycy1taWdyYXRpb24udXMuYXV0aDAuY29tL3VzZXJpbmZvIl0sImlhdCI6MTcyNDA1NDA2NSwiZXhwIjoxNzI1NzgyMDY1LCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIHVwZGF0ZTpjdXJyZW50X3VzZXJfbWV0YWRhdGEgcmVhZDpjdXJyZW50X3VzZXIgb2ZmbGluZV9hY2Nlc3MiLCJhenAiOiJydjlqaTNvajRidHNJaWNVc3JrSHI3QnN2Q1JodXdIYiJ9.HzsVVHuE-drtBFBHc8ILlbRL3J7a1CVuEvHXUA_38SU7Dd1y0S5FurJRgzHXTqIM8vm2toRqBj9j5Ow05c_wqGYzPWQg_tHgdiJaP0Y6Zy5CCtlKknGLB1zMcXGr5BbDm4uTtNLVmigqD4GsfKqhBWJebPgk-Bdu9b-YdhVgHsjT8fRB_6uUSTLeUbBKu0QyUcyaU2igcxLXWbdUeqrrAa9EzShCDuuF5Zeh9udkWrTmt7WUxH5nv8fXsuTDQ5BoS8YWGBUBMFMZRBOkx_7iZSAig5Jg9KOjZaG47LslmKY7rYa1WjD9NbKTSzk3ti5aNpSEb_Kwu73U90aGhubiDw',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36',
}

response = requests.get('https://api.io.solutions/v1/io-explorer/network/info', headers=headers)
if response.status_code == 200:
    data = response.json()['data'][1]
    print(data)
    render_network = pd.DataFrame([data])
    render_network.drop(columns=['id', 'icon','busy_percent' ], inplace=True)
    render_network= render_network[['name','active_gpu', 'active_cpu', 'total_gpu', 'total_cpu']]
    render_network.columns = ['name','activeGPU', 'activeCPU', 'totalGPU', 'totalCPU']
    render_network.rename(columns={'name': 'project'}, inplace=True)
    render_network

render_network

#io.net data
response = requests.get('https://api.io.solutions/v1/io-explorer/network/info', headers=headers)
data = response.json()['data'][0]
io_net = pd.DataFrame([data])
io_net= io_net[['name','active_gpu', 'active_cpu']]

# io_net.drop(columns=['id', 'icon','busy_percent' ], inplace=True)
io_net.columns = ['project','totalGPU', 'totalCPU']
io_net

#icp
import requests
import pandas as pd

headers = {
    'accept': 'application/json, text/plain, */*',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'origin': 'https://www.icpexplorer.org',
    'referer': 'https://www.icpexplorer.org/',
    'sec-ch-ua': '"Google Chrome";v="123", "Not:A-Brand";v="8", "Chromium";v="123"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'cross-site',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
}

response = requests.get('https://ic-api.internetcomputer.org/api/v3/metrics/ic-memory-total', headers=headers)
data = response.json()['ic_memory_total'][0]
icp_memory = pd.DataFrame([data])
icp_memory.columns = ['date', 'totalMemory']
icp_memory['date'] = pd.to_datetime(icp_memory['date'], unit = 's').dt.date
icp_memory['project'] = 'icp'

response = requests.get('https://ic-api.internetcomputer.org/api/v3/metrics/ic-cpu-cores', headers=headers)
data = response.json()['ic_cpu_cores'][0]
icp_cpu = pd.DataFrame([data])
icp_cpu.columns = ['date', 'totalCPU']
icp_cpu['date'] = pd.to_datetime(icp_cpu['date'], unit = 's').dt.date
icp_cpu['project'] = 'icp'

icp_network = pd.merge(icp_memory, icp_cpu, on = ['date','project'], how = 'inner')
icp_network['totalMemory'] = icp_network['totalMemory'].astype('float64')/10**12
icp_network

import requests

headers = {
    'accept': '*/*',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'origin': 'https://stats.test.grid.tf',
    'referer': 'https://stats.test.grid.tf/',
    'sec-ch-ua': '"Google Chrome";v="123", "Not:A-Brand";v="8", "Chromium";v="123"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-site',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
}

params = {
    'status': 'up',
}

response = requests.get('https://gridproxy.dev.grid.tf/stats', params=params, headers=headers)
response
data = response.json()
df1 = pd.DataFrame([data])
df1 = df1.transpose().reset_index()
df1.columns = ['index', 'value']

response = requests.get('https://gridproxy.grid.tf/stats', params=params, headers=headers)
data = response.json()
df2 = pd.DataFrame([data])
df2 = df2.transpose().reset_index()
df2.columns = ['index', 'value1']

response = requests.get('https://gridproxy.test.grid.tf/stats', params=params, headers=headers)
data = response.json()
df3 = pd.DataFrame([data])
df3 = df3.transpose().reset_index()
df3.columns = ['index', 'value2']




import requests

headers = {
    'accept': '*/*',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'origin': 'https://stats.test.grid.tf',
    'referer': 'https://stats.test.grid.tf/',
    'sec-ch-ua': '"Google Chrome";v="123", "Not:A-Brand";v="8", "Chromium";v="123"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-site',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
}

params = {
    'status': 'standby',
}


response = requests.get('https://gridproxy.test.grid.tf/stats', params=params, headers=headers)
data = response.json()
df4= pd.DataFrame([data])
df4 = df4.transpose().reset_index()
df4.columns = ['index', 'value3']






response = requests.get('https://gridproxy.grid.tf/stats', params=params, headers=headers)
data = response.json()
df5 = pd.DataFrame([data])
df5 = df5.transpose().reset_index()
df5.columns = ['index', 'value4']


df_merged = df5.merge(df1, on='index', how='inner')
df_merged = df_merged.merge(df2, on='index', how='inner')
df_merged = df_merged.merge(df3, on='index', how='inner')
df_merged = df_merged.merge(df4, on='index', how='inner')
df_merged.drop(12, axis=0, inplace=True)
# df_merged


df_merged['final_value'] = df_merged['value'] + df_merged['value1'] + df_merged['value2'] + df_merged['value3'] + df_merged['value4']
df_merged = df_merged[['index', 'final_value']]
df_merged.set_index('index', inplace=True)
df_merged = df_merged.transpose()
df_merged = df_merged[['totalCru', 'totalSru','totalMru', 'totalHru', 'gpus']]
df_merged['totalStorage'] = df_merged['totalSru'] + df_merged['totalHru']



df_merged['totalStorage'] = df_merged['totalSru'] + df_merged['totalHru']
df_merged = df_merged[['totalCru', 'totalMru',  'totalStorage', 'gpus']]
df_merged.columns = ['totalCPU', 'totalMemory', 'totalStorage', 'totalGPU']
df_merged['project'] = 'tft'
df_merged.reset_index(drop=True, inplace=True)
df_merged['totalStorage'] = df_merged['totalStorage'] / (1024*1024*1024*1024)
df_merged['totalMemory'] = df_merged['totalMemory'] / (1024*1024*1024*1024)
df_merged

#github stars and watcher data

import requests
owner_list = ["akash-network", "rndr-network", "dfinity", "cudoventures", "aleph-im", "golemfactory", "ionet-official", "threefoldfoundation"]
repo_list = ["node", "RNPs", "ic", "cudos-eth-token-contract", "aleph-vm", "yagna", "io_launch_binaries", "tft"]
projectlst = ['akash-network','Render Network', 'icp', 'cudos', 'aleph', 'golem', 'io.net', 'tft']
print(len(projectlst))
print(len(owner_list))
print(len(repo_list))
github_star = pd.DataFrame()

for i in range(len(owner_list)):
    headers = {"Authorization": "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"}
    url = (f"https://api.github.com/repos/{owner_list[i]}/{repo_list[i]}")
    response = requests.get(url, headers=headers)
    print(url)
    if response.status_code == 200:
        stars = response.json()['stargazers_count']
        watchers = response.json()['subscribers_count']
        temp = pd.DataFrame([[projectlst[i], stars, watchers]], columns=['project', 'stars', 'watchers'])
        github_star = pd.concat([github_star, temp], ignore_index=True)

#weekly commits

lsts = ['Akash Network', 'Cudos', 'Aleph.im', 'FLUX', 'Golem', 'ThreeFold Token']
project_lst = ['akash-network', 'cudos', 'aleph', 'flux', 'golem', 'tft']

weekly_commit_data = pd.DataFrame()
for i in range(len(lsts)):
    url = 'https://api.gokustats.xyz/weekly-commits/?daysBack={}&ecosystem={}'.format(90, lsts[i])
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        temp = pd.DataFrame(data)
        temp['project'] = project_lst[i]
        temp.columns = ['date', 'weekly_commits', 'project']
        weekly_commit_data = pd.concat([weekly_commit_data, temp], ignore_index=True)
    else:
        print('Failed to retrieve data. Status code:', response.status_code)


url = 'https://api.gokustats.xyz/weekly-commits/?daysBack=90&ecosystem=Internet Computer'
response = requests.get(url)
data = response.json()
df = pd.DataFrame(data)
df = df[['date', 'Internet Computer Core']]
df.columns = ['date', 'weekly_commits']
df['project'] = 'icp'
df


weekly_commit_data = pd.concat([weekly_commit_data, df], ignore_index=True)




#weekly active devs

lsts1 = ['Akash Network', 'Cudos', 'Aleph.im', 'FLUX', 'Golem', 'ThreeFold Token']
project_lst = ['akash-network', 'cudos', 'aleph', 'flux', 'golem', 'tft']

weekly_dev_data = pd.DataFrame()
for i in range(len(lsts1)):
    url = 'https://api.gokustats.xyz/weekly-active-devs/?daysBack={}&ecosystem={}'.format(90, lsts1[i])
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        temp = pd.DataFrame(data)
        temp['project'] = project_lst[i]
        temp.columns = ['date', 'weekly_devs', 'project']
        weekly_dev_data = pd.concat([weekly_dev_data, temp], ignore_index=True)
    else:
        print('Failed to retrieve data. Status code:', response.status_code)



url = 'https://api.gokustats.xyz/weekly-active-devs/?daysBack=90&ecosystem=Internet Computer'
response = requests.get(url)
data = response.json()
df = pd.DataFrame(data)
df = df[['date', 'Internet Computer Core']]
df.columns = ['date', 'weekly_devs']
df['project'] = 'icp'
df

weekly_dev_data = pd.concat([weekly_dev_data, df], ignore_index=True)




weekly_commit_dev_data = pd.merge(weekly_commit_data, weekly_dev_data, how='inner', on=['date', 'project'])
weekly_commit_dev_data = weekly_commit_dev_data[weekly_commit_dev_data['date']==weekly_commit_dev_data['date'].max()]
weekly_commit_dev_data

# cloreAi

headers = {
    'accept': 'application/json, text/plain, */*',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'content-type': 'application/json',
    'origin': 'https://opendata.clore.ai',
    'referer': 'https://opendata.clore.ai/d/aKQRJmA4z/clore-ai?orgId=1&viewPanel=38',
    'sec-ch-ua': '"Google Chrome";v="123", "Not:A-Brand";v="8", "Chromium";v="123"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-origin',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
    'x-dashboard-uid': 'aKQRJmA4z',
    'x-datasource-uid': 'NUm_0m0Vz',
    'x-grafana-org-id': '1',
    'x-panel-id': '38',
    'x-plugin-id': 'prometheus',
}

json_data = {
    'queries': [
        {
            'datasource': {
                'type': 'prometheus',
                'uid': 'NUm_0m0Vz',
            },
            'editorMode': 'builder',
            'expr': 'max(p_total_gpus)',
            'legendFormat': '{{Total GPUs}} Total GPUs',
            'range': True,
            'refId': 'A',
            'queryType': 'timeSeriesQuery',
            'exemplar': False,
            'requestId': '38A',
            'utcOffsetSec': 19800,
            'interval': '',
            'datasourceId': 1,
            'intervalMs': 600000,
            'maxDataPoints': 1280,
        },
        {
            'datasource': {
                'type': 'prometheus',
                'uid': 'NUm_0m0Vz',
            },
            'editorMode': 'builder',
            'expr': 'max(p_rented_gpus)',
            'hide': False,
            'legendFormat': '{{Rented GPUs}} Rented GPUs',
            'range': True,
            'refId': 'B',
            'queryType': 'timeSeriesQuery',
            'exemplar': False,
            'requestId': '38B',
            'utcOffsetSec': 19800,
            'interval': '',
            'datasourceId': 1,
            'intervalMs': 600000,
            'maxDataPoints': 1280,
        },
    ],
    'range': {
        'from': '2024-04-04T11:36:38.259Z',
        'to': '2024-04-11T11:36:38.259Z',
        'raw': {
            'from': 'now-7d',
            'to': 'now',
        },
    },
    'from': '1712230598259',
    'to': '1712835398259',
}

response = requests.post('https://opendata.clore.ai/api/ds/query', headers=headers, json=json_data)


data = response.json()['results']['A']['frames'][0]['data']['values']
cloreAI_data1 = pd.DataFrame(data)
cloreAI_data1 = cloreAI_data1.transpose()
cloreAI_data1.columns = ['date', 'totalGPU']
cloreAI_data1['date'] = cloreAI_data1['date']/1000
cloreAI_data1['date'] = pd.to_datetime(cloreAI_data1['date'], unit='s').dt.date
cloreAI_data1.drop_duplicates(subset='date', keep='last', inplace=True)
cloreAI_data1['project'] = 'clore_ai'
cloreAI_data1



data = response.json()['results']['B']['frames'][0]['data']['values']
cloreAI_data2 = pd.DataFrame(data)
cloreAI_data2 = cloreAI_data2.transpose()
cloreAI_data2.columns = ['date', 'activeGPU']
cloreAI_data2['date'] = cloreAI_data2['date']/1000
cloreAI_data2['date'] = pd.to_datetime(cloreAI_data2['date'], unit='s').dt.date
cloreAI_data2.drop_duplicates(subset='date', keep='last', inplace=True)
cloreAI_data2['project'] = 'clore_ai'
cloreAI_data2

cloreAI_data = pd.merge(cloreAI_data1, cloreAI_data2, how='inner', on=['date', 'project'])
cloreAI_data = cloreAI_data[cloreAI_data['date']==cloreAI_data['date'].max()]
cloreAI_data

#golem data

import requests

headers = {
    'accept': '*/*',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'origin': 'https://stats.golem.network',
    'referer': 'https://stats.golem.network/',
    'sec-ch-ua': '"Google Chrome";v="123", "Not:A-Brand";v="8", "Chromium";v="123"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-site',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
}

response = requests.get('https://api.stats.golem.network/v1/network/historical/stats/30m', headers=headers)
data = response.json()
golem_data = pd.DataFrame(data)
golem_data = golem_data[['date','cores', 'memory', 'disk']]
golem_data.columns = ['date','totalCPU', 'totalMemory', 'totalStorage']
golem_data = golem_data[golem_data['date']==golem_data['date'].max()]
golem_data['totalMemory'] = golem_data['totalMemory'].astype(int)/1000
golem_data['totalStorage'] = golem_data['totalStorage'].astype(int)/1000
golem_data['project'] = 'golem'
golem_data

#stackos data
stackos_data = pd.DataFrame({'totalCPU': [26400], 'totalMemory': [240], 'totalStorage': [12000], 'project': ['Store Cloud']})
stackos_data

date_string

dashboard3 = pd.concat([akash_network, render_network, icp_network, io_net, df_merged, cloreAI_data, golem_data, flux_data, stackos_data], ignore_index=True)
dashboard3

dashboard3 = dashboard3.merge(weekly_commit_dev_data, on='project', how='outer')
dashboard3 = dashboard3.merge(github_star, on='project', how='outer')
dashboard3


dashboard3.drop(columns={'date_x', 'date_y'}, inplace=True)
dashboard3['date'] = date_string
dashboard3_compute = dashboard3[['date','project', 'totalCPU', 'activeCPU', 'totalGPU','activeGPU', 'totalMemory', 'totalStorage',  'stars', 'watchers', 'weekly_commits', 'weekly_devs']]
dashboard3_compute
dashboard3_compute['totalMemory'] = dashboard3_compute['totalMemory'].apply(lambda x: round(x, 2) if pd.notnull(x) else x)
dashboard3_compute['totalStorage'] = dashboard3_compute['totalStorage'].apply(lambda x: round(x, 2) if pd.notnull(x) else x)
dashboard3_compute

pre_project = dashboard3_compute['project'].unique().tolist()
pre_project
m31_project = ['AKT', 'RNDR','ICP','io.net','TFT','CLORE.AI','GLM','FLUX','Store Cloud','CUDOS','ALEPH']

# Update values in the 'project' column
for i in range(len(pre_project)):
    dashboard3_compute.loc[dashboard3_compute['project'] == pre_project[i], 'project'] = m31_project[i]
    print(pre_project[i])

dashboard3_compute

q = """select B.date, A.name_y as project, A.market_cap, A.fdmcap, A.price_change_30d, B.totalCPU, B.activeCPU,
B.totalGPU, B.activeGPU, B.totalMemory, B.totalStorage,
 B.stars, B.watchers, B.weekly_commits, B.weekly_devs, A.'logo id' as logo_id
from dashboard3_compute B
inner join market_data A
on A.ticker=B.project"""
dashboard3_compute = pysqldf(q)
dashboard3_compute

previous_data_compute =read_full_table('raw_m31', 'compute_hist')
previous_data_compute = previous_data_compute[previous_data_compute['date']==previous_data_compute['date'].max()]
previous_data_compute

# dashboard3_compute[['totalCPU', 'activeCPU', 'totalGPU', 'activeGPU', 'totalMemory', 'totalStorage']] = dashboard3_compute[['totalCPU', 'activeCPU', 'totalGPU', 'activeGPU', 'totalMemory', 'totalStorage']].apply(pd.to_numeric, errors='coerce')

dashboard3_compute_copy = dashboard3_compute.copy()

q = """select A.*, B.totalCPU as totalCPU_previous, B.activeCPU as activeCPU_previous, B.totalGPU as totalGPU_previous, B.activeGPU as activeGPU_previous
from dashboard3_compute_copy A
left join previous_data_compute B
on A.project = B.project"""

dashboard3_compute_data = pysqldf(q)
dashboard3_compute_data

dashboard3_compute_data[['totalCPU', 'activeCPU', 'totalGPU', 'activeGPU', 'totalMemory', 'totalStorage']] = dashboard3_compute_data[['totalCPU', 'activeCPU', 'totalGPU', 'activeGPU', 'totalMemory', 'totalStorage']].apply(pd.to_numeric, errors='coerce')

dashboard3_compute_data['totalCPU_change_7d'] =((dashboard3_compute_data['totalCPU']-dashboard3_compute_data['totalCPU_previous'])/dashboard3_compute_data['totalCPU_previous'])*100
dashboard3_compute_data['activeCPU_change_7d'] =((dashboard3_compute_data['activeCPU']-dashboard3_compute_data['activeCPU_previous'])/dashboard3_compute_data['activeCPU_previous'])*100
dashboard3_compute_data['totalGPU_change_7d'] =((dashboard3_compute_data['totalGPU']-dashboard3_compute_data['totalGPU_previous'])/dashboard3_compute_data['totalGPU_previous'])*100
dashboard3_compute_data['activeGPU_change_7d'] =((dashboard3_compute_data['activeGPU']-dashboard3_compute_data['activeGPU_previous'])/dashboard3_compute_data['activeGPU_previous'])*100

dashboard3_compute_data.drop(columns=['totalCPU_previous','activeCPU_previous','totalGPU_previous','activeGPU_previous'], inplace=True)
dashboard3_compute_data

dashboard3_compute_data.replace([np.inf, -np.inf], np.nan, inplace=True)
dashboard3_compute_data

dashboard3_compute_data.loc[dashboard3_compute_data['project'] == 'Aleph.im', 'logo_id'] = 5821
dashboard3_compute_data

# dashboard3_compute_data[['totalCPU', 'activeCPU', 'totalGPU', 'activeGPU', 'totalMemory', 'totalStorage']] = dashboard3_compute_data[['totalCPU', 'activeCPU', 'totalGPU', 'activeGPU', 'totalMemory', 'totalStorage']].apply(pd.to_numeric, errors='coerce')


#flooring num data
dashboard3_compute_data = floor_except_first_two_columns(dashboard3_compute_data)
dashboard3_compute_data.to_csv('compute_data.csv', index=False)


dashboard3_compute_data['date'] = pd.to_datetime(dashboard3_compute_data['date'])

provider = 'm31'
dataset = 'compute'

create_or_overwrite_table(dashboard3_compute_data, provider, dataset)

compute = read_full_table('raw_m31', 'compute')
df = pd.DataFrame({'date': [date_string],'project': ['Aethir'], 'fdmcap': [2575], 'market_cap': [248], 'price_change_30d': [14.9],
'totalgpu':[43000], 'totalgpu_change_7d': [0.0], 'logo_id': [30083]})
df
compute = pd.concat([compute, df], ignore_index=True)
compute['date'] = pd.to_datetime(compute['date'])
dashboard3_compute_data = compute.copy()
dashboard3_compute_data.drop_duplicates(subset='project', keep='last', inplace=True)
provider = 'm31'
dataset = 'compute'
create_or_overwrite_table(dashboard3_compute_data, provider, dataset)
dashboard3_compute_data

df = read_full_table('raw_m31', 'compute_hist')
df.columns = dashboard3_compute_data.columns.to_list()
x = dashboard3_compute_data.copy()
df = pd.concat([df, x], ignore_index=True)
df
provider = 'm31'
dataset = 'compute_hist'

create_or_overwrite_table(df, provider, dataset)


# df

"""### Dashboard 4"""

#SC (sia.tech)

import requests

headers = {
    'accept': 'application/json, text/plain, */*',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'if-none-match': '"2g3yihpkdk6i"',
    'referer': 'https://sia.tech/',
    'sec-ch-ua': '"Google Chrome";v="123", "Not:A-Brand";v="8", "Chromium";v="123"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-origin',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
}

response = requests.get('https://sia.tech/api/stats', headers=headers)
data = response.json()
sia_data = pd.DataFrame([data])
sia_data = sia_data[['totalStorage', 'usedStorage']]
sia_data['project'] = 'SC'
sia_data


#converting from PB to TB
PB_to_TB = 1000

# Convert PB to TB for 'totalStorage' and 'usedStorage'
sia_data[['totalStorage', 'usedStorage']] = sia_data[['totalStorage', 'usedStorage']].replace({' PB': ''}, regex=True).astype(float) * PB_to_TB
sia_data

#storj data
headers = {
    'accept': 'application/json, text/plain, */*',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'content-type': 'application/x-www-form-urlencoded',
    'origin': 'https://storjstats.info',
    'referer': 'https://storjstats.info/d/storj/storj-network-statistics?orgId=1&viewPanel=405',
    'sec-ch-ua': '\"Google Chrome\";v=\"123\", \"Not:A-Brand\";v=\"8\", \"Chromium\";v=\"123\"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '\"macOS\"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-origin',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
    'x-dashboard-id': '1',
    'x-grafana-org-id': '1',
    'x-panel-id': '405',
}

data = {
    'query': 'max(storj_stats_storage_free_capacity_estimate_bytes)',
    'start': '1712121300',
    'end': '1712726100',
    'step': '900',
}

response = requests.post('https://storjstats.info/api/datasources/proxy/6/api/v1/query_range', headers=headers, data=data)
data = response.json()['data']['result'][0]['values']
storj_data = pd.DataFrame(data)
storj_data.columns = ['date', 'freeStorage']
storj_data['freeStorage'] = storj_data['freeStorage'].astype(float)
storj_data['date'] = pd.to_datetime(storj_data['date'], unit='s').dt.date
storj_data.drop_duplicates(subset='date', keep='last', inplace=True)



data = {
    'query': 'sum(storj_stats_storage_remote_bytes)',
    'start': '1712121300',
    'end': '1712726100',
    'step': '900',
}

response = requests.post('https://storjstats.info/api/datasources/proxy/6/api/v1/query_range', headers=headers, data=data)
data = response.json()['data']['result'][0]['values']
storj_data1 = pd.DataFrame(data)
storj_data1.columns = ['date', 'usedStorage']
storj_data1['usedStorage'] = storj_data1['usedStorage'].astype(float)
storj_data1['date'] = pd.to_datetime(storj_data1['date'], unit='s').dt.date
storj_data1.drop_duplicates(subset='date', keep='last', inplace=True)



storj_data = storj_data.merge(storj_data1, how='inner', on='date')
storj_data['totalStorage'] = storj_data['freeStorage'] + storj_data['usedStorage']
storj_data.drop(columns=['freeStorage'], inplace=True)
storj_data['project'] = 'STORJ'
storj_data = storj_data[storj_data['date']==storj_data['date'].max()]
storj_data['usedStorage'] = (storj_data['usedStorage']/(10**12)).round(2)
storj_data['totalStorage'] = (storj_data['totalStorage']/(10**12)).round(2)
storj_data

#btfs data

headers = {
    'accept': 'application/json, text/plain, */*',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'origin': 'https://scan.btfs.io',
    'referer': 'https://scan.btfs.io/',
    'sec-ch-ua': '"Google Chrome";v="123", "Not:A-Brand";v="8", "Chromium";v="123"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-site',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
}

params = {
    'days': '8',
}

response = requests.get('https://scan-backend.btfs.io/api/node/v2/btfsscan/report_data_history', params=params, headers=headers)
data = response.json()['data']
btfs_data = pd.DataFrame(data)
btfs_data = btfs_data[['date', 'storage_left_when_contract', 'contract_all_storage']]
btfs_data['storage_left_when_contract'] = (btfs_data['storage_left_when_contract']/(1099511627776)).round(2)
btfs_data['contract_all_storage'] = (btfs_data['contract_all_storage']/(1099511627776)).round(2)
btfs_data['totalStorage'] = btfs_data['storage_left_when_contract'] + btfs_data['contract_all_storage']
btfs_data.rename(columns={'contract_all_storage':'usedStorage'}, inplace=True)
btfs_data = btfs_data[['date', 'totalStorage', 'usedStorage']]
btfs_data['date'] = pd.to_datetime(btfs_data['date']).dt.date
btfs_data = btfs_data[btfs_data['date'] == btfs_data['date'].max()]
btfs_data['project'] = 'BTT'
btfs_data

import requests

headers = {
    'Accept': '*/*',
    'Accept-Language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'Connection': 'keep-alive',
    'Origin': 'https://starboard.static.observableusercontent.com',
    'Referer': 'https://starboard.static.observableusercontent.com/',
    'Sec-Fetch-Dest': 'empty',
    'Sec-Fetch-Mode': 'cors',
    'Sec-Fetch-Site': 'cross-site',
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
    'authorization': 'Bearer ghp_PuRLZYtjiBqQHrmNeNKNJdBKqJZSPyKGKiow',
    'sec-ch-ua': '"Chromium";v="124", "Google Chrome";v="124", "Not-A.Brand";v="99"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
}

params = {
    'end_date': '2024-05-10',
    'start_date': '2024-02-10',
}

response = requests.get('https://api.spacescope.io/v2/deals/deal_size', params=params, headers=headers)
data =response.json()['data']
df = pd.DataFrame(data)
filcoin_used = df[['stat_date', 'active_verified_deal_size']]
filcoin_used.columns = ['date','usedStorage' ]
filcoin_used

import requests

headers = {
    'Accept': '*/*',
    'Accept-Language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'Connection': 'keep-alive',
    'Origin': 'https://starboard.static.observableusercontent.com',
    'Referer': 'https://starboard.static.observableusercontent.com/',
    'Sec-Fetch-Dest': 'empty',
    'Sec-Fetch-Mode': 'cors',
    'Sec-Fetch-Site': 'cross-site',
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
    'authorization': 'Bearer ghp_PuRLZYtjiBqQHrmNeNKNJdBKqJZSPyKGKiow',
    'sec-ch-ua': '"Chromium";v="124", "Google Chrome";v="124", "Not-A.Brand";v="99"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
}

params = {
    'end_date': '2024-05-12',
    'start_date': '2024-02-12',
}

response = requests.get('https://api.spacescope.io/v2/power/network_storage_capacity', params=params, headers=headers)

data =response.json()['data']
df = pd.DataFrame(data)
filcoin_cap = df[['stat_date', 'total_raw_bytes_power']]
filcoin_cap.columns = ['date','totalStorage' ]
filcoin_cap

filecoin_data = pd.merge(filcoin_cap, filcoin_used, on='date', how='inner')
filecoin_data['project'] = 'FIL'
filecoin_data['totalStorage'] = filecoin_data['totalStorage']/1024**4
filecoin_data['usedStorage'] = filecoin_data['usedStorage']/1024**4
filecoin_data = filecoin_data[filecoin_data['date'] == filecoin_data['date'].max()]
filecoin_data

import requests
import re
import json
import requests



# headers = {
#     'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
#     'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
#     'cache-control': 'max-age=0',
#     # 'cookie': 'i18nHash=f6a8e27d049ca2f73d4b7cd7ff72efdc7f5deead29dec6dd7b6502db07887eeb; _ga=GA1.1.946865155.1712569981; sidebar=opened; _ga_M5RFQ435JM=GS1.1.1712992242.12.1.1712996092.0.0.0',
#     'referer': 'https://www.google.com/',
#     'sec-ch-ua': '"Google Chrome";v="123", "Not:A-Brand";v="8", "Chromium";v="123"',
#     'sec-ch-ua-mobile': '?0',
#     'sec-ch-ua-platform': '"macOS"',
#     'sec-fetch-dest': 'document',
#     'sec-fetch-mode': 'navigate',
#     'sec-fetch-site': 'same-origin',
#     'sec-fetch-user': '?1',
#     'upgrade-insecure-requests': '1',
#     'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
# }

# response = requests.get('https://viewblock.io/arweave/stat/cumulativeDataSize',  headers=headers,proxies=proxies,verify=False)
# data = response.text

import re
import json
import requests


# matches = re.search(r'(?<=<script id="__NEXT_DATA__" type="application/json">)(.*?)(?=\s*</script>)', data, re.DOTALL)

# if matches:
#     json_data = matches.group(1)
#     # print(json_data)
# else:
#     print("No JSON data found between delimiters")



# store_data = json.loads(json_data)['props']['pageProps']['ssrData']['/arweave/stats/advanced/charts/cumulativeDataSize']['day']['data']

arweave = pd.DataFrame({'date':[date_string], 'usedStorage':[181.88]})
# arweave = arweave.transpose()
arweave.columns = ['date', 'usedStorage']
# arweave['date'] =arweave['date']/1000
# arweave['date'] = pd.to_datetime(arweave['date'], unit='s')
# arweave['usedStorage'] = (arweave['cumulativeWeaveSize'].astype(float) / (1024 ** 4)).round(2)
# arweave.drop(columns={'cumulativeWeaveSize'}, inplace=True)
arweave['project'] = 'AR'
# arweave = arweave[arweave['date']==arweave['date'].max()]

arweave


# response = requests.get('https://viewblock.io/arweave/stat/dataUploaded',  headers=headers)

# data1 = response.text

# matches = re.search(r'(?<=<script id="__NEXT_DATA__" type="application/json">)(.*?)(?=\s*</script>)', data1, re.DOTALL)

# if matches:
#     json_data1 = matches.group(1)
#     # print(json_data)
# else:
#     print("No JSON data found between delimiters")



# store_data1 = json.loads(json_data1)['props']['pageProps']['ssrData']['/arweave/stats/advanced/charts/dataUploaded']['day']['data']

# arweave_data = pd.DataFrame(store_data1)
# arweave_data = arweave_data.transpose()
# arweave_data.columns = ['date', 'cumulativeWeaveSize']
# arweave_data['date'] =arweave_data['date']/1000
# arweave_data['date'] = pd.to_datetime(arweave_data['date'], unit='s')
# arweave_data['usedStorage'] = (arweave_data['cumulativeWeaveSize'].astype(float) / (1024 ** 3)).round(2)
# arweave_data.drop(columns={'cumulativeWeaveSize'}, inplace=True)
# arweave_data['project'] = 'arweave'
# arweave_data


# # combining both dataframes
# arweave_data_df = arweave_data.merge(arweave, on=['date','project'], how='inner')
# arweave_data_df = arweave_data_df[arweave_data_df['date']==arweave_data_df['date'].max()]
# arweave_data_df

# import requests
# import numpy as np
# proxies = {
#     'http': 'http://scraperapi:8a457df49571f250d536be0990207d40@proxy-server.scraperapi.com:8001',
#     'https': 'http://scraperapi:8a457df49571f250d536be0990207d40@proxy-server.scraperapi.com:8001',
# }
# headers = {
#     'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
#     'Accept-Language': 'en-GB,en-US;q=0.9,en;q=0.8',
#     'Cache-Control': 'max-age=0',
#     'Connection': 'keep-alive',
#     # 'Cookie': '_ga=GA1.2.467425836.1712573170',
#     'If-None-Match': '"9th4jkuk6r107v"',
#     'Referer': 'https://www.lambda.im/',
#     'Sec-Fetch-Dest': 'document',
#     'Sec-Fetch-Mode': 'navigate',
#     'Sec-Fetch-Site': 'same-origin',
#     'Sec-Fetch-User': '?1',
#     'Upgrade-Insecure-Requests': '1',
#     'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
#     'sec-ch-ua': '"Google Chrome";v="123", "Not:A-Brand";v="8", "Chromium";v="123"',
#     'sec-ch-ua-mobile': '?0',
#     'sec-ch-ua-platform': '"macOS"',
# }

# response = requests.get('https://storage.lambda.im/', headers=headers,  proxies=proxies, verify=False)
# print(response)
# tree = etree.HTML(response.text)
# data = tree.xpath('/html/body/main/div/div[1]/div[2]/div/div/div/p/text()')
# lamda_data = pd.DataFrame(data)
# lamda_data.columns = ['usedStorage']
# lamda_data['project'] = 'LAMB'
# lamda_data.replace('GB', '', regex=True, inplace=True)
# lamda_data['usedStorage'] = (lamda_data['usedStorage'].astype(float)/1024).round(2)

lamda_data = pd.DataFrame({'totalStorage':640000,  'project':['LAMB']})

# lamda_data = lamda_data.merge(lambda_df, on=['project'], how='inner')
lamda_data.loc[lamda_data['project'] == 'LAMB', 'usedStorage'] = np.nan
lamda_data

#github stars and watcher data

import requests
owner_list = [ "storj","SiaFoundation", "bittorrent", "LambdaIM"]
repo_list = [ "storj",  "siad", "go-btfs", "lambdavm"]
projectlst = [ 'STORJ','SC', 'BTT', 'LAMB']
print(len(projectlst))
print(len(owner_list))
print(len(repo_list))
github_star = pd.DataFrame()

for i in range(len(owner_list)):
    headers = {"Authorization": "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"}
    url = (f"https://api.github.com/repos/{owner_list[i]}/{repo_list[i]}")
    response = requests.get(url, headers=headers, proxies=proxies, verify=False)
    print(url)
    print(response)
    if response.status_code == 200:

        stars = response.json()['stargazers_count']
        watchers = response.json()['subscribers_count']
        temp = pd.DataFrame([[projectlst[i], stars, watchers]], columns=['project', 'stars', 'watchers'])
        github_star = pd.concat([github_star, temp], ignore_index=True)

#github data for filecoin

owner_list = ['filecoin-project', 'filecoin-project', 'filecoin-project', 'filecoin-project', 'filecoin-project', 'filecoin-project']
repo_name = ['community','FIPs', 'lotus', 'rust-fil-proofs', 'builtin-actors', 'ref-fvm']

filecoin_github_data = pd.DataFrame()
for i in range(len(owner_list)):
    headers = {"Authorization": "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"}
    url = (f"https://api.github.com/repos/{owner_list[i]}/{repo_name[i]}")
    response = requests.get(url, headers=headers, proxies=proxies, verify=False)
    print(url)
    print(response)
    if response.status_code == 200:

        stars = response.json()['stargazers_count']
        watchers = response.json()['subscribers_count']
        temp = pd.DataFrame([[repo_name[i], stars, watchers]], columns=['project', 'stars', 'watchers'])
        filecoin_github_data = pd.concat([filecoin_github_data, temp], ignore_index=True)

filecoin_github_data


filecoin_github_data.drop(columns={'project'}, inplace=True)
filecoin_github_data['project'] = 'FIL'
filecoin_github_data = filecoin_github_data.groupby('project')[['stars', 'watchers']].sum().reset_index()

#github data for arweave

arweave_list = ['ArweaveTeam', 'ArweaveTeam', 'ArweaveTeam', 'ArweaveTeam']
ar_repo = ['arweave', 'arweave-js', 'arweave-standards', 'SmartWeave']
arweave_github_data = pd.DataFrame()
for i in range(len(arweave_list)):
    headers = {"Authorization": "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"}
    url = (f"https://api.github.com/repos/{arweave_list[i]}/{ar_repo[i]}")
    print(url)
    response = requests.get(url, headers=headers, proxies=proxies, verify=False)
    print(url)
    print(response)
    if response.status_code == 200:

        stars = response.json()['stargazers_count']
        watchers = response.json()['subscribers_count']
        temp = pd.DataFrame([[ar_repo[i], stars, watchers]], columns=['project', 'stars', 'watchers'])
        arweave_github_data = pd.concat([arweave_github_data, temp], ignore_index=True)

arweave_github_data

arweave_github_data.drop(columns={'project'}, inplace=True)
arweave_github_data['project'] = 'AR'
arweave_github_data = arweave_github_data.groupby('project')[['stars', 'watchers']].sum().reset_index()
arweave_github_data

github_star = pd.concat([github_star, filecoin_github_data, arweave_github_data], ignore_index=True)

dashboard4 = pd.concat([sia_data, storj_data, btfs_data, filecoin_data, arweave, lamda_data], ignore_index=True)
dashboard4 = github_star.merge(dashboard4, on='project', how='left')
dashboard4.drop(columns={'date'}, inplace=True)
dashboard4['date'] = date_string
dashboard4_storage = dashboard4[['date', 'project',  'totalStorage', 'usedStorage','stars', 'watchers',]]

#flooring num data
dashboard4_storage = floor_except_first_two_cols(dashboard4_storage)
dashboard4_storage

q = """select B.date, A.name_y as project, A.market_cap, A.fdmcap, A.price_change_30d,
B.totalStorage, B.usedStorage,
B.stars, B.watchers, A.'logo id' as logo_id
from dashboard4_storage B
inner join market_data A
on A.ticker=B.project"""
dashboard4_strg = pysqldf(q)
dashboard4_strg

previous_data_storage = read_full_table('raw_m31', 'storage_hist')
previous_data_storage = previous_data_storage[previous_data_storage['date']==previous_data_storage['date'].max()]
previous_data_storage

dashboard4_storage_copy = dashboard4_strg.copy()
q = """select A.*, B.totalStorage as totalStorage_previous, B.usedStorage as usedStorage_previous
from dashboard4_storage_copy A
join previous_data_storage B
on A.project = B.project"""

dashboard4_storage_data = pysqldf(q)
dashboard4_storage_data

dashboard4_storage_data['capacity_change_7d'] =((dashboard4_storage_data['totalStorage']-dashboard4_storage_data['totalStorage_previous'])/dashboard4_storage_data['totalStorage_previous'])*100
dashboard4_storage_data['used_storage_change_7d'] =((dashboard4_storage_data['usedStorage']-dashboard4_storage_data['usedStorage_previous'])/dashboard4_storage_data['usedStorage_previous'])*100
dashboard4_storage_data.drop(columns={'totalStorage_previous', 'usedStorage_previous'}, inplace=True)
dashboard4_storage_data

x = read_full_table('raw_m31', 'storage_hist')
x.columns = dashboard4_storage_data.columns.to_list()
df = pd.concat([x, dashboard4_storage_data], ignore_index=True)
df
df['date'] = pd.to_datetime(df['date'])

provider = 'm31'
dataset = 'storage_hist'

create_or_overwrite_table(df, provider, dataset)

df

dashboard4_storage_data = floor_except_first_two_cols(dashboard4_storage_data)

dashboard4_storage_data.to_csv('storage_data.csv', index=False)

dashboard4_storage_data['date'] = pd.to_datetime(dashboard4_storage_data['date'])

provider = 'm31'
dataset = 'storage'

create_or_overwrite_table(dashboard4_storage_data, provider, dataset)

dashboard4_storage_data

"""## M31 Realtive Metric data 2nd part."""

API_KEY = os.getenv('COINGECKO_PRO_PASSWORD', '')
headers = {
    'content-type': "application/json"
}

API_URL = "https://pro-api.coingecko.com/api/v3"
headers["x-cg-pro-api-key"] = API_KEY

print(os.environ.get("ENVIRONMENT", "local") == "local")
print(API_URL)

class CoingeckoClient:

    REQUESTS_PER_MINUTE = 490

    def __init__(self):
        self.request_count = 0
        self.start_time = time.time()

    def make_request(self, url):
        c = 0
        response_json = None
        while c < 5 and response_json is None:
            try:
                response = requests.get(url, headers=headers)
                response.raise_for_status()
                self.update_rate_limit()
                response_json = response.json()
                return response_json
            except Exception as e:
                logging.exception(f"Error while Fetching data: {url}")
                c += 1
                sleep = (2 ** random.uniform(0, 1))
                time.sleep(sleep)
                if c == 5:
                    raise e
        return response_json

    def wait_for_rate_limit(self):
        elapsed_time = time.time() - self.start_time
        if elapsed_time < 60 and self.request_count >= CoingeckoClient.REQUESTS_PER_MINUTE:
            time_to_wait = 60 - elapsed_time
            time.sleep(time_to_wait)
            self.request_count = 0
        self.start_time = time.time()

    def update_rate_limit(self):
        self.request_count += 1

    def get_coins(self):
        url = API_URL + "/coins/list?include_platform=true"
        self.wait_for_rate_limit()
        response_json = self.make_request(url)
        return response_json

    def get_coingecko_price_data(self, id, days):
        template_url = API_URL + "/coins/{}/market_chart?vs_currency=usd&days={}&interval=daily"
        url = template_url.format(id, days)
        self.wait_for_rate_limit()
        response_json = self.make_request(url)
        return response_json


import numpy as np

def floor_except_first_two_columns(df):
    # Iterate over columns starting from the third column
    for col in df.columns:
        if col not in ['date','project','totalCPU_change_7d','activeCPU_change_7d', 'totalGPU_change_7d', 'activeGPU_change_7d']:
            # Ignore infinite values and then perform the floor operation
            df[col] = np.where(np.isfinite(df[col]), np.floor(df[col]), df[col])
    return df

# Assuming dashboard3_compute_data is your dataframe
# dashboard3_compute_data = floor_except_first_two_cols(dashboard3_compute_data)
import datetime

current_datetime = datetime.datetime.now()
days_to_subtract = 0
new_date = current_datetime - datetime.timedelta(days=days_to_subtract)
new_date_pd = pd.to_datetime(new_date)
date_only = new_date_pd.date()
date_string = date_only.strftime('%Y-%m-%d')

proxies = {
    'http': 'http://scraperapi:8a457df49571f250d536be0990207d40@proxy-server.scraperapi.com:8001',
    'https': 'http://scraperapi:8a457df49571f250d536be0990207d40@proxy-server.scraperapi.com:8001',
}

def floor_columns(df, col1, col2):
    df[col1] = np.floor(df[col1])
    df[col2] = np.floor(df[col2])
    return df

#MArket Data

token_list =['planetwatch','hivemapper','geodnet','foam-protocol','dimo','meson-network','theta-token','swarm','helium','xnet-mobile','wifi','wicrypt']

client = CoingeckoClient()
x=client.get_coins()
coins_df=pd.DataFrame(x)
coins_list=coins_df['id'].tolist()

coins_list=token_list
market_data_df=pd.DataFrame()
print(coins_list)

for i in range(len(token_list)):
    if i%50==0:
        print(i)
    # print("*********")
    try:
        coin_data=client.get_coingecko_price_data(coins_list[i], 'max')
        coin_data_prices=coin_data['prices']
        coin_data_market_caps=coin_data['market_caps']
        coin_data_total_volumes=coin_data['total_volumes']

        coin_data_prices_df=pd.DataFrame(coin_data_prices)
        coin_data_market_caps_df=pd.DataFrame(coin_data_market_caps)
        coin_data_total_volumes_df=pd.DataFrame(coin_data_total_volumes)
        coin_data_prices_df.columns=['date','prices']
        coin_data_market_caps_df.columns=['date','market_cap']
        coin_data_total_volumes_df.columns=['date','volume']

        merged = pd.merge(coin_data_prices_df, coin_data_market_caps_df, on='date', how='inner')
        coin_data_df = pd.merge(merged,coin_data_total_volumes_df, on='date', how='inner')
        coin_data_df['token']=coins_list[i]

        coin_data_df=coin_data_df[['date','token','prices','market_cap','volume']]

        market_data_df=pd.concat([market_data_df,coin_data_df],ignore_index=True)
    except:
        pass

print(1)
market_data_df['date']=market_data_df['date']/1000
print(2)

market_data_df["date"] = pd.to_datetime(market_data_df["date"], unit="s")
market_data_df['date'] = market_data_df['date'].dt.to_period('D').dt.to_timestamp()


market_data_df.drop_duplicates(subset=['date','token'], keep='first', inplace=True)
market_data_df


# coins_df = coins_df[['id', 'name']]
# map_token_list = coins_df[coins_df['id'].isin(token_list)]
# map_token_list

#calculating 30 day price change
market_data_df = market_data_df.sort_values(by=['token', 'date'])
market_data_df['price_change_30d'] = market_data_df.groupby('token')['prices'].pct_change(periods=30) * 100
market_data_df = market_data_df[market_data_df['date']==market_data_df['date'].max()]

market_data_df = market_data_df[market_data_df['date']==market_data_df['date'].max()]
market_data_df

#fully diluated market cap
import requests
fully_diluted_mcap = pd.DataFrame()
for i in range(len(token_list)):
    headers = {
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
        'cache-control': 'max-age=0',
        # 'cookie': '_cc_id=775f68c8131637f213faf7dbc28f380c; _au_1d=AU1D-0100-001700044659-GEQ6FTCC-4MAD; cookie_notice_accept=1; _au_last_seen_pixels=eyJhcG4iOjE3MDQyNjA0NzAsInR0ZCI6MTcwNDI2MDQ3MCwicHViIjoxNzA0MjYwNDcwLCJydWIiOjE3MDQyNjA0NzAsInRhcGFkIjoxNzA0MjYwNDcwLCJhZHgiOjE3MDQyNjA0NzAsImdvbyI6MTcwNDI2MDQ3MCwic21hcnQiOjE3MDQyNjA1MDUsImltcHIiOjE3MDQyNjA1MDcsInNvbiI6MTcwNDI2MDQ3MCwiYmVlcyI6MTcwNDI2MDUwNSwiY29sb3NzdXMiOjE3MDQyNjA1MDUsImFtbyI6MTcwNDI2MDUwNSwiYWRvIjoxNzA0MjYwNTA1LCJvcGVueCI6MTcwNDI2MDUwNSwidW5ydWx5IjoxNzA0MjYwNTA1LCJ0YWJvb2xhIjoxNzA0MjYwNTA1LCJpbmRleCI6MTcwNDI2MDQ3MCwicHBudCI6MTcwNDI2MDUwNX0%3D; _ga=GA1.1.1762277819.1700044653; _uc_referrer=https://www.google.com/; optimize_uuid=2cf9a0aa221593149b1004353dfa2f657c3ed62f2d17864275; _gcl_au=1.1.36717941.1707894211; __cuid=8f6542c1dcba4a45a5ce9bf8d12db9ef; amp_fef1e8=17dd2c29-3767-433e-9310-f09fc37f80a6R...1hn3dhunj.1hn3eid8f.3.0.3; _ga_4PE9HX30YZ=GS1.1.1708438123.1.1.1708439795.0.0.0; _cq_duid=1.1712128835.57Q9ClfFwSZDfVQA; selected_iso=usd; remember_user_token=eyJfcmFpbHMiOnsibWVzc2FnZSI6Ilcxc3hNemd5TXpNME0xMHNJaVF5WVNReE1DUlllVWhzYVc1MmJXOVpUblJDWmxJeGQyRkdkMkpQSWl3aU1UY3hNemM0TkRRNE5pNDBOVGd6TXpVMklsMD0iLCJleHAiOiIyMDI0LTA1LTIwVDExOjE0OjQ2LjQ1OFoiLCJwdXIiOiJjb29raWUucmVtZW1iZXJfdXNlcl90b2tlbiJ9fQ%3D%3D--8c76870ee463c26710551fb09f3b9046a94894e2; __stripe_mid=970cd071-beb4-4222-9bc5-bf823d4d9523a2a913; hubspotutk=767356041fa3f4d93a81809594ef3054; _cq_suid=1.1714373337.d3nyeLS2oG74tpah; cf_clearance=22Hp44swBk52JqlF1Fgif2H8muGpq2ppFWfxDD.J1ko-1714374363-1.0.1.1-V5IAU1l_.ckZ9U.03TihNVNRMPvDzXCojRoxRGmboz32xUUUZx2oI5R0Br1Ja.yAf.q4NdXPs308CrbaxzafqQ; __hstc=234611265.767356041fa3f4d93a81809594ef3054.1713784524510.1713784524510.1714388686002.2; __hssrc=1; _hjSessionUser_3096373=eyJpZCI6IjJiNDAzNmQ0LTM5ZDQtNWQ0NC04NmNlLThkNjY1N2QzNDA1NiIsImNyZWF0ZWQiOjE3MTQzODg2ODUzMDAsImV4aXN0aW5nIjp0cnVlfQ==; mp_4e762b32525bd7ecddda43ce51776343_mixpanel=%7B%22distinct_id%22%3A%20%22%24device%3A18cc8eef9757c1-0fd151e51af53f-1f525637-157188-18cc8eef9757c1%22%2C%22%24device_id%22%3A%20%2218cc8eef9757c1-0fd151e51af53f-1f525637-157188-18cc8eef9757c1%22%2C%22%24search_engine%22%3A%20%22google%22%2C%22%24initial_referrer%22%3A%20%22https%3A%2F%2Fwww.google.com%2F%22%2C%22%24initial_referring_domain%22%3A%20%22www.google.com%22%2C%22__mps%22%3A%20%7B%7D%2C%22__mpso%22%3A%20%7B%22%24initial_referrer%22%3A%20%22https%3A%2F%2Fwww.google.com%2F%22%2C%22%24initial_referring_domain%22%3A%20%22www.google.com%22%7D%2C%22__mpus%22%3A%20%7B%7D%2C%22__mpa%22%3A%20%7B%7D%2C%22__mpu%22%3A%20%7B%7D%2C%22__mpr%22%3A%20%5B%5D%2C%22__mpap%22%3A%20%5B%5D%7D; _session_id=b657903aa0299419ebfec9f92041b6fc; OptanonConsent=isGpcEnabled=0&datestamp=Mon+May+13+2024+11%3A27%3A29+GMT%2B0530+(India+Standard+Time)&version=202312.1.0&browserGpcFlag=0&isIABGlobal=false&hosts=&consentId=4ccc8ca6-aa1d-4bba-8c6e-c9245bf79771&interactionCount=1&landingPath=NotLandingPage&groups=C0001%3A1%2CC0002%3A1%2CC0004%3A1%2CC0003%3A1&AwaitingReconsent=false; _ga_1Y6C78JXR3=GS1.1.1715579420.181.1.1715579849.0.0.0; __gads=ID=9ab8fa3c042baf1e:T=1700044657:RT=1715580058:S=ALNI_MZ6qNOWtibSp2iM267SxkxqrS1YGA; __gpi=UID=00000c87cc192efd:T=1700044657:RT=1715580058:S=ALNI_MbNOh8E2v_n1PZaHUvetMCp_lBFoA; __eoi=ID=1a3aa11a5e6c73f8:T=1706855080:RT=1715580058:S=AA-Afjb1_FkUCVAU4vf9nalz3tjC; _ga_LJR3232ZPB=GS1.1.1715579420.265.1.1715580088.0.0.0',
        'priority': 'u=0, i',
        'referer': 'https://www.coingecko.com/en/coins/api3',
        'sec-ch-ua': '"Chromium";v="124", "Google Chrome";v="124", "Not-A.Brand";v="99"',
        'sec-ch-ua-arch': '"arm"',
        'sec-ch-ua-bitness': '"64"',
        'sec-ch-ua-full-version': '"124.0.6367.79"',
        'sec-ch-ua-full-version-list': '"Chromium";v="124.0.6367.79", "Google Chrome";v="124.0.6367.79", "Not-A.Brand";v="99.0.0.0"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-model': '""',
        'sec-ch-ua-platform': '"macOS"',
        'sec-ch-ua-platform-version': '"13.3.0"',
        'sec-fetch-dest': 'document',
        'sec-fetch-mode': 'navigate',
        'sec-fetch-site': 'same-origin',
        'sec-fetch-user': '?1',
        'upgrade-insecure-requests': '1',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
    }

    response = requests.get(f'https://www.coingecko.com/en/coins/{token_list[i]}',headers=headers, proxies=proxies, verify=False)
    tree = etree.HTML(response.text)
    data = tree.xpath('//*[@id="gecko-coin-page-container"]/div[6]/div[2]/table/tbody/tr[3]/td/span/text()')
    print(response)
    temp = pd.DataFrame({'token': [token_list[i]], 'fdmcap': data})
    temp['fdmcap'] = temp['fdmcap'].str.replace('$', '').str.replace(',', '').astype(float)
    fully_diluted_mcap = pd.concat([fully_diluted_mcap, temp], ignore_index=True)
fully_diluted_mcap





#combining fdv and market data
market_data = pd.merge(market_data_df, fully_diluted_mcap, on='token', how='inner')
market_data

market_data['market_cap'] = market_data['market_cap']/10**6
market_data['fdmcap'] = market_data['fdmcap']/10**6

floor_columns(market_data, 'market_cap', 'fdmcap')

market_data.to_csv('market_data_relative_metric.csv', index=False)
market_data

market_data.to_csv('market_data_relative_metric.csv', index=False)
market_data = pd.read_csv('market_data_relative_metric.csv')

#logo url for project

token_list =['planetwatch','hivemapper','geodnet','foam-protocol','dimo','meson-network','theta-token','swarm','helium','xnet-mobile','wifi','wicrypt']
import pandas as pd
image_urls = pd.read_csv('/content/sample_data/coin_image_url.csv')
image_urls = image_urls[image_urls['token'].isin(token_list)]

project = ['Planet Watch','Meson', 'Swarm', 'THETA','DIMO Network', 'FOAM', 'GEODNET', 'Hivemapper','Helium', 'HelixNetwork', 'Wifi Map', 'Wicrypt']
token = ['planetwatch','meson-network', 'swarm', 'theta-token','dimo', 'foam-protocol', 'geodnet', 'hivemapper','helium', 'xnet-mobile', 'wifi', 'wicrypt']
token_mapp = pd.DataFrame({'project': project, 'token': token})




image_urls = image_urls.merge(token_mapp, on='token', how='left')
image_urls.drop(columns=['token'], inplace=True)
image_urls

"""## CDN Network Data"""

import pandas as pd
import requests

cdn_lsts = ['Meson', 'THETA', 'Swarm']
weekly_commit_data = pd.DataFrame()
for i in range(len(cdn_lsts)):
    headers = {
        'sec-ch-ua': '"Chromium";v="124", "Google Chrome";v="124", "Not-A.Brand";v="99"',
        'Referer': 'https://app.artemis.xyz/',
        'sec-ch-ua-mobile': '?0',
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
        'sec-ch-ua-platform': '"macOS"',
    }

    params = {
        'ecosystem': f'{cdn_lsts[i]}',
        'includeForks': 'false',
        'daysBack': '3650',
    }

    response = requests.get('https://api.artemisxyz.com/weekly-commits/', params=params, headers=headers)
    data = response.json()
    temp = pd.DataFrame(data)

    temp['project'] = cdn_lsts[i]

    temp.columns = ['date', 'weekly_commits', 'project']
    weekly_commit_data = pd.concat([weekly_commit_data, temp], ignore_index=True)


weekly_commit_data.drop_duplicates(subset=['project'], keep='last', inplace=True)
weekly_commit_data.drop(columns={'date'}, inplace=True)



#weekly Active Developer data


active_devs = pd.DataFrame()
for i in range(len(cdn_lsts)):
    import requests

    headers = {
        'sec-ch-ua': '"Chromium";v="124", "Google Chrome";v="124", "Not-A.Brand";v="99"',
        'Referer': 'https://app.artemis.xyz/',
        'sec-ch-ua-mobile': '?0',
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
        'sec-ch-ua-platform': '"macOS"',
    }

    params = {
        'ecosystem': 'THETA',
        'includeForks': 'false',
        'daysBack': '3650',
    }

    response = requests.get('https://api.artemisxyz.com/weekly-active-devs/', params=params, headers=headers)
    data = response.json()
    temp = pd.DataFrame(data)

    temp['project'] = cdn_lsts[i]

    temp.columns = ['date', 'active_devs', 'project']
    active_devs = pd.concat([active_devs, temp], ignore_index=True)

active_devs = active_devs[active_devs['date']== active_devs['date'].max()]
active_devs.drop(columns={'date'}, inplace=True)
active_devs


#combining all data
commit_devs_data = pd.merge(weekly_commit_data, active_devs, on='project', how='left')
commit_devs_data

owner_list = ['MesonFi', 'filecoin-saturn', 'ethersphere']
repo_name = ['meson-to', 'L1-node','swarm']
project = ['Meson', 'Filecoin Saturn', 'Swarm']


cdn_github_data = pd.DataFrame()
for i in range(len(owner_list)):
    headers = {"Authorization": "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"}
    url = (f"https://api.github.com/repos/{owner_list[i]}/{repo_name[i]}")
    response = requests.get(url, headers=headers, proxies=proxies, verify=False)
    print(url)
    print(response)
    if response.status_code == 200:

        stars = response.json()['stargazers_count']
        watchers = response.json()['subscribers_count']
        temp = pd.DataFrame([[repo_name[i], stars, watchers]], columns=['project', 'stars', 'watchers'])
        temp['project'] = project[i]
        cdn_github_data = pd.concat([cdn_github_data, temp], ignore_index=True)

cdn_github_data

#stars and watchers devs data for theta token
owner_lists = ['thetatoken', 'thetatoken', 'thetatoken', 'thetatoken','thetatoken','thetatoken']
repos_name = ['theta-protocol-ledger', 'theta-protocol-subchain','theta-infrastructure-ledger-explorer','guardian-mainnet-guide','theta-eth-rpc-adaptor','theta-metachain-guide']
theta_github_data = pd.DataFrame()
for i in range(len(owner_lists)):
    headers = {"Authorization": "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"}
    url = (f"https://api.github.com/repos/{owner_lists[i]}/{repos_name[i]}")
    response = requests.get(url, headers=headers, proxies=proxies, verify=False)
    print(url)
    print(response)
    if response.status_code == 200:

        stars = response.json()['stargazers_count']
        watchers = response.json()['subscribers_count']
        temp = pd.DataFrame([[ stars, watchers]], columns=['stars', 'watchers'])
        temp['project'] = 'THETA'
        theta_github_data = pd.concat([theta_github_data, temp], ignore_index=True)
theta_github_data = theta_github_data.groupby('project')[['stars', 'watchers']].sum().reset_index()

#combining all data of stars and watchers

cdn_github_data = pd.concat([cdn_github_data, theta_github_data], ignore_index=True)
cdn_github_data

#meson data
headers = {
    'Accept-Language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'Connection': 'keep-alive',
    'Origin': 'https://explorer.meson.network:1984',
    'Referer': 'https://explorer.meson.network:1984/d/mzO2Tpg4k/home?orgId=2&viewPanel=54',
    'Sec-Fetch-Dest': 'empty',
    'Sec-Fetch-Mode': 'cors',
    'Sec-Fetch-Site': 'same-origin',
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
    'accept': 'application/json, text/plain, */*',
    'content-type': 'application/json',
    'sec-ch-ua': '"Chromium";v="124", "Google Chrome";v="124", "Not-A.Brand";v="99"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'x-grafana-org-id': '2',
}

json_data = {
    'queries': [
        {
            'datasource': {
                'uid': 'OcH94YC4k',
                'type': 'influxdb',
            },
            'query': 'import "strings"\n\nfrom(bucket: "meson_prod")\n  |> range(start: v.timeRangeStart, stop:v.timeRangeStop)\n  |> filter(fn: (r) => r["_measurement"] == "meson_prod")\n  |> filter(fn: (r) => r["_field"] == "meson.node_count.all.gauge")\n  |> last()',
            'refId': 'A',
            'datasourceId': 13,
            'intervalMs': 3600000,
            'maxDataPoints': 692,
        },
    ],
    'range': {
        'from': '2024-05-16T07:35:29.901Z',
        'to': '2024-05-20T07:35:29.901Z',
        'raw': {
            'from': 'now-4d',
            'to': 'now',
        },
    },
    'from': '1715844929901',
    'to': '1716190529901',
}

response = requests.post('https://explorer.meson.network:1984/api/ds/query', headers=headers, json=json_data)
data = response.json()['results']['A']['frames'][0]['data']['values']
meson_data = pd.DataFrame(data)
meson_data.drop(0, inplace=True)
meson_data['project'] = 'Meson'
meson_data.columns = ['no_of_nodes', 'project']
meson_data

#Filecoin Saturn
fil_dat = pd.DataFrame({'no_of_nodes':[115], 'project':['Filecoin Saturn']})
# fil_dat['project'] = 'Filecoin Saturn'
fil_dat

#swarm data

headers = {
    'accept': '*/*',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    # 'cookie': 'secid=hdp27rsc0mae3jerc2ppm63cmr',
    'priority': 'u=1, i',
    'referer': 'https://swarmscan.io/',
    'sec-ch-ua': '"Chromium";v="124", "Google Chrome";v="124", "Not-A.Brand";v="99"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-origin',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
}
response = requests.get('https://swarmscan.io/i/network/basic-info', headers=headers)
data = response.json()['countDay']
swarm_data = pd.DataFrame([data])
swarm_data['project'] = 'Swarm'
swarm_data.columns = ['no_of_nodes', 'project']
swarm_data




#theta data

import requests
headers = {
    'accept': 'application/json, text/plain, */*',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'origin': 'https://explorer.thetatoken.org',
    'priority': 'u=1, i',
    'referer': 'https://explorer.thetatoken.org/',
    'sec-ch-ua': '"Chromium";v="124", "Google Chrome";v="124", "Not-A.Brand";v="99"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-site',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
    'cache-control': 'no-cache',  # Added to prevent caching
    'pragma': 'no-cache',  # Added to prevent caching
}
params = {
    'type': 'tfuel',
}
response = requests.get('https://explorer-api.thetatoken.org/api/stake/totalAmount', params=params, headers=headers)
if response.status_code == 200:
    print("Data fetched successfully")
else:
    print(f"Failed to fetch data. Status code: {response.status_code}")
print(response.json())  # If the response is in JSON format, print the JSON content
data = response.json()['body']['totalNodes']
theta_data = pd.DataFrame([data])
theta_data['project'] = 'THETA'
theta_data.columns = ['no_of_nodes', 'project']
theta_data

#combining all data

all_data = pd.concat([fil_dat, meson_data, swarm_data, theta_data])
all_data


#combing stars and watchers data
cdn_data = pd.merge(all_data, cdn_github_data, on='project', how='inner')
cdn_data = pd.merge(cdn_data, commit_devs_data, on='project', how='left')
cdn_data


project = ['Meson', 'Swarm', 'THETA']
token = ['meson-network', 'swarm', 'theta-token']
token_map = pd.DataFrame({'project': project, 'token': token})
token_map

cdn_market_data = pd.merge(market_data, token_map, on='token', how='left')
cdn_market_data

from datetime import date

# today = date.today()

q = """select A.* , B.market_cap,B.fdmcap, B.price_change_30d from cdn_data as A
left join cdn_market_data as B on A.project = B.project"""
dash3_cdn_data = pysqldf(q)
dash3_cdn_data['date'] = date.today()
dash3_cdn_data = dash3_cdn_data[['date', 'project', 'no_of_nodes', 'market_cap', 'fdmcap', 'price_change_30d', 'stars', 'watchers','weekly_commits', 'active_devs' ]]
dash3_cdn_data['date'] = pd.to_datetime(dash3_cdn_data['date'])

#adding image urls
dash3_cdn_data = dash3_cdn_data.merge(image_urls, on = 'project', how='left')
# dash3_cdn_data['market_cap'] = dash3_cdn_data['market_cap']/10**6
# dash3_cdn_data['fdmcap'] = dash3_cdn_data['fdmcap']/10**6

# floor_columns(dash3_cdn_data, 'market_cap', 'fdmcap')
dash3_cdn_data.to_csv('dash3_cdn_data.csv', index=False)

dash3_cdn_data

cdn_pre_data = read_full_table('raw_m31', 'cdn_hist')
cdn_pre_data =cdn_pre_data[cdn_pre_data['date']==cdn_pre_data['date'].max()]
cdn_pre_data

# cdn_pre_data = pd.read_csv('dash3_cdn_data.csv')

q = """select A.* , B.no_of_nodes as pre_no_of_nodes from dash3_cdn_data
as A left join cdn_pre_data as b on A.project = B.project"""

dash3_cdn = pysqldf(q)
dash3_cdn['no_of_nodes_change_7d'] = ((dash3_cdn['no_of_nodes'] - dash3_cdn['pre_no_of_nodes']) / dash3_cdn['pre_no_of_nodes'])*100
dash3_cdn.drop(columns='pre_no_of_nodes', inplace=True)
dash3_cdn

dash3_cdn.loc[dash3_cdn['project'] == 'Filecoin Saturn', 'image_url'] = np.nan
dash3_cdn

dash3_cdn['date'] = pd.to_datetime(dash3_cdn['date'])
provider = 'm31'
dataset = 'cdn_data'

create_or_overwrite_table(dash3_cdn, provider, dataset)

dash3_cdn

cdn_hist = read_full_table('raw_m31', 'cdn_hist')
cdn_hist = pd.concat([dash3_cdn, cdn_hist ], ignore_index=True)
cdn_hist['date'] = pd.to_datetime(cdn_hist['date'])

provider = 'm31'
dataset = 'cdn_hist'

create_or_overwrite_table(cdn_hist, provider, dataset)

cdn_hist

"""## Sensor Networks"""

lst = ['FOAM', 'DIMO Network', 'WeatherXM']

weekly_commits = pd.DataFrame()
for i in range(len(lst)):
    headers = {
        'sec-ch-ua': '"Chromium";v="124", "Google Chrome";v="124", "Not-A.Brand";v="99"',
        'Referer': 'https://app.artemis.xyz/',
        'sec-ch-ua-mobile': '?0',
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
        'sec-ch-ua-platform': '"macOS"',
    }

    params = {
        'ecosystem': f'{lst[i]}',
        'includeForks': 'false',
        'daysBack': '3650',
    }

    response = requests.get('https://api.artemisxyz.com/weekly-commits/', params=params, headers=headers)
    data = response.json()
    temp = pd.DataFrame(data)
    temp.columns = ['date', 'weekly_commits']
    temp['project'] = lst[i]
    weekly_commits = pd.concat([weekly_commits, temp], axis=0)

weekly_commits = weekly_commits[weekly_commits['date'] == weekly_commits['date'].max()]
weekly_commits


#weekly devs data

active_devs = pd.DataFrame()
for i in range(len(lst)):
    headers = {
        'sec-ch-ua': '"Chromium";v="124", "Google Chrome";v="124", "Not-A.Brand";v="99"',
        'Referer': 'https://app.artemis.xyz/',
        'sec-ch-ua-mobile': '?0',
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
        'sec-ch-ua-platform': '"macOS"',
    }

    params = {
        'ecosystem': f'{lst[i]}',
        'includeForks': 'false',
        'daysBack': '3650',
    }

    response = requests.get('https://api.artemisxyz.com/weekly-active-devs/', params=params, headers=headers)

    data = response.json()
    temp = pd.DataFrame(data)
    temp.columns = ['date', 'active_devs']
    temp['project'] = lst[i]
    active_devs = pd.concat([active_devs, temp], axis=0)

active_devs = active_devs[active_devs['date'] == active_devs['date'].max()]
active_devs

#combining weekly commits and devs data

commits_devs_data = pd.merge(weekly_commits, active_devs, on=['date','project'], how='inner')
commits_devs_data.drop(columns=['date'], inplace=True)
commits_devs_data

#dimo network

owner = ['DIMO-Network', 'DIMO-Network', 'DIMO-Network']
repo = ['dimo-identity', 'DIP', 'device-definitions-api']

dimo_github_data = pd.DataFrame()
for i in range(len(owner)):
    headers = {"Authorization": "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"}
    url = (f"https://api.github.com/repos/{owner[i]}/{repo[i]}")
    response = requests.get(url, headers=headers, proxies=proxies, verify=False)
    print(url)
    print(response)
    if response.status_code == 200:

        stars = response.json()['stargazers_count']
        watchers = response.json()['subscribers_count']
        temp = pd.DataFrame([[ stars, watchers]], columns=['stars', 'watchers'])
        temp['project'] = 'DIMO Network'
        dimo_github_data = pd.concat([dimo_github_data, temp], ignore_index=True)
dimo_github_data = dimo_github_data.groupby('project')[['stars', 'watchers']].sum().reset_index()
dimo_github_data


#GEODNET github data
repos = ['whitepaper', 'GEODToken', 'GIP']

geodnet_github_data = pd.DataFrame()
for i in range(len(repos)):
    headers = {"Authorization": "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"}
    url = (f"https://api.github.com/repos/geodnet/{repos[i]}")
    response = requests.get(url, headers=headers, proxies=proxies, verify=False)
    print(url)
    print(response)
    if response.status_code == 200:

        stars = response.json()['stargazers_count']
        watchers = response.json()['subscribers_count']
        temp = pd.DataFrame([[ stars, watchers]], columns=['stars', 'watchers'])
        temp['project'] = 'GEODNET'
        geodnet_github_data = pd.concat([geodnet_github_data, temp], ignore_index=True)
geodnet_github_data = geodnet_github_data.groupby('project')[['stars', 'watchers']].sum().reset_index()
geodnet_github_data

#hivemapper github data

headers = {"Authorization": "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"}
url = (f"https://api.github.com/repos/Hivemapper/hive-py")
response = requests.get(url, headers=headers, proxies=proxies, verify=False)
print(url)
print(response)
if response.status_code == 200:

    stars = response.json()['stargazers_count']
    watchers = response.json()['subscribers_count']
    hivemapper_github_data = pd.DataFrame([[ stars, watchers]], columns=['stars', 'watchers'])
    hivemapper_github_data['project'] = 'Hivemapper'
hivemapper_github_data


#foam github data
repos = ['kepler','sample-nft-project', 'purescript-deck-gl', 'chanterelle', 'purescript-web3', 'cliquebait']
foam_github_data = pd.DataFrame()
for i in range(len(repos)):
    headers = {"Authorization": "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"}
    url = (f"https://api.github.com/repos/f-o-a-m/{repos[i]}")
    response = requests.get(url, headers=headers, proxies=proxies, verify=False)
    print(url)
    print(response)
    if response.status_code == 200:

        stars = response.json()['stargazers_count']
        watchers = response.json()['subscribers_count']
        temp = pd.DataFrame([[ stars, watchers]], columns=['stars', 'watchers'])
        temp['project'] = 'FOAM'
        foam_github_data = pd.concat([foam_github_data, temp], ignore_index=True)
foam_github_data = foam_github_data.groupby('project')[['stars', 'watchers']].sum().reset_index()
foam_github_data



repo = ['wxm-android', 'wxm-ios', 'docs']
weatherxm_github_data = pd.DataFrame()
for i in range(len(repo)):
    headers = {"Authorization": "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"}
    url = (f"https://api.github.com/repos/WeatherXM/{repo[i]}")
    response = requests.get(url, headers=headers, proxies=proxies, verify=False)
    print(url)
    print(response)
    if response.status_code == 200:

        stars = response.json()['stargazers_count']
        watchers = response.json()['subscribers_count']
        temp = pd.DataFrame([[ stars, watchers]], columns=['stars', 'watchers'])
        temp['project'] = 'WeatherXM'
        weatherxm_github_data = pd.concat([weatherxm_github_data, temp], ignore_index=True)
weatherxm_github_data = weatherxm_github_data.groupby('project')[['stars', 'watchers']].sum().reset_index()
weatherxm_github_data


repos = ['pw-pipeline', 'docker-plane-watch', 'pw-feeder']
plane_github_data = pd.DataFrame()
for i in range(len(repos)):
    headers = {"Authorization": "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"}
    url = (f"https://api.github.com/repos/plane-watch/{repos[i]}")
    response = requests.get(url, headers=headers, proxies=proxies, verify=False)
    print(url)
    print(response)
    if response.status_code == 200:

        stars = response.json()['stargazers_count']
        watchers = response.json()['subscribers_count']
        temp = pd.DataFrame([[ stars, watchers]], columns=['stars', 'watchers'])
        temp['project'] = 'Planet Watch'
        plane_github_data = pd.concat([plane_github_data, temp], ignore_index=True)
plane_github_data = plane_github_data.groupby('project')[['stars', 'watchers']].sum().reset_index()
plane_github_data


#combining all github data
github_data = pd.concat([dimo_github_data, geodnet_github_data, hivemapper_github_data, foam_github_data, weatherxm_github_data, plane_github_data], ignore_index=True)
github_data

#comining github data and weekly commit and devs data
combine_data = pd.merge(github_data, commits_devs_data, on='project', how='left')
combine_data

import requests
import pandas as pd
headers = {
    'Accept': 'application/json, text/plain, */*',
    'Accept-Language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'Authorization': 'Bearer undefined',
    'Connection': 'keep-alive',
    'Referer': 'https://explorer.planetwatch.io/',
    'Sec-Fetch-Dest': 'empty',
    'Sec-Fetch-Mode': 'cors',
    'Sec-Fetch-Site': 'same-origin',
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
    'sec-ch-ua': '"Chromium";v="124", "Google Chrome";v="124", "Not-A.Brand";v="99"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'Cache-Control': 'no-cache',  # Added to prevent caching
    'Pragma': 'no-cache',  # Added to prevent caching
}
response = requests.get('https://explorer.planetwatch.io/api/planetwatch/sensors/count', headers=headers)
data = response.json()['data']
planetwatch_data = pd.DataFrame(data)
planetwatch_data['total'] = planetwatch_data['type1'] + planetwatch_data['type2'] + planetwatch_data['type3'] + planetwatch_data['type4']
planetwatch_data.drop(columns = ['type1','type2','type3','type4'], inplace = True)
planetwatch_data['project'] = 'Planet Watch'
planetwatch_data.columns = ['total_sensor', 'project']
planetwatch_data.reset_index(drop=True, inplace=True)
planetwatch_data

import requests

headers = {
    'accept': 'application/json, text/plain, */*',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'priority': 'u=1, i',
    'referer': 'https://explorer.dimo.zone/',
    'sec-ch-ua': '"Chromium";v="124", "Google Chrome";v="124", "Not-A.Brand";v="99"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-origin',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
}

params = {
    'make': '',
}

response = requests.get('https://explorer.dimo.zone/api/v1/connected/dune', params=params, headers=headers)
data = response.json()['count']
dimo = pd.DataFrame({'car_count': [data], 'project': ['DIMO Network']})
dimo

## WeatherXM Project
import requests
import numpy as np
headers = {
    'accept': 'application/json, text/plain, */*',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'common': '[object Object]',
    'if-none-match': 'W/"12c1-1P0lUh0xwfbCx4saFpc55bNddpo"',
    'origin': 'https://explorer.weatherxm.com',
    'priority': 'u=1, i',
    'sec-ch-ua': '"Chromium";v="124", "Google Chrome";v="124", "Not-A.Brand";v="99"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-site',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
}
response = requests.get('https://api.weatherxm.com/api/v1/network/stats', headers=headers)
data = response.json()['weather_stations']['onboarded']['total']
weatherxm = pd.DataFrame({'total_sensor':[data],'stations':np.nan, 'project':['WeatherXM']})
weatherxm

## GeoDNET Project
# import requests
headers = {
    'X-Dune-API-Key': 'your-dune-api-key',
}
params = {
    'limit': '10000',
}
response = requests.get('https://api.dune.com/api/v1/query/3378710/results', params=params, headers=headers)
data = response.json()['result']['rows'][0]['total_miner']
geodnet = pd.DataFrame({'stations':[data],'miners':np.nan, 'project':['GEODNET']})
geodnet

#Natix data

headers = {
    'Accept': '*/*',
    'Accept-Language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'Connection': 'keep-alive',
    'Origin': 'https://www.natix.network',
    'Referer': 'https://www.natix.network/',
    'Sec-Fetch-Dest': 'empty',
    'Sec-Fetch-Mode': 'cors',
    'Sec-Fetch-Site': 'cross-site',
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
    'sec-ch-ua': '"Chromium";v="124", "Google Chrome";v="124", "Not-A.Brand";v="99"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
}
response = requests.get('https://l3s-analytics-apim.azure-api.net/stats/nnwebsitemetrics', headers=headers)
data = response.json()
natix = pd.DataFrame([data])
natix.drop(columns={'totalDetections'}, inplace=True)
natix['project'] = 'Natix'
natix.columns = [ 'km_mapped','total_contributors','project']
natix['register_driver'] = np.nan
natix

import requests
import json
headers = {
    'accept': '*/*',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'cache-control': 'no-cache',
    'content-type': 'application/json',
    'origin': 'https://hivemapper.com',
    'priority': 'u=1, i',
    'referer': 'https://hivemapper.com/',
    'sec-ch-ua': '"Chromium";v="124", "Google Chrome";v="124", "Not-A.Brand";v="99"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-site',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
    'pragma': 'no-cache'
}
response = requests.get('https://www.hivemapper.com/api/explorer', headers=headers)
#combine all data
data = response.json()['stats']['byDay'][list(response.json()['stats']['byDay'].keys())[-1]]
hivemapper = pd.DataFrame([data])
hivemapper['project'] = 'Hivemapper'
hivemapper = hivemapper[['totalContributors','totalKm','project']]
hivemapper.columns = ['total_contributors','km_mapped','project']
hivemapper

#combine all data

sensor_data = pd.concat([planetwatch_data, dimo, weatherxm, geodnet, natix, hivemapper], axis=0)
sensor_data.reset_index(drop=True, inplace=True)


#combining github data and all metrics
sensor_data = sensor_data.merge(combine_data, how='outer', on='project')




project_df = pd.DataFrame({'project':[ 'Onocoy']})
sensor_data = pd.concat([sensor_data, project_df], axis=0)
sensor_data.reset_index(drop=True, inplace=True)
sensor_data

project = ['DIMO Network', 'FOAM', 'GEODNET', 'Hivemapper', 'Planet Watch']
token = ['dimo', 'foam-protocol', 'geodnet', 'hivemapper', 'planetwatch']
token_map = pd.DataFrame({'project': project, 'token': token})
token_map

sensor_market_data = pd.merge(market_data, token_map, on='token', how='left')
sensor_market_data

from datetime import date

# today = date.today()

q = """select A.* , B.market_cap,B.fdmcap, B.price_change_30d from sensor_data as A
left join sensor_market_data as B on A.project = B.project"""
sensor_network = pysqldf(q)
sensor_network['date'] = date.today()

sensor_network = sensor_network[['date', 'project', 'total_contributors','register_driver','km_mapped','total_sensor','stations','miners','car_count','market_cap', 'fdmcap', 'price_change_30d', 'stars', 'watchers','weekly_commits', 'active_devs' ]]
sensor_network['date'] = pd.to_datetime(sensor_network['date'])
sensor_network

#mapping sensor data
mapping_data = sensor_network[sensor_network['project'].isin(['Hivemapper', 'Natix', 'Spexigon'])]
mapping_data = mapping_data[['date', 'project', 'total_contributors','register_driver','km_mapped', 'market_cap', 'fdmcap', 'price_change_30d', 'stars', 'watchers', 'weekly_commits', 'active_devs']]
mapping_data


# adding image urls
mapping_data = mapping_data.merge(image_urls, on='project', how='left')


mapping_data['km_mapped'] = mapping_data['km_mapped']/10**6

mapping_data['km_mapped'] = mapping_data['km_mapped'].round(0)


mapping_data

# mapping_sensor = pd.read_csv('mapping.csv')
mapping_sensor = read_full_table('raw_m31', 'mapping_sensor_network_hist')
mapping_sensor = mapping_sensor[mapping_sensor['date'] == mapping_sensor['date'].max() ]
mapping_sensor

q = """select A.*, B.total_contributors as pre_total_contributors, B.km_mapped as pre_km_mapped
from mapping_data A
left join mapping_sensor B
on A.project = B.project"""

mappings_data = pysqldf(q)
# mappings_data.drop('Spexigon', axis=0, inplace=True)
mappings_data['total_contributors_change_7d'] = ((mappings_data['total_contributors'] - mappings_data['pre_total_contributors']) / mappings_data['pre_total_contributors'])*100
mappings_data['km_mapped_change_7d'] = ((mappings_data['km_mapped'] - mappings_data['pre_km_mapped']) / mappings_data['pre_km_mapped'])*100

mappings_data.drop(columns=['pre_km_mapped','pre_total_contributors'], inplace=True)
mappings_data

mappings_data['weekly_commits'] = np.nan
mappings_data['active_devs'] = np.nan
mappings_data['register_driver'] = np.nan

mappings_data['date'] = pd.to_datetime(mappings_data['date'])
mappings_data.to_csv('mapping_data.csv', index=False)

provider = 'm31'
dataset = 'mapping_sensor_network'

create_or_overwrite_table(mappings_data, provider, dataset)
mappings_data

mapping_senso_hist = read_full_table('raw_m31', 'mapping_sensor_network_hist')

mapping_senso_hist = pd.concat([mappings_data,mapping_senso_hist],ignore_index=True)

provider = 'm31'
dataset = 'mapping_sensor_network_hist'

create_or_overwrite_table(mapping_senso_hist, provider, dataset)

mapping_senso_hist

#positioning sensor data
positioning_data = sensor_network[sensor_network['project'].isin(['GEODNET', 'FOAM', 'Onocoy'])]
positioning_data = positioning_data[['date', 'project', 'miners','stations', 'market_cap', 'fdmcap', 'price_change_30d', 'stars', 'watchers', 'weekly_commits', 'active_devs']]
positioning_data


#adding image urls
positioning_data = positioning_data.merge(image_urls, on='project', how='left')
positioning_data['miners'] = np.nan

positioning_data_hist = read_full_table('raw_m31', 'positioning_sensor_network')
positioning_data_hist =positioning_data_hist[positioning_data_hist['date'] == positioning_data_hist['date'].max()]

q = """select A.*, B.stations as pre_stations
from positioning_data A
left join positioning_data_hist B
on A.project = B.project"""

positioning_data = pysqldf(q)
positioning_data['stations_change_7d'] = ((positioning_data['stations'] - positioning_data['pre_stations']) / positioning_data['pre_stations'])*100

positioning_data.drop(columns=['pre_stations'], inplace=True)
positioning_data


positioning_data['date'] = pd.to_datetime(positioning_data['date'])
positioning_data.to_csv('positioning_sensor.csv', index=False)

positioning_data['miners'] = np.nan

positioning_data['active_devs'] = np.nan
positioning_data['weekly_commits'] = np.nan

provider = 'm31'
dataset = 'positioning_sensor_network'

create_or_overwrite_table(positioning_data, provider, dataset)

positioning_data

positioning_sensor_hist = read_full_table('raw_m31', 'positioning_sensor_network')
positioning_sensor_hist = pd.concat([positioning_data, positioning_sensor_hist], ignore_index=True)

provider = 'm31'
dataset = 'positioning_sensor_hist'

create_or_overwrite_table(positioning_sensor_hist, provider, dataset)
positioning_sensor_hist

# #climate sensor data
climate_data = sensor_network[sensor_network['project'].isin(['Planet Watch', 'WeatherXM'])]
climate_data = climate_data[['date', 'project', 'total_sensor','stations', 'market_cap', 'fdmcap', 'price_change_30d', 'stars', 'watchers', 'weekly_commits', 'active_devs']]
climate_data

# #adding image urls
climate_data = climate_data.merge(image_urls, on='project', how='left')

# # Cast image_url column to string to prevent Athena data type issues
climate_data['image_url'] = climate_data['image_url'].astype('string')


climate_data_pre = read_full_table('raw_m31', 'climate_sensor_hist')
climate_data_pre = climate_data_pre[climate_data_pre['date']==climate_data_pre['date'].max()]
climate_data_pre


q = """select A.*, B.total_sensor as pre_total_sensor
from climate_data A
left join climate_data_pre B
on A.project = B.project"""

climate_data = pysqldf(q)
climate_data['stations_change_7d'] = ((climate_data['total_sensor'] - climate_data['pre_total_sensor']) / climate_data['pre_total_sensor'])*100

# climate_data.drop(columns=['pre_total_sensor'], inplace=True)
# climate_data
# climate_data.loc[climate_data['project']=='WeatherXM', 'stations_change_7d'] =

climate_data['date'] = pd.to_datetime(climate_data['date'])
climate_data.to_csv('climate_senso.csv', index=False)

climate_data['stations'] = np.nan


provider = 'm31'
dataset = 'climate_sensor_network'

create_or_overwrite_table(climate_data, provider, dataset)

climate_data

climate_sensor_hist = read_full_table('raw_m31', 'climate_sensor_hist')
climate_sensor_hist = pd.concat([climate_data, climate_sensor_hist], ignore_index=True)

provider = 'm31'
dataset = 'climate_sensor_hist'

create_or_overwrite_table(climate_sensor_hist, provider, dataset)
# climate_sensor_pre = climate_data_pre[climate_data_pre['date'] == climate_data_pre['date'].max()]
# climate_data_pre
climate_sensor_hist

car_data = sensor_network[sensor_network['project']=='DIMO Network']
car_data = car_data[['date', 'project', 'car_count', 'market_cap', 'fdmcap', 'price_change_30d', 'stars', 'watchers', 'weekly_commits', 'active_devs']]
car_data

#adding image urls
car_data = car_data.merge(image_urls, on='project', how='left')
car_sensor_pre = read_full_table('raw_m31', 'car_sensor_hist')
car_sensor_pre = car_sensor_pre[car_sensor_pre['date']==car_sensor_pre['date'].max()]
q = """select A.*, B.car_count as pre_car_count
from car_data A
left join car_sensor_pre B
on A.project = B.project"""

car_data = pysqldf(q)
car_data['car_count_change_7d'] = ((car_data['car_count'] - car_data['pre_car_count']) / car_data['pre_car_count'])*100

car_data.drop(columns=['pre_car_count'], inplace=True)
car_data


car_data['date'] = pd.to_datetime(car_data['date'])
car_data.to_csv('car_count_pre.csv', index=False)


provider = 'm31'
dataset = 'car_sensor_network'

create_or_overwrite_table(car_data, provider, dataset)

car_data

car_sensor_hist = read_full_table('raw_m31', 'car_sensor_hist')
car_sensor_hist = pd.concat([car_data, car_sensor_hist], ignore_index=True)

provider = 'm31'
dataset = 'car_sensor_hist'

create_or_overwrite_table(car_sensor_hist, provider, dataset)


# car_sensor_pre = car_sensor_hist[car_sensor_hist['date']==car_sensor_hist['date'].max()]
car_sensor_hist

"""## Wireless Network"""

#weekly commit and weekly devs data

weekly_commits = pd.DataFrame()


lsts = ['HelixNetwork', 'Helium']
for i in range(len(lsts)):

    headers = {
        'sec-ch-ua': '"Chromium";v="124", "Google Chrome";v="124", "Not-A.Brand";v="99"',
        'Referer': 'https://app.artemis.xyz/',
        'sec-ch-ua-mobile': '?0',
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
        'sec-ch-ua-platform': '"macOS"',
    }

    params = {
        'ecosystem': f'{lsts[i]}',
        'includeForks': 'false',
        'daysBack': '3650',
    }

    response = requests.get('https://api.artemisxyz.com/weekly-commits/', params=params, headers=headers)
    data = response.json()
    temp = pd.DataFrame(data)
    temp.columns = ['date', 'weekly_commits']
    temp['project'] = lsts[i]
    weekly_commits = pd.concat([weekly_commits, temp], axis=0)

# weekly_commits = weekly_commits[weekly_commits['date'] == weekly_commits['date'].max()]
weekly_commits.drop_duplicates(subset=['project'], keep='last', inplace=True)
weekly_commits

#weekly devs data

active_devs = pd.DataFrame()
for i in range(len(lsts)):
    headers = {
        'sec-ch-ua': '"Chromium";v="124", "Google Chrome";v="124", "Not-A.Brand";v="99"',
        'Referer': 'https://app.artemis.xyz/',
        'sec-ch-ua-mobile': '?0',
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
        'sec-ch-ua-platform': '"macOS"',
    }

    params = {
        'ecosystem': f'{lsts[i]}',
        'includeForks': 'false',
        'daysBack': '3650',
    }

    response = requests.get('https://api.artemisxyz.com/weekly-active-devs/', params=params, headers=headers)

    data = response.json()
    temp = pd.DataFrame(data)
    temp.columns = ['date', 'active_devs']
    temp['project'] = lsts[i]
    active_devs = pd.concat([active_devs, temp], axis=0)

active_devs.drop_duplicates(subset=['project'], keep='last', inplace=True)
active_devs


#combining weekly commits and devs data

commits_devs_data = pd.merge(weekly_commits, active_devs, on=['date','project'], how='inner')
commits_devs_data.drop(columns=['date'], inplace=True)
commits_devs_data

#wifi dabba stars watches data

repo = ['todo-react', 'todo-nodejs', 'dabba']

wifi_dabba_github = pd.DataFrame()
for i in range(len(repo)):
    headers = {"Authorization": "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"}
    url = (f"https://api.github.com/repos/wifidabba/{repo[i]}")
    response = requests.get(url, headers=headers, proxies=proxies, verify=False)
    print(url)
    print(response)
    if response.status_code == 200:

        stars = response.json()['stargazers_count']
        watchers = response.json()['subscribers_count']
        temp = pd.DataFrame([[ stars, watchers]], columns=['stars', 'watchers'])
        temp['project'] = 'Wifi Dabba'
        wifi_dabba_github = pd.concat([wifi_dabba_github, temp], ignore_index=True)
wifi_dabba_github = wifi_dabba_github.groupby('project')[['stars', 'watchers']].sum().reset_index()
wifi_dabba_github


#wayru data

headers = {"Authorization": "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"}
url = (f"https://api.github.com/repos/Wayru-Network/rembg")
response = requests.get(url, headers=headers, proxies=proxies, verify=False)
print(url)
print(response)
if response.status_code == 200:
    stars = response.json()['stargazers_count']
    watchers = response.json()['subscribers_count']
    wayru = pd.DataFrame([[ stars, watchers]], columns=['stars', 'watchers'])
    wayru['project'] = 'Wayru'
wayru

#helium data
headers = {"Authorization": "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"}
url = (f"https://api.github.com/repos/helium/network-explorer")
response = requests.get(url, headers=headers, proxies=proxies, verify=False)
print(url)
print(response)
if response.status_code == 200:
    stars = response.json()['stargazers_count']
    watchers = response.json()['subscribers_count']
    helium = pd.DataFrame([[ stars, watchers]], columns=['stars', 'watchers'])
    helium['project'] = 'Helium'
helium



#pollen data
headers = {"Authorization": "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"}
url = (f"https://api.github.com/repos/pollen-mobile/PIP")
response = requests.get(url, headers=headers, proxies=proxies, verify=False)
print(url)
print(response)
if response.status_code == 200:
    stars = response.json()['stargazers_count']
    watchers = response.json()['subscribers_count']
    pollen_mobile = pd.DataFrame([[ stars, watchers]], columns=['stars', 'watchers'])
    pollen_mobile['project'] = 'Pollen Mobile'
pollen_mobile





#XNET DATA
repo = ['PIP', 'geometry']

xnet_github = pd.DataFrame()
for i in range(len(repo)):
    headers = {"Authorization": "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"}
    url = (f"https://api.github.com/repos/xnet-mobile/{repo[i]}")
    response = requests.get(url, headers=headers, proxies=proxies, verify=False)
    print(url)
    print(response)
    if response.status_code == 200:

        stars = response.json()['stargazers_count']
        watchers = response.json()['subscribers_count']
        temp = pd.DataFrame([[ stars, watchers]], columns=['stars', 'watchers'])
        temp['project'] = 'HelixNetwork'
        xnet_github = pd.concat([xnet_github, temp], ignore_index=True)
xnet_github = xnet_github.groupby('project')[['stars', 'watchers']].sum().reset_index()
xnet_github

#combining all github data
github_data = pd.concat([xnet_github, pollen_mobile, helium, wayru, wifi_dabba_github], ignore_index=True)
github_data


#combining github data with commits and devs data
github_data_combine = pd.merge(github_data, commits_devs_data, on=['project'], how='outer')
github_data_combine

import requests
import re
import json
continent = ['north_america', 'south_america', 'europe', 'asia_pacific', 'middle_east', 'africa']

wifi_count = pd.DataFrame()
for i in range(len(continent)):
    headers = {
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
        'cache-control': 'max-age=0',
        # 'cookie': '_ga=GA1.1.480661980.1715776949; afUserId=cdc556dd-fff0-4473-89c8-55aec8580a30-p; AF_SYNC=1715776949343; _ymab_param=wLnP9a4EQfmP6icSlEZ-P9Ihc1KCrICzgIkMs1a2o-632BztO7G5zCyAmtrwE5MWDUaxsNBd23eCKxGDbb6l-CPECwQ; _ym_uid=1715776950849288728; _ym_d=1715776950; _ym_isad=2; _ym_visorc=b; AWSALB=h9TpD+Hci/JQfUue8+1QFoiSEU0DiGIQSdeJ0290hKWRi3ZwAiaUMFwvjeTgS2NwsWdsluyNUrW2QDOGMQKUozsbxUidDtpx8T43uDAxFrC6+1x49YkJlu7mwbrp; AWSALBCORS=h9TpD+Hci/JQfUue8+1QFoiSEU0DiGIQSdeJ0290hKWRi3ZwAiaUMFwvjeTgS2NwsWdsluyNUrW2QDOGMQKUozsbxUidDtpx8T43uDAxFrC6+1x49YkJlu7mwbrp; _ga_6FJCQXGB8D=GS1.1.1716277720.10.1.1716277743.37.0.0',
        'priority': 'u=0, i',
        'referer': 'https://www.google.com/',
        'sec-ch-ua': '"Chromium";v="124", "Google Chrome";v="124", "Not-A.Brand";v="99"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"macOS"',
        'sec-fetch-dest': 'document',
        'sec-fetch-mode': 'navigate',
        'sec-fetch-site': 'same-origin',
        'sec-fetch-user': '?1',
        'upgrade-insecure-requests': '1',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
    }

    params = {
        'region': f'{continent[i]}',
    }

    response = requests.get('https://www.wifimap.io/countries', params=params, headers=headers)
    # response = requests.get('https://viewblock.io/arweave/stat/dataUploaded',  headers=headers)

    data1 = response.text

    matches = re.search(r'(?<=<script id="__NEXT_DATA__" type="application/json">)(.*?)(?=\s*</script>)', data1, re.DOTALL)

    if matches:
        json_data1 = matches.group(1)
        # print(json_data)
    else:
        print("No JSON data found between delimiters")

    data = json.loads(json_data1)['props']['pageProps']['countriesForCurrentRegion']['data']
    df = pd.DataFrame(data)
    temp = df.groupby('region_id')['hotspots_count'].sum().reset_index()
    wifi_count = pd.concat([wifi_count, temp], ignore_index=True)
wifi_count

# total_hotspots_value = wifi_count['hotspots_count'].sum()
total_hotspots = pd.DataFrame({'total_hotspots': [wifi_count['hotspots_count'].sum()]})
total_hotspots['project'] = 'Wifi Map'

total_hotspots

## Roam
import requests
headers = {
    'sec-ch-ua': '"Chromium";v="124", "Google Chrome";v="124", "Not-A.Brand";v="99"',
    'content-type': 'application/json',
    'Referer': 'https://depinscan.io/projects/roam',
    'sec-ch-ua-mobile': '?0',
    'authorization': 'undefined',
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
    'sec-ch-ua-platform': '"macOS"',
}
params = {
    'batch': '1',
    'input': '{"0":{"json":null,"meta":{"values":["undefined"]}}}',
}
response = requests.get('https://depinscan.io/api/trpc/projectRouter.publicProjectList', params=params, headers=headers)
data = response.json()[0]['result']['data']['json']
df = pd.DataFrame(data)
roam_data = df[df['project_name'] == 'Roam']
roam_data = roam_data[['project_name','total_devices' ]]
roam_data.columns = ['project', 'total_hotspots']
roam_data

import requests

headers = {
    'Accept': 'application/json, text/plain, */*',
    'Accept-Language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'Cache-Control': 'no-cache',
    'Connection': 'keep-alive',
    'Origin': 'https://scan.wicrypt.com',
    'Pragma': 'no-cache',
    'Referer': 'https://scan.wicrypt.com/',
    'Sec-Fetch-Dest': 'empty',
    'Sec-Fetch-Mode': 'cors',
    'Sec-Fetch-Site': 'same-site',
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
    'sec-ch-ua': '"Not/A)Brand";v="8", "Chromium";v="126", "Google Chrome";v="126"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
}

params = {
    'lookBackPeriodInHours': '',
    'chartLookbackPeriodInDays': '365',
}

response = requests.get('https://explorer.wicrypt.com/api/v1/Device/Statistics/ChartPoint', params=params, headers=headers)
data = response.json()
data = response.json()['data']['chartPoints']
wicrypt = pd.DataFrame(data)
wicrypt = pd.DataFrame({'total_hotspots': [wicrypt['count'].sum()]})
wicrypt['project'] = 'Wicrypt'
wicrypt
# data

import requests


headers = {
    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
    'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',
    'cache-control': 'max-age=0',
    # 'cookie': '_ga=GA1.1.816481769.1715840098; _ga_82VK1Z2SRZ=GS1.1.1716283817.5.1.1716283843.0.0.0',
    'if-none-match': 'W/"f5aa66ad975af4ad641900c7c32f2782"',
    'priority': 'u=0, i',
    'referer': 'https://www.google.com/',
    'sec-ch-ua': '"Chromium";v="124", "Google Chrome";v="124", "Not-A.Brand";v="99"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'sec-fetch-dest': 'document',
    'sec-fetch-mode': 'navigate',
    'sec-fetch-site': 'same-origin',
    'sec-fetch-user': '?1',
    'upgrade-insecure-requests': '1',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
}

response = requests.get('https://dabba.com/', headers=headers, proxies=proxies, verify = False)
tree = etree.HTML(response.text)
data = tree.xpath('//*[@id="__next"]/div[2]/div/div[2]/div/a[1]/div/div/div[1]/div/div[1]/p[1]/text()')
print(response)
wifi_dabba = pd.DataFrame([data])
wifi_dabba['project'] = 'Wifi Dabba'
wifi_dabba.columns = ['total_hotspots','project']
# wifi_dabba['total_hotspots'] = wifi_dabba['total_hotspots'].str.replace('+', '').astype(int)
wifi_dabba

import requests

headers = {
    'X-Dune-API-Key': 'your-dune-api-key',
}

params = {
    'limit': '1000',
}

response = requests.get('https://api.dune.com/api/v1/query/2470067/results/csv', params=params, headers=headers)
print(response)
data = response.text
import csv
from io import StringIO

# Instead of using json, we should use the CSV module as the endpoint returns CSV data.
data_list = list(csv.DictReader(StringIO(data)))
data_df = pd.DataFrame(data_list)
data_df['total_onboarded'] = pd.to_numeric(data_df['total_onboarded'], errors='coerce')
# Fixing the error by selecting only the 'total_onboarded' column after pct_change and assigning it to 'total_onboarded_change_7d'
data_df['total_onboarded_change_7d'] = data_df.groupby('maker_name')['total_onboarded'].pct_change(periods=7) * 100
data_df = data_df[data_df['date'] == data_df['date'].max()]
data_df = data_df[['total_onboarded', 'total_onboarded_change_7d']]
data_df['project'] = 'Helium'
data_df.columns = ['total_hotspots', 'total_hotspots_change_7d','project']
data_df.drop_duplicates(subset=['project'], keep='first', inplace=True)
data_df

xnet_data = pd.DataFrame({'project':'HelixNetwork','total_hotspots': [221], 'total_hotspots_change_7d':0})
xnet_data

#combining all data

metric_data = pd.concat([wifi_dabba, wicrypt, roam_data, total_hotspots,data_df,xnet_data], ignore_index=True)
metric_data

#combining github and weekly commits and devs data

wireless_data = github_data_combine.merge(metric_data, on='project', how='outer')
wireless_data

project = ['Helium', 'HelixNetwork', 'Wifi Map', 'Wicrypt']
token = ['helium', 'xnet-mobile', 'wifi', 'wicrypt']
token_map = pd.DataFrame({'project': project, 'token': token})
token_map

wireless_market_data = pd.merge(market_data, token_map, on='token', how='left')
wireless_market_data

q = """select A.* , B.market_cap,B.fdmcap, B.price_change_30d from wireless_data as A
left join wireless_market_data as B on A.project = B.project"""
wireless_network = pysqldf(q)
wireless_network['date'] = date.today()
wireless_network

wireless_network = wireless_network[['date', 'project','total_hotspots', 'market_cap', 'fdmcap', 'price_change_30d', 'stars', 'watchers','weekly_commits', 'active_devs' ]]
wireless_network['date'] = pd.to_datetime(wireless_network['date'])
wireless_network

wifi = ['Wifi Map', 'Roam', 'Wifi Dabba', 'Wicrypt', 'Wayru']
wifi_wireless_network = wireless_network[wireless_network['project'].isin(wifi)]
wifi_wireless_network['total_hotspots'] = pd.to_numeric(wifi_wireless_network['total_hotspots'], errors='coerce')
wifi_wireless_network


#adding image urls
wifi_wireless_network = wifi_wireless_network.merge(image_urls, on='project', how='left')

wifi_wireless_pre =  read_full_table('raw_m31', 'wifi_wireless_hist')
wifi_wireless_pre = wifi_wireless_pre[wifi_wireless_pre['date']==wifi_wireless_pre['date'].max()]
wifi_wireless_pre
q = """select A.*, B.total_hotspots as pre_total_hotspots
from wifi_wireless_network A
left join wifi_wireless_pre B
on A.project = B.project"""

wifi_wireless_network = pysqldf(q)
wifi_wireless_network['total_hotspots_change_7d'] = ((wifi_wireless_network['total_hotspots'] - wifi_wireless_network['pre_total_hotspots']) / wifi_wireless_network['pre_total_hotspots'])*100

wifi_wireless_network.drop(columns=['pre_total_hotspots'], inplace=True)
wifi_wireless_network


wifi_wireless_network['date'] = pd.to_datetime(wifi_wireless_network['date'])

wifi_wireless_network.to_csv('wifi_wireless_network.csv', index=False)


wifi_wireless_network['weekly_commits'] = np.nan
wifi_wireless_network['active_devs'] = np.nan


provider = 'm31'
dataset = 'wifi_wireless_network'

# cast weekly_commits to a string data type


create_or_overwrite_table(wifi_wireless_network, provider, dataset)
wifi_wireless_network

wifi_wireless_hist = read_full_table('raw_m31', 'wifi_wireless_hist')
wifi_wireless_hist = pd.concat([wifi_wireless_network, wifi_wireless_hist], ignore_index=True)


provider = 'm31'
dataset = 'wifi_wireless_hist'

create_or_overwrite_table(wifi_wireless_hist, provider, dataset)
wifi_wireless_hist

# Fixing the code for wireless mobile data
wifi = ['Wifi Map', 'Roam', 'Wifi Dabba', 'Wicrypt', 'Wayru']
mobile_wireless_network = wireless_network[~wireless_network['project'].isin(wifi)]
mobile_wireless_network['total_hotspots'] = pd.to_numeric(mobile_wireless_network['total_hotspots'], errors='coerce')

mobile_wireless_network = mobile_wireless_network.merge(image_urls, on = 'project', how = 'left')

mobile_hist = read_full_table('raw_m31', 'mobile_wireless_network_hist')
mobile_hist = mobile_hist[mobile_hist['date']==mobile_hist['date'].max()]

q = """select A.*, B.total_hotspots as pre_total_hotspots
from mobile_wireless_network A
left join mobile_hist B
on A.project = B.project"""

mobile_wireless_network = pysqldf(q)
mobile_wireless_network['total_hotspots_change_7d'] = ((mobile_wireless_network['total_hotspots'] - mobile_wireless_network['pre_total_hotspots']) / mobile_wireless_network['pre_total_hotspots'])*100
mobile_wireless_network
mobile_wireless_network.drop(columns=['pre_total_hotspots'], inplace=True)
wifi_wireless_network


# # wifi_wireless_network['date'] = pd.to_datetime(wifi_wireless_network['date'])

# # wifi_wireless_network.to_csv('mobile_wireless_network.csv', index=False)


# # wifi_wireless_network['weekly_commits'] = np.nan
# # wifi_wireless_network['active_devs'] = np.nan



mobile_wireless_network['date'] = pd.to_datetime(mobile_wireless_network['date'])




provider = 'm31'
dataset = 'mobile_wireless_network'

create_or_overwrite_table(mobile_wireless_network, provider, dataset)
mobile_wireless_network

dff = read_full_table('raw_m31', 'mobile_wireless_network')
dff.loc[dff['project'] == 'HelixNetwork', 'project'] = 'XNET'
dff.to_csv('mobile_wireless_network.csv', index=False)
dff

mobile_wireless_network = dff.copy()
mobile_wireless_network.to_csv('mobile_wireless_network.csv', index=False)

provider = 'm31'
dataset = 'mobile_wireless_network'

create_or_overwrite_table(mobile_wireless_network, provider, dataset)
mobile_wireless_network

mobile_hist = read_full_table('raw_m31', 'mobile_wireless_network_hist')
mobile_hist = pd.concat([mobile_wireless_network,mobile_hist],ignore_index=True)

provider = 'm31'
dataset = 'mobile_wireless_network_hist'

create_or_overwrite_table(mobile_hist, provider, dataset)
mobile_hist

import pandas

df = read_full_table('raw_m31', 'compute')
df
df['project'] = df['project'].replace('Internet Computer', 'ICP')
df['project'] = df['project'].replace('Akash Network', 'Akash')
df

provider = 'm31'
dataset = 'compute'
create_or_overwrite_table(df, provider, dataset)

"""<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=acd04261-284b-42ed-9c40-7dfba8e13a4d' target="_blank">
<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>
Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>
"""